[
    {
        "id": "32896467403",
        "type": "ForkEvent",
        "actor": {
            "id": 16216325,
            "login": "xzuyn",
            "display_login": "xzuyn",
            "gravatar_id": "",
            "url": "https://api.github.com/users/xzuyn",
            "avatar_url": "https://avatars.githubusercontent.com/u/16216325?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "forkee": {
                "id": 710748488,
                "node_id": "R_kgDOKl0pSA",
                "name": "kohya_ss",
                "full_name": "xzuyn/kohya_ss",
                "private": false,
                "owner": {
                    "login": "xzuyn",
                    "id": 16216325,
                    "node_id": "MDQ6VXNlcjE2MjE2MzI1",
                    "avatar_url": "https://avatars.githubusercontent.com/u/16216325?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/xzuyn",
                    "html_url": "https://github.com/xzuyn",
                    "followers_url": "https://api.github.com/users/xzuyn/followers",
                    "following_url": "https://api.github.com/users/xzuyn/following{/other_user}",
                    "gists_url": "https://api.github.com/users/xzuyn/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/xzuyn/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/xzuyn/subscriptions",
                    "organizations_url": "https://api.github.com/users/xzuyn/orgs",
                    "repos_url": "https://api.github.com/users/xzuyn/repos",
                    "events_url": "https://api.github.com/users/xzuyn/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/xzuyn/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "html_url": "https://github.com/xzuyn/kohya_ss",
                "description": null,
                "fork": true,
                "url": "https://api.github.com/repos/xzuyn/kohya_ss",
                "forks_url": "https://api.github.com/repos/xzuyn/kohya_ss/forks",
                "keys_url": "https://api.github.com/repos/xzuyn/kohya_ss/keys{/key_id}",
                "collaborators_url": "https://api.github.com/repos/xzuyn/kohya_ss/collaborators{/collaborator}",
                "teams_url": "https://api.github.com/repos/xzuyn/kohya_ss/teams",
                "hooks_url": "https://api.github.com/repos/xzuyn/kohya_ss/hooks",
                "issue_events_url": "https://api.github.com/repos/xzuyn/kohya_ss/issues/events{/number}",
                "events_url": "https://api.github.com/repos/xzuyn/kohya_ss/events",
                "assignees_url": "https://api.github.com/repos/xzuyn/kohya_ss/assignees{/user}",
                "branches_url": "https://api.github.com/repos/xzuyn/kohya_ss/branches{/branch}",
                "tags_url": "https://api.github.com/repos/xzuyn/kohya_ss/tags",
                "blobs_url": "https://api.github.com/repos/xzuyn/kohya_ss/git/blobs{/sha}",
                "git_tags_url": "https://api.github.com/repos/xzuyn/kohya_ss/git/tags{/sha}",
                "git_refs_url": "https://api.github.com/repos/xzuyn/kohya_ss/git/refs{/sha}",
                "trees_url": "https://api.github.com/repos/xzuyn/kohya_ss/git/trees{/sha}",
                "statuses_url": "https://api.github.com/repos/xzuyn/kohya_ss/statuses/{sha}",
                "languages_url": "https://api.github.com/repos/xzuyn/kohya_ss/languages",
                "stargazers_url": "https://api.github.com/repos/xzuyn/kohya_ss/stargazers",
                "contributors_url": "https://api.github.com/repos/xzuyn/kohya_ss/contributors",
                "subscribers_url": "https://api.github.com/repos/xzuyn/kohya_ss/subscribers",
                "subscription_url": "https://api.github.com/repos/xzuyn/kohya_ss/subscription",
                "commits_url": "https://api.github.com/repos/xzuyn/kohya_ss/commits{/sha}",
                "git_commits_url": "https://api.github.com/repos/xzuyn/kohya_ss/git/commits{/sha}",
                "comments_url": "https://api.github.com/repos/xzuyn/kohya_ss/comments{/number}",
                "issue_comment_url": "https://api.github.com/repos/xzuyn/kohya_ss/issues/comments{/number}",
                "contents_url": "https://api.github.com/repos/xzuyn/kohya_ss/contents/{+path}",
                "compare_url": "https://api.github.com/repos/xzuyn/kohya_ss/compare/{base}...{head}",
                "merges_url": "https://api.github.com/repos/xzuyn/kohya_ss/merges",
                "archive_url": "https://api.github.com/repos/xzuyn/kohya_ss/{archive_format}{/ref}",
                "downloads_url": "https://api.github.com/repos/xzuyn/kohya_ss/downloads",
                "issues_url": "https://api.github.com/repos/xzuyn/kohya_ss/issues{/number}",
                "pulls_url": "https://api.github.com/repos/xzuyn/kohya_ss/pulls{/number}",
                "milestones_url": "https://api.github.com/repos/xzuyn/kohya_ss/milestones{/number}",
                "notifications_url": "https://api.github.com/repos/xzuyn/kohya_ss/notifications{?since,all,participating}",
                "labels_url": "https://api.github.com/repos/xzuyn/kohya_ss/labels{/name}",
                "releases_url": "https://api.github.com/repos/xzuyn/kohya_ss/releases{/id}",
                "deployments_url": "https://api.github.com/repos/xzuyn/kohya_ss/deployments",
                "created_at": "2023-10-27T11:01:43Z",
                "updated_at": "2023-10-27T11:01:43Z",
                "pushed_at": "2023-10-25T15:32:35Z",
                "git_url": "git://github.com/xzuyn/kohya_ss.git",
                "ssh_url": "git@github.com:xzuyn/kohya_ss.git",
                "clone_url": "https://github.com/xzuyn/kohya_ss.git",
                "svn_url": "https://github.com/xzuyn/kohya_ss",
                "homepage": null,
                "size": 11763,
                "stargazers_count": 0,
                "watchers_count": 0,
                "language": null,
                "has_issues": false,
                "has_projects": true,
                "has_downloads": true,
                "has_wiki": true,
                "has_pages": false,
                "has_discussions": false,
                "forks_count": 0,
                "mirror_url": null,
                "archived": false,
                "disabled": false,
                "open_issues_count": 0,
                "license": null,
                "allow_forking": true,
                "is_template": false,
                "web_commit_signoff_required": false,
                "topics": [],
                "visibility": "public",
                "forks": 0,
                "open_issues": 0,
                "watchers": 0,
                "default_branch": "main",
                "public": true
            }
        },
        "public": true,
        "created_at": "2023-10-27T11:01:44Z"
    },
    {
        "id": "32895979182",
        "type": "ForkEvent",
        "actor": {
            "id": 16216325,
            "login": "xzuyn",
            "display_login": "xzuyn",
            "gravatar_id": "",
            "url": "https://api.github.com/users/xzuyn",
            "avatar_url": "https://avatars.githubusercontent.com/u/16216325?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "forkee": {
                "id": 710741416,
                "node_id": "R_kgDOKl0NqA",
                "name": "kohya_ss",
                "full_name": "xzuyn/kohya_ss",
                "private": false,
                "owner": {
                    "login": "xzuyn",
                    "id": 16216325,
                    "node_id": "MDQ6VXNlcjE2MjE2MzI1",
                    "avatar_url": "https://avatars.githubusercontent.com/u/16216325?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/xzuyn",
                    "html_url": "https://github.com/xzuyn",
                    "followers_url": "https://api.github.com/users/xzuyn/followers",
                    "following_url": "https://api.github.com/users/xzuyn/following{/other_user}",
                    "gists_url": "https://api.github.com/users/xzuyn/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/xzuyn/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/xzuyn/subscriptions",
                    "organizations_url": "https://api.github.com/users/xzuyn/orgs",
                    "repos_url": "https://api.github.com/users/xzuyn/repos",
                    "events_url": "https://api.github.com/users/xzuyn/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/xzuyn/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "html_url": "https://github.com/xzuyn/kohya_ss",
                "description": null,
                "fork": true,
                "url": "https://api.github.com/repos/xzuyn/kohya_ss",
                "forks_url": "https://api.github.com/repos/xzuyn/kohya_ss/forks",
                "keys_url": "https://api.github.com/repos/xzuyn/kohya_ss/keys{/key_id}",
                "collaborators_url": "https://api.github.com/repos/xzuyn/kohya_ss/collaborators{/collaborator}",
                "teams_url": "https://api.github.com/repos/xzuyn/kohya_ss/teams",
                "hooks_url": "https://api.github.com/repos/xzuyn/kohya_ss/hooks",
                "issue_events_url": "https://api.github.com/repos/xzuyn/kohya_ss/issues/events{/number}",
                "events_url": "https://api.github.com/repos/xzuyn/kohya_ss/events",
                "assignees_url": "https://api.github.com/repos/xzuyn/kohya_ss/assignees{/user}",
                "branches_url": "https://api.github.com/repos/xzuyn/kohya_ss/branches{/branch}",
                "tags_url": "https://api.github.com/repos/xzuyn/kohya_ss/tags",
                "blobs_url": "https://api.github.com/repos/xzuyn/kohya_ss/git/blobs{/sha}",
                "git_tags_url": "https://api.github.com/repos/xzuyn/kohya_ss/git/tags{/sha}",
                "git_refs_url": "https://api.github.com/repos/xzuyn/kohya_ss/git/refs{/sha}",
                "trees_url": "https://api.github.com/repos/xzuyn/kohya_ss/git/trees{/sha}",
                "statuses_url": "https://api.github.com/repos/xzuyn/kohya_ss/statuses/{sha}",
                "languages_url": "https://api.github.com/repos/xzuyn/kohya_ss/languages",
                "stargazers_url": "https://api.github.com/repos/xzuyn/kohya_ss/stargazers",
                "contributors_url": "https://api.github.com/repos/xzuyn/kohya_ss/contributors",
                "subscribers_url": "https://api.github.com/repos/xzuyn/kohya_ss/subscribers",
                "subscription_url": "https://api.github.com/repos/xzuyn/kohya_ss/subscription",
                "commits_url": "https://api.github.com/repos/xzuyn/kohya_ss/commits{/sha}",
                "git_commits_url": "https://api.github.com/repos/xzuyn/kohya_ss/git/commits{/sha}",
                "comments_url": "https://api.github.com/repos/xzuyn/kohya_ss/comments{/number}",
                "issue_comment_url": "https://api.github.com/repos/xzuyn/kohya_ss/issues/comments{/number}",
                "contents_url": "https://api.github.com/repos/xzuyn/kohya_ss/contents/{+path}",
                "compare_url": "https://api.github.com/repos/xzuyn/kohya_ss/compare/{base}...{head}",
                "merges_url": "https://api.github.com/repos/xzuyn/kohya_ss/merges",
                "archive_url": "https://api.github.com/repos/xzuyn/kohya_ss/{archive_format}{/ref}",
                "downloads_url": "https://api.github.com/repos/xzuyn/kohya_ss/downloads",
                "issues_url": "https://api.github.com/repos/xzuyn/kohya_ss/issues{/number}",
                "pulls_url": "https://api.github.com/repos/xzuyn/kohya_ss/pulls{/number}",
                "milestones_url": "https://api.github.com/repos/xzuyn/kohya_ss/milestones{/number}",
                "notifications_url": "https://api.github.com/repos/xzuyn/kohya_ss/notifications{?since,all,participating}",
                "labels_url": "https://api.github.com/repos/xzuyn/kohya_ss/labels{/name}",
                "releases_url": "https://api.github.com/repos/xzuyn/kohya_ss/releases{/id}",
                "deployments_url": "https://api.github.com/repos/xzuyn/kohya_ss/deployments",
                "created_at": "2023-10-27T10:41:17Z",
                "updated_at": "2023-10-27T10:41:17Z",
                "pushed_at": "2023-10-25T15:32:35Z",
                "git_url": "git://github.com/xzuyn/kohya_ss.git",
                "ssh_url": "git@github.com:xzuyn/kohya_ss.git",
                "clone_url": "https://github.com/xzuyn/kohya_ss.git",
                "svn_url": "https://github.com/xzuyn/kohya_ss",
                "homepage": null,
                "size": 11763,
                "stargazers_count": 0,
                "watchers_count": 0,
                "language": null,
                "has_issues": false,
                "has_projects": true,
                "has_downloads": true,
                "has_wiki": true,
                "has_pages": false,
                "has_discussions": false,
                "forks_count": 0,
                "mirror_url": null,
                "archived": false,
                "disabled": false,
                "open_issues_count": 0,
                "license": null,
                "allow_forking": true,
                "is_template": false,
                "web_commit_signoff_required": false,
                "topics": [],
                "visibility": "public",
                "forks": 0,
                "open_issues": 0,
                "watchers": 0,
                "default_branch": "main",
                "public": true
            }
        },
        "public": true,
        "created_at": "2023-10-27T10:41:18Z"
    },
    {
        "id": "32892672304",
        "type": "WatchEvent",
        "actor": {
            "id": 61530942,
            "login": "chunhui01",
            "display_login": "chunhui01",
            "gravatar_id": "",
            "url": "https://api.github.com/users/chunhui01",
            "avatar_url": "https://avatars.githubusercontent.com/u/61530942?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-27T08:35:20Z"
    },
    {
        "id": "32892504332",
        "type": "IssueCommentEvent",
        "actor": {
            "id": 33434783,
            "login": "itsmeherefolks",
            "display_login": "itsmeherefolks",
            "gravatar_id": "",
            "url": "https://api.github.com/users/itsmeherefolks",
            "avatar_url": "https://avatars.githubusercontent.com/u/33434783?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "created",
            "issue": {
                "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1635",
                "repository_url": "https://api.github.com/repos/bmaltais/kohya_ss",
                "labels_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1635/labels{/name}",
                "comments_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1635/comments",
                "events_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1635/events",
                "html_url": "https://github.com/bmaltais/kohya_ss/issues/1635",
                "id": 1962465875,
                "node_id": "I_kwDOIVqU3M50-NpT",
                "number": 1635,
                "title": "Lora training (RUNPOD)",
                "user": {
                    "login": "thekotfather",
                    "id": 148998686,
                    "node_id": "U_kgDOCOGKHg",
                    "avatar_url": "https://avatars.githubusercontent.com/u/148998686?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/thekotfather",
                    "html_url": "https://github.com/thekotfather",
                    "followers_url": "https://api.github.com/users/thekotfather/followers",
                    "following_url": "https://api.github.com/users/thekotfather/following{/other_user}",
                    "gists_url": "https://api.github.com/users/thekotfather/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/thekotfather/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/thekotfather/subscriptions",
                    "organizations_url": "https://api.github.com/users/thekotfather/orgs",
                    "repos_url": "https://api.github.com/users/thekotfather/repos",
                    "events_url": "https://api.github.com/users/thekotfather/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/thekotfather/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "labels": [],
                "state": "open",
                "locked": false,
                "assignee": null,
                "assignees": [],
                "milestone": null,
                "comments": 1,
                "created_at": "2023-10-25T23:45:13Z",
                "updated_at": "2023-10-27T08:28:47Z",
                "closed_at": null,
                "author_association": "NONE",
                "active_lock_reason": null,
                "body": "Help me, pleeeease!\r\n\r\nI've tried this installation 500 times, but no result.\r\nEverything is starting good, but..\r\n\r\n****\"ModuleNotFoundError: No module named 'scipy'\"**\r\nand then...\r\n...During handling of the above exception, another exception occurred:**\r\nblablablah(attached the full log)\r\n\r\n**scipy** is installed, of course.\r\n\r\nHELP ME UNDERSTAND, PLEASE =(\r\n\r\n\r\n\r\nLog file:\r\n\r\nroot@34890166845e:/workspace/kohya_ss# ./gui.sh --share --headless\r\n23:33:18-830649 INFO     Version: v22.1.0                                                                                       \r\n                                                                                                                                \r\n23:33:18-843460 INFO     nVidia toolkit detected                                                                                \r\n23:33:20-275862 INFO     Torch 2.0.1+cu118                                                                                      \r\n23:33:20-313302 INFO     Torch backend: nVidia CUDA 11.8 cuDNN 8904                                                             \r\n23:33:20-329518 INFO     Torch detected GPU: Tesla V100-SXM2-16GB VRAM 16151 Arch (7, 0) Cores 80                               \r\n23:33:20-330969 INFO     Verifying modules installation status from /workspace/kohya_ss/requirements_runpod.txt...              \r\n23:33:20-334579 INFO     Verifying modules installation status from requirements.txt...                                         \r\n23:33:20-340323 WARNING  Package wrong version: huggingface-hub 0.17.3 required 0.15.1                                          \r\n23:33:20-341864 INFO     Installing package: huggingface-hub==0.15.1                                                            \r\n23:33:26-377521 INFO     headless: True                                                                                         \r\n23:33:26-382540 INFO     Load CSS...                                                                                            \r\nRunning on local URL:  http://127.0.0.1:7863\r\nRunning on public URL: https://09bdb39e27a344aa7b.gradio.live\r\n\r\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\r\n23:34:59-397707 INFO     Loading config...                                                                                      \r\n23:35:15-756463 INFO     Loading config...                                                                                      \r\n23:35:59-035797 INFO     Start training LoRA Standard ...                                                                       \r\n23:35:59-038059 INFO     Checking for duplicate image filenames in training data directory...                                   \r\n23:35:59-040859 INFO     Valid image folder names found in: /workspace/test1/img                                                \r\n23:35:59-043044 INFO     Headless mode, skipping verification if model already exist... if model already exist it will be       \r\n                         overwritten...                                                                                         \r\n23:35:59-045912 INFO     Folder 40_natusyavelikovnagross woman: 13 images found                                                 \r\n23:35:59-048047 INFO     Folder 40_natusyavelikovnagross woman: 520 steps                                                       \r\n23:35:59-049271 INFO     Total steps: 520                                                                                       \r\n23:35:59-050185 INFO     Train batch size: 1                                                                                    \r\n23:35:59-051122 INFO     Gradient accumulation steps: 1                                                                         \r\n23:35:59-052064 INFO     Epoch: 1                                                                                               \r\n23:35:59-052973 INFO     Regulatization factor: 1                                                                               \r\n23:35:59-053918 INFO     max_train_steps (520 / 1 / 1 * 1 * 1) = 520                                                            \r\n23:35:59-055174 INFO     stop_text_encoder_training = 0                                                                         \r\n23:35:59-056143 INFO     lr_warmup_steps = 52                                                                                   \r\n23:35:59-057189 INFO     Saving training config to /workspace/test1/model/last_20231025-233559.json...                          \r\n23:35:59-058623 INFO     accelerate launch --num_cpu_threads_per_process=2 \"./train_network.py\" --enable_bucket                 \r\n                         --min_bucket_reso=256 --max_bucket_reso=2048                                                           \r\n                         --pretrained_model_name_or_path=\"/workspace/stable-diffusion-webui/models/Stable-diffusion/deliberate.s\r\n                         afetensors\" --train_data_dir=\"/workspace/test1/img\" --resolution=\"512,512\"                             \r\n                         --output_dir=\"/workspace/test1/model\" --logging_dir=\"/workspace/test1/log\" --network_alpha=\"1\"         \r\n                         --save_model_as=safetensors --network_module=networks.lora --text_encoder_lr=5e-05 --unet_lr=0.0001    \r\n                         --network_dim=256 --output_name=\"last\" --lr_scheduler_num_cycles=\"1\" --no_half_vae                     \r\n                         --learning_rate=\"0.0001\" --lr_scheduler=\"cosine\" --lr_warmup_steps=\"52\" --train_batch_size=\"1\"         \r\n                         --max_train_steps=\"520\" --save_every_n_epochs=\"1\" --mixed_precision=\"fp16\" --save_precision=\"fp16\"     \r\n                         --cache_latents --optimizer_type=\"AdamW8bit\" --max_data_loader_n_workers=\"0\" --bucket_reso_steps=64    \r\n                         --xformers --bucket_no_upscale --noise_offset=0.0                                                      \r\n2023-10-25 23:36:03.826827: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2023-10-25 23:36:03.826886: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2023-10-25 23:36:03.826914: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2023-10-25 23:36:03.835111: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-10-25 23:36:04.783174: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\nprepare tokenizer\r\nUsing DreamBooth method.\r\nprepare images.\r\nfound directory /workspace/test1/img/40_natusyavelikovnagross woman contains 13 image files\r\nNo caption file found for 13 images. Training will continue without captions for these images. If class token exists, it will be used. / 13\u679a\u306e\u753b\u50cf\u306b\u30ad\u30e3\u30d7\u30b7\u30e7\u30f3\u30d5\u30a1\u30a4\u30eb\u304c\u898b\u3064\u304b\u308a\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u3053\u308c\u3089\u306e\u753b\u50cf\u306b\u3064\u3044\u3066\u306f\u30ad\u30e3\u30d7\u30b7\u30e7\u30f3\u306a\u3057\u3067\u5b66\u7fd2\u3092\u7d9a\u884c\u3057\u307e\u3059\u3002class token\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306f\u305d\u308c\u3092\u4f7f\u3044\u307e\u3059\u3002\r\n/workspace/test1/img/40_natusyavelikovnagross woman/image (10).png\r\n/workspace/test1/img/40_natusyavelikovnagross woman/image (11).png\r\n/workspace/test1/img/40_natusyavelikovnagross woman/image (12).png\r\n/workspace/test1/img/40_natusyavelikovnagross woman/image (13).png\r\n/workspace/test1/img/40_natusyavelikovnagross woman/image (14).png\r\n/workspace/test1/img/40_natusyavelikovnagross woman/image (15).png... and 8 more\r\n520 train images with repeating.\r\n0 reg images.\r\nno regularization images / \u6b63\u5247\u5316\u753b\u50cf\u304c\u898b\u3064\u304b\u308a\u307e\u305b\u3093\u3067\u3057\u305f\r\n[Dataset 0]\r\n  batch_size: 1\r\n  resolution: (512, 512)\r\n  enable_bucket: True\r\n  min_bucket_reso: 256\r\n  max_bucket_reso: 2048\r\n  bucket_reso_steps: 64\r\n  bucket_no_upscale: True\r\n\r\n  [Subset 0 of Dataset 0]\r\n    image_dir: \"/workspace/test1/img/40_natusyavelikovnagross woman\"\r\n    image_count: 13\r\n    num_repeats: 40\r\n    shuffle_caption: False\r\n    keep_tokens: 0\r\n    caption_dropout_rate: 0.0\r\n    caption_dropout_every_n_epoches: 0\r\n    caption_tag_dropout_rate: 0.0\r\n    caption_prefix: None\r\n    caption_suffix: None\r\n    color_aug: False\r\n    flip_aug: False\r\n    face_crop_aug_range: None\r\n    random_crop: False\r\n    token_warmup_min: 1,\r\n    token_warmup_step: 0,\r\n    is_reg: False\r\n    class_tokens: natusyavelikovnagross woman\r\n    caption_extension: .caption\r\n\r\n\r\n[Dataset 0]\r\nloading image sizes.\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00<00:00, 1611.33it/s]\r\nmake buckets\r\nmin_bucket_reso and max_bucket_reso are ignored if bucket_no_upscale is set, because bucket reso is defined by image size automatically / bucket_no_upscale\u304c\u6307\u5b9a\u3055\u308c\u305f\u5834\u5408\u306f\u3001bucket\u306e\u89e3\u50cf\u5ea6\u306f\u753b\u50cf\u30b5\u30a4\u30ba\u304b\u3089\u81ea\u52d5\u8a08\u7b97\u3055\u308c\u308b\u305f\u3081\u3001min_bucket_reso\u3068max_bucket_reso\u306f\u7121\u8996\u3055\u308c\u307e\u3059\r\nnumber of images (including repeats) / \u5404bucket\u306e\u753b\u50cf\u679a\u6570\uff08\u7e70\u308a\u8fd4\u3057\u56de\u6570\u3092\u542b\u3080\uff09\r\nbucket 0: resolution (512, 512), count: 520\r\nmean ar error (without repeats): 0.0\r\npreparing accelerator\r\nloading model for process 0/1\r\nload StableDiffusion checkpoint: /workspace/stable-diffusion-webui/models/Stable-diffusion/deliberate.safetensors\r\nUNet2DConditionModel: 64, 8, 768, False, False\r\nloading u-net: <All keys matched successfully>\r\nloading vae: <All keys matched successfully>\r\nloading text encoder: <All keys matched successfully>\r\nEnable xformers for U-Net\r\nimport network module: networks.lora\r\n[Dataset 0]\r\ncaching latents.\r\nchecking cache validity...\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00<00:00, 254794.17it/s]\r\ncaching latents...\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:02<00:00,  5.96it/s]\r\ncreate LoRA network. base dim (rank): 256, alpha: 1.0\r\nneuron dropout: p=None, rank dropout: p=None, module dropout: p=None\r\ncreate LoRA for Text Encoder:\r\ncreate LoRA for Text Encoder: 72 modules.\r\ncreate LoRA for U-Net: 192 modules.\r\nenable LoRA for text encoder\r\nenable LoRA for U-Net\r\nprepare optimizer, data loader etc.\r\nTraceback (most recent call last):\r\n  File \"/workspace/kohya_ss/library/train_util.py\", line 3419, in get_optimizer\r\n    import bitsandbytes as bnb\r\n  File \"/workspace/kohya_ss/venv/lib/python3.10/site-packages/bitsandbytes/__init__.py\", line 6, in <module>\r\n    from . import cuda_setup, utils, research\r\n  File \"/workspace/kohya_ss/venv/lib/python3.10/site-packages/bitsandbytes/research/__init__.py\", line 1, in <module>\r\n    from . import nn\r\n  File \"/workspace/kohya_ss/venv/lib/python3.10/site-packages/bitsandbytes/research/nn/__init__.py\", line 1, in <module>\r\n    from .modules import LinearFP8Mixed, LinearFP8Global\r\n  File \"/workspace/kohya_ss/venv/lib/python3.10/site-packages/bitsandbytes/research/nn/modules.py\", line 8, in <module>\r\n    from bitsandbytes.optim import GlobalOptimManager\r\n  File \"/workspace/kohya_ss/venv/lib/python3.10/site-packages/bitsandbytes/optim/__init__.py\", line 8, in <module>\r\n    from .adagrad import Adagrad, Adagrad8bit, Adagrad32bit\r\n  File \"/workspace/kohya_ss/venv/lib/python3.10/site-packages/bitsandbytes/optim/adagrad.py\", line 5, in <module>\r\n    from bitsandbytes.optim.optimizer import Optimizer1State\r\n  File \"/workspace/kohya_ss/venv/lib/python3.10/site-packages/bitsandbytes/optim/optimizer.py\", line 12, in <module>\r\n    import bitsandbytes.functional as F\r\n  File \"/workspace/kohya_ss/venv/lib/python3.10/site-packages/bitsandbytes/functional.py\", line 12, in <module>\r\n    from scipy.stats import norm\r\nModuleNotFoundError: No module named 'scipy'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/workspace/kohya_ss/./train_network.py\", line 1009, in <module>\r\n    trainer.train(args)\r\n  File \"/workspace/kohya_ss/./train_network.py\", line 338, in train\r\n    optimizer_name, optimizer_args, optimizer = train_util.get_optimizer(args, trainable_params)\r\n  File \"/workspace/kohya_ss/library/train_util.py\", line 3421, in get_optimizer\r\n    raise ImportError(\"No bitsandbytes / bitsandbytes\u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u306a\u3044\u3088\u3046\u3067\u3059\")\r\nImportError: No bitsandbytes / bitsandbytes\u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u306a\u3044\u3088\u3046\u3067\u3059\r\nTraceback (most recent call last):\r\n  File \"/workspace/kohya_ss/venv/bin/accelerate\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/workspace/kohya_ss/venv/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py\", line 47, in main\r\n    args.func(args)\r\n  File \"/workspace/kohya_ss/venv/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 986, in launch_command\r\n    simple_launcher(args)\r\n  File \"/workspace/kohya_ss/venv/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 628, in simple_launcher\r\n    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\r\nsubprocess.CalledProcessError: Command '['/workspace/kohya_ss/venv/bin/python', './train_network.py', '--enable_bucket', '--min_bucket_reso=256', '--max_bucket_reso=2048', '--pretrained_model_name_or_path=/workspace/stable-diffusion-webui/models/Stable-diffusion/deliberate.safetensors', '--train_data_dir=/workspace/test1/img', '--resolution=512,512', '--output_dir=/workspace/test1/model', '--logging_dir=/workspace/test1/log', '--network_alpha=1', '--save_model_as=safetensors', '--network_module=networks.lora', '--text_encoder_lr=5e-05', '--unet_lr=0.0001', '--network_dim=256', '--output_name=last', '--lr_scheduler_num_cycles=1', '--no_half_vae', '--learning_rate=0.0001', '--lr_scheduler=cosine', '--lr_warmup_steps=52', '--train_batch_size=1', '--max_train_steps=520', '--save_every_n_epochs=1', '--mixed_precision=fp16', '--save_precision=fp16', '--cache_latents', '--optimizer_type=AdamW8bit', '--max_data_loader_n_workers=0', '--bucket_reso_steps=64', '--xformers', '--bucket_no_upscale', '--noise_offset=0.0']' returned non-zero exit status 1.",
                "reactions": {
                    "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1635/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "timeline_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1635/timeline",
                "performed_via_github_app": null,
                "state_reason": null
            },
            "comment": {
                "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/comments/1782512149",
                "html_url": "https://github.com/bmaltais/kohya_ss/issues/1635#issuecomment-1782512149",
                "issue_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1635",
                "id": 1782512149,
                "node_id": "IC_kwDOIVqU3M5qPvoV",
                "user": {
                    "login": "itsmeherefolks",
                    "id": 33434783,
                    "node_id": "MDQ6VXNlcjMzNDM0Nzgz",
                    "avatar_url": "https://avatars.githubusercontent.com/u/33434783?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/itsmeherefolks",
                    "html_url": "https://github.com/itsmeherefolks",
                    "followers_url": "https://api.github.com/users/itsmeherefolks/followers",
                    "following_url": "https://api.github.com/users/itsmeherefolks/following{/other_user}",
                    "gists_url": "https://api.github.com/users/itsmeherefolks/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/itsmeherefolks/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/itsmeherefolks/subscriptions",
                    "organizations_url": "https://api.github.com/users/itsmeherefolks/orgs",
                    "repos_url": "https://api.github.com/users/itsmeherefolks/repos",
                    "events_url": "https://api.github.com/users/itsmeherefolks/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/itsmeherefolks/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "created_at": "2023-10-27T08:28:47Z",
                "updated_at": "2023-10-27T08:28:47Z",
                "author_association": "NONE",
                "body": "I had a similar problem on my Linux desktop. The solution there was simply to install scipy into the venv:\r\n`pip install scipy`\r\n ",
                "reactions": {
                    "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/comments/1782512149/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "performed_via_github_app": null
            }
        },
        "public": true,
        "created_at": "2023-10-27T08:28:47Z"
    },
    {
        "id": "32887405936",
        "type": "WatchEvent",
        "actor": {
            "id": 62335372,
            "login": "cqflgl",
            "display_login": "cqflgl",
            "gravatar_id": "",
            "url": "https://api.github.com/users/cqflgl",
            "avatar_url": "https://avatars.githubusercontent.com/u/62335372?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-27T03:44:20Z"
    },
    {
        "id": "32887349979",
        "type": "WatchEvent",
        "actor": {
            "id": 41569443,
            "login": "KaKeimei",
            "display_login": "KaKeimei",
            "gravatar_id": "",
            "url": "https://api.github.com/users/KaKeimei",
            "avatar_url": "https://avatars.githubusercontent.com/u/41569443?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-27T03:39:29Z"
    },
    {
        "id": "32887260386",
        "type": "WatchEvent",
        "actor": {
            "id": 89783477,
            "login": "ZCHUHan",
            "display_login": "ZCHUHan",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ZCHUHan",
            "avatar_url": "https://avatars.githubusercontent.com/u/89783477?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-27T03:31:53Z"
    },
    {
        "id": "32887243005",
        "type": "IssuesEvent",
        "actor": {
            "id": 101603910,
            "login": "Pokilokui",
            "display_login": "Pokilokui",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Pokilokui",
            "avatar_url": "https://avatars.githubusercontent.com/u/101603910?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "opened",
            "issue": {
                "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1638",
                "repository_url": "https://api.github.com/repos/bmaltais/kohya_ss",
                "labels_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1638/labels{/name}",
                "comments_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1638/comments",
                "events_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1638/events",
                "html_url": "https://github.com/bmaltais/kohya_ss/issues/1638",
                "id": 1964700071,
                "node_id": "I_kwDOIVqU3M51GvGn",
                "number": 1638,
                "title": "ValueError: torch.cuda.is_available() should be True but is False. xformers' memory efficient attention is only available for GPU",
                "user": {
                    "login": "Pokilokui",
                    "id": 101603910,
                    "node_id": "U_kgDOBg5aRg",
                    "avatar_url": "https://avatars.githubusercontent.com/u/101603910?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/Pokilokui",
                    "html_url": "https://github.com/Pokilokui",
                    "followers_url": "https://api.github.com/users/Pokilokui/followers",
                    "following_url": "https://api.github.com/users/Pokilokui/following{/other_user}",
                    "gists_url": "https://api.github.com/users/Pokilokui/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/Pokilokui/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/Pokilokui/subscriptions",
                    "organizations_url": "https://api.github.com/users/Pokilokui/orgs",
                    "repos_url": "https://api.github.com/users/Pokilokui/repos",
                    "events_url": "https://api.github.com/users/Pokilokui/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/Pokilokui/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "labels": [],
                "state": "open",
                "locked": false,
                "assignee": null,
                "assignees": [],
                "milestone": null,
                "comments": 0,
                "created_at": "2023-10-27T03:30:22Z",
                "updated_at": "2023-10-27T03:30:22Z",
                "closed_at": null,
                "author_association": "NONE",
                "active_lock_reason": null,
                "body": "tried to set all in lower case to gpu select but does not work\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\AI\\Kohya\\kohya_ss\\train_network.py\", line 1009, in <module>\r\n    trainer.train(args)\r\n  File \"D:\\AI\\Kohya\\kohya_ss\\train_network.py\", line 232, in train\r\n    vae.set_use_memory_efficient_attention_xformers(args.xformers)\r\n  File \"D:\\AI\\Kohya\\kohya_ss\\venv\\lib\\site-packages\\diffusers\\models\\modeling_utils.py\", line 251, in set_use_memory_efficient_attention_xformers\r\n    fn_recursive_set_mem_eff(module)\r\n  File \"D:\\AI\\Kohya\\kohya_ss\\venv\\lib\\site-packages\\diffusers\\models\\modeling_utils.py\", line 247, in fn_recursive_set_mem_eff\r\n    fn_recursive_set_mem_eff(child)\r\n  File \"D:\\AI\\Kohya\\kohya_ss\\venv\\lib\\site-packages\\diffusers\\models\\modeling_utils.py\", line 247, in fn_recursive_set_mem_eff\r\n    fn_recursive_set_mem_eff(child)\r\n  File \"D:\\AI\\Kohya\\kohya_ss\\venv\\lib\\site-packages\\diffusers\\models\\modeling_utils.py\", line 247, in fn_recursive_set_mem_eff\r\n    fn_recursive_set_mem_eff(child)\r\n  File \"D:\\AI\\Kohya\\kohya_ss\\venv\\lib\\site-packages\\diffusers\\models\\modeling_utils.py\", line 244, in fn_recursive_set_mem_eff\r\n    module.set_use_memory_efficient_attention_xformers(valid, attention_op)\r\n  File \"D:\\AI\\Kohya\\kohya_ss\\venv\\lib\\site-packages\\diffusers\\models\\attention_processor.py\", line 203, in set_use_memory_efficient_attention_xformers\r\n    raise ValueError(\r\nValueError: torch.cuda.is_available() should be True but is False. xformers' memory efficient attention is only available for GPU\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Elouan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"C:\\Users\\Elouan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"D:\\AI\\Kohya\\kohya_ss\\venv\\Scripts\\accelerate.exe\\__main__.py\", line 7, in <module>\r\n  File \"D:\\AI\\Kohya\\kohya_ss\\venv\\lib\\site-packages\\accelerate\\commands\\accelerate_cli.py\", line 47, in main\r\n    args.func(args)\r\n  File \"D:\\AI\\Kohya\\kohya_ss\\venv\\lib\\site-packages\\accelerate\\commands\\launch.py\", line 986, in launch_command\r\n    simple_launcher(args)\r\n  File \"D:\\AI\\Kohya\\kohya_ss\\venv\\lib\\site-packages\\accelerate\\commands\\launch.py\", line 628, in simple_launcher\r\n    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\r\nsubprocess.CalledProcessError: Command '['D:\\\\AI\\\\Kohya\\\\kohya_ss\\\\venv\\\\Scripts\\\\python.exe', './train_network.py', '--enable_bucket', '--min_bucket_reso=256', '--max_bucket_reso=2048', '--pretrained_model_name_or_path=runwayml/stable-diffusion-v1-5', '--train_data_dir=D:/AI/Kohya/kunaboto/kunaboto style/image', '--resolution=512,512', '--output_dir=D:/AI/Kohya/kunaboto/kunaboto style/model', '--logging_dir=D:/AI/Kohya/kunaboto/kunaboto style/log', '--network_alpha=128', '--save_model_as=safetensors', '--network_module=networks.lora', '--text_encoder_lr=5e-05', '--unet_lr=0.0001', '--network_dim=128', '--output_name=Kunaboto style', '--lr_scheduler_num_cycles=1', '--no_half_vae', '--learning_rate=0.0001', '--lr_scheduler=constant', '--train_batch_size=2', '--max_train_steps=1300', '--save_every_n_epochs=1', '--mixed_precision=bf16', '--save_precision=bf16', '--seed=1234', '--caption_extension=.txt', '--cache_latents', '--optimizer_type=AdamW8bit', '--max_data_loader_n_workers=1', '--clip_skip=2', '--bucket_reso_steps=64', '--xformers', '--bucket_no_upscale', '--noise_offset=0.0']' returned non-zero exit status 1.",
                "reactions": {
                    "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1638/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "timeline_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1638/timeline",
                "performed_via_github_app": null,
                "state_reason": null
            }
        },
        "public": true,
        "created_at": "2023-10-27T03:30:24Z"
    },
    {
        "id": "32886309653",
        "type": "IssueCommentEvent",
        "actor": {
            "id": 146362857,
            "login": "tlegower",
            "display_login": "tlegower",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tlegower",
            "avatar_url": "https://avatars.githubusercontent.com/u/146362857?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "created",
            "issue": {
                "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1400",
                "repository_url": "https://api.github.com/repos/bmaltais/kohya_ss",
                "labels_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1400/labels{/name}",
                "comments_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1400/comments",
                "events_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1400/events",
                "html_url": "https://github.com/bmaltais/kohya_ss/issues/1400",
                "id": 1855255028,
                "node_id": "I_kwDOIVqU3M5ulPH0",
                "number": 1400,
                "title": "Blip Captioning not working ",
                "user": {
                    "login": "mikesaa309",
                    "id": 121466693,
                    "node_id": "U_kgDOBz1vRQ",
                    "avatar_url": "https://avatars.githubusercontent.com/u/121466693?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/mikesaa309",
                    "html_url": "https://github.com/mikesaa309",
                    "followers_url": "https://api.github.com/users/mikesaa309/followers",
                    "following_url": "https://api.github.com/users/mikesaa309/following{/other_user}",
                    "gists_url": "https://api.github.com/users/mikesaa309/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/mikesaa309/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/mikesaa309/subscriptions",
                    "organizations_url": "https://api.github.com/users/mikesaa309/orgs",
                    "repos_url": "https://api.github.com/users/mikesaa309/repos",
                    "events_url": "https://api.github.com/users/mikesaa309/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/mikesaa309/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "labels": [],
                "state": "open",
                "locked": false,
                "assignee": null,
                "assignees": [],
                "milestone": null,
                "comments": 3,
                "created_at": "2023-08-17T15:41:28Z",
                "updated_at": "2023-10-27T02:15:49Z",
                "closed_at": null,
                "author_association": "NONE",
                "active_lock_reason": null,
                "body": "When I try to use blip captioning I get this in the CMD and no text files are generated. If I add a prefix then texts files are generated but only with the prefix I typed. \r\n![blip caption](https://github.com/bmaltais/kohya_ss/assets/121466693/5a6155b1-0da4-48bf-b4cc-5db965b6d6db)\r\n",
                "reactions": {
                    "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1400/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "timeline_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1400/timeline",
                "performed_via_github_app": null,
                "state_reason": null
            },
            "comment": {
                "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/comments/1782182101",
                "html_url": "https://github.com/bmaltais/kohya_ss/issues/1400#issuecomment-1782182101",
                "issue_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1400",
                "id": 1782182101,
                "node_id": "IC_kwDOIVqU3M5qOfDV",
                "user": {
                    "login": "tlegower",
                    "id": 146362857,
                    "node_id": "U_kgDOCLlR6Q",
                    "avatar_url": "https://avatars.githubusercontent.com/u/146362857?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/tlegower",
                    "html_url": "https://github.com/tlegower",
                    "followers_url": "https://api.github.com/users/tlegower/followers",
                    "following_url": "https://api.github.com/users/tlegower/following{/other_user}",
                    "gists_url": "https://api.github.com/users/tlegower/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/tlegower/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/tlegower/subscriptions",
                    "organizations_url": "https://api.github.com/users/tlegower/orgs",
                    "repos_url": "https://api.github.com/users/tlegower/repos",
                    "events_url": "https://api.github.com/users/tlegower/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/tlegower/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "created_at": "2023-10-27T02:15:49Z",
                "updated_at": "2023-10-27T02:15:49Z",
                "author_association": "NONE",
                "body": "> Try to downgrade transformers version in kohya requirements.txt Change original transformers to transformers==4.25.1 At least for me, this worked.\r\n\r\nThank you!  Downgrading worked for me too.  Batch 8, Beam Search 10, and it completed successfully",
                "reactions": {
                    "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/comments/1782182101/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "performed_via_github_app": null
            }
        },
        "public": true,
        "created_at": "2023-10-27T02:15:50Z"
    },
    {
        "id": "32886141099",
        "type": "WatchEvent",
        "actor": {
            "id": 5786233,
            "login": "selfdem",
            "display_login": "selfdem",
            "gravatar_id": "",
            "url": "https://api.github.com/users/selfdem",
            "avatar_url": "https://avatars.githubusercontent.com/u/5786233?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-27T02:02:25Z"
    },
    {
        "id": "32886078621",
        "type": "WatchEvent",
        "actor": {
            "id": 26643846,
            "login": "YuTianShi",
            "display_login": "YuTianShi",
            "gravatar_id": "",
            "url": "https://api.github.com/users/YuTianShi",
            "avatar_url": "https://avatars.githubusercontent.com/u/26643846?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-27T01:57:55Z"
    },
    {
        "id": "32883544914",
        "type": "IssuesEvent",
        "actor": {
            "id": 106035416,
            "login": "hurfifx",
            "display_login": "hurfifx",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hurfifx",
            "avatar_url": "https://avatars.githubusercontent.com/u/106035416?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "opened",
            "issue": {
                "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1637",
                "repository_url": "https://api.github.com/repos/bmaltais/kohya_ss",
                "labels_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1637/labels{/name}",
                "comments_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1637/comments",
                "events_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1637/events",
                "html_url": "https://github.com/bmaltais/kohya_ss/issues/1637",
                "id": 1964473137,
                "node_id": "I_kwDOIVqU3M51F3sx",
                "number": 1637,
                "title": "Dreambooth error after pressing train",
                "user": {
                    "login": "hurfifx",
                    "id": 106035416,
                    "node_id": "U_kgDOBlH42A",
                    "avatar_url": "https://avatars.githubusercontent.com/u/106035416?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/hurfifx",
                    "html_url": "https://github.com/hurfifx",
                    "followers_url": "https://api.github.com/users/hurfifx/followers",
                    "following_url": "https://api.github.com/users/hurfifx/following{/other_user}",
                    "gists_url": "https://api.github.com/users/hurfifx/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/hurfifx/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/hurfifx/subscriptions",
                    "organizations_url": "https://api.github.com/users/hurfifx/orgs",
                    "repos_url": "https://api.github.com/users/hurfifx/repos",
                    "events_url": "https://api.github.com/users/hurfifx/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/hurfifx/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "labels": [],
                "state": "open",
                "locked": false,
                "assignee": null,
                "assignees": [],
                "milestone": null,
                "comments": 0,
                "created_at": "2023-10-26T22:40:47Z",
                "updated_at": "2023-10-26T22:40:47Z",
                "closed_at": null,
                "author_association": "NONE",
                "active_lock_reason": null,
                "body": "Hi there, I have no idea of whats happening..please help \r\n\r\nCUDA SETUP: Loading binary D:\\Users\\Bureau\\Tests-ia\\kohya_ss\\venv\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda116.dll...\r\nuse 8-bit AdamW optimizer | {}\r\nresume training from local state: B:/mariamerced/savess\r\nTraceback (most recent call last):\r\n  File \"D:\\Users\\Bureau\\Tests-ia\\kohya_ss\\train_db.py\", line 488, in <module>\r\n    train(args)\r\n  File \"D:\\Users\\Bureau\\Tests-ia\\kohya_ss\\train_db.py\", line 229, in train\r\n    train_util.resume_from_local_or_hf_if_specified(accelerator, args)\r\n  File \"D:\\Users\\Bureau\\Tests-ia\\kohya_ss\\library\\train_util.py\", line 3316, in resume_from_local_or_hf_if_specified\r\n    accelerator.load_state(args.resume)\r\n  File \"D:\\Users\\Bureau\\Tests-ia\\kohya_ss\\venv\\lib\\site-packages\\accelerate\\accelerator.py\", line 2938, in load_state\r\n    load_accelerator_state(\r\n  File \"D:\\Users\\Bureau\\Tests-ia\\kohya_ss\\venv\\lib\\site-packages\\accelerate\\checkpointing.py\", line 159, in load_accelerator_state\r\n    models[i].load_state_dict(torch.load(input_model_file, map_location=map_location), **load_model_func_kwargs)\r\n  File \"D:\\Users\\Bureau\\Tests-ia\\kohya_ss\\venv\\lib\\site-packages\\torch\\serialization.py\", line 791, in load\r\n    with _open_file_like(f, 'rb') as opened_file:\r\n  File \"D:\\Users\\Bureau\\Tests-ia\\kohya_ss\\venv\\lib\\site-packages\\torch\\serialization.py\", line 271, in _open_file_like\r\n    return _open_file(name_or_buffer, mode)\r\n  File \"D:\\Users\\Bureau\\Tests-ia\\kohya_ss\\venv\\lib\\site-packages\\torch\\serialization.py\", line 252, in __init__\r\n    super().__init__(open(name, mode))\r\nFileNotFoundError: [Errno 2] No such file or directory: 'B:/mariamerced/savess\\\\pytorch_model.bin'\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\hurfi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"C:\\Users\\hurfi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"D:\\Users\\Bureau\\Tests-ia\\kohya_ss\\venv\\Scripts\\accelerate.exe\\__main__.py\", line 7, in <module>\r\n  File \"D:\\Users\\Bureau\\Tests-ia\\kohya_ss\\venv\\lib\\site-packages\\accelerate\\commands\\accelerate_cli.py\", line 47, in main\r\n    args.func(args)\r\n  File \"D:\\Users\\Bureau\\Tests-ia\\kohya_ss\\venv\\lib\\site-packages\\accelerate\\commands\\launch.py\", line 986, in launch_command\r\n    simple_launcher(args)\r\n  File \"D:\\Users\\Bureau\\Tests-ia\\kohya_ss\\venv\\lib\\site-packages\\accelerate\\commands\\launch.py\", line 628, in simple_launcher\r\n    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\r\nsubprocess.CalledProcessError: Command '['D:\\\\Users\\\\Bureau\\\\Tests-ia\\\\kohya_ss\\\\venv\\\\Scripts\\\\python.exe', './train_db.py', '--enable_bucket', '--min_bucket_reso=256', '--max_bucket_reso=2048', '--pretrained_model_name_or_path=D:/Users/Bureau/Tests-ia/models/Stable-diffusion/realisticVisionV51_v51VAE.safetensors', '--train_data_dir=B:/mariamerced/trainingresult\\\\img', '--resolution=512,512', '--output_dir=B:/mariamerced/trainingresult\\\\model', '--logging_dir=B:/mariamerced/trainingresult\\\\log', '--save_model_as=safetensors', '--output_name=mariamerced', '--lr_scheduler_num_cycles=10', '--max_data_loader_n_workers=0', '--learning_rate=0.0001', '--lr_scheduler=cosine', '--lr_warmup_steps=660', '--train_batch_size=1', '--max_train_steps=6600', '--save_every_n_epochs=1', '--mixed_precision=fp16', '--save_precision=fp16', '--optimizer_type=AdamW8bit', '--max_data_loader_n_workers=0', '--resume=B:/mariamerced/savess', '--bucket_reso_steps=64', '--save_state', '--mem_eff_attn', '--gradient_checkpointing', '--xformers', '--bucket_no_upscale', '--noise_offset=0.0']' returned non-zero exit status 1.",
                "reactions": {
                    "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1637/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "timeline_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1637/timeline",
                "performed_via_github_app": null,
                "state_reason": null
            }
        },
        "public": true,
        "created_at": "2023-10-26T22:40:49Z"
    },
    {
        "id": "32879326198",
        "type": "IssueCommentEvent",
        "actor": {
            "id": 37406615,
            "login": "iqddd",
            "display_login": "iqddd",
            "gravatar_id": "",
            "url": "https://api.github.com/users/iqddd",
            "avatar_url": "https://avatars.githubusercontent.com/u/37406615?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "created",
            "issue": {
                "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1236",
                "repository_url": "https://api.github.com/repos/bmaltais/kohya_ss",
                "labels_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1236/labels{/name}",
                "comments_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1236/comments",
                "events_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1236/events",
                "html_url": "https://github.com/bmaltais/kohya_ss/issues/1236",
                "id": 1814732108,
                "node_id": "I_kwDOIVqU3M5sKp1M",
                "number": 1236,
                "title": "Error while running BLIP captioning in Kohya_ss version kohya_ss-21.8.3",
                "user": {
                    "login": "jondoe231",
                    "id": 140114851,
                    "node_id": "U_kgDOCFn7ow",
                    "avatar_url": "https://avatars.githubusercontent.com/u/140114851?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/jondoe231",
                    "html_url": "https://github.com/jondoe231",
                    "followers_url": "https://api.github.com/users/jondoe231/followers",
                    "following_url": "https://api.github.com/users/jondoe231/following{/other_user}",
                    "gists_url": "https://api.github.com/users/jondoe231/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/jondoe231/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/jondoe231/subscriptions",
                    "organizations_url": "https://api.github.com/users/jondoe231/orgs",
                    "repos_url": "https://api.github.com/users/jondoe231/repos",
                    "events_url": "https://api.github.com/users/jondoe231/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/jondoe231/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "labels": [],
                "state": "open",
                "locked": false,
                "assignee": null,
                "assignees": [],
                "milestone": null,
                "comments": 17,
                "created_at": "2023-07-20T20:15:49Z",
                "updated_at": "2023-10-26T19:09:13Z",
                "closed_at": null,
                "author_association": "NONE",
                "active_lock_reason": null,
                "body": "I get the following error when trying to run BLIP captioning in Kohya_ss version kohya_ss-21.8.3. Please help.\r\n\r\n00:47:54-819080 INFO     Version: v21.8.3\r\n00:47:54-828177 INFO     nVidia toolkit detected\r\n00:47:56-805450 INFO     Torch 2.0.1+cu118\r\n00:47:56-881315 INFO     Torch backend: nVidia CUDA 11.8 cuDNN 8700\r\n00:47:56-883852 INFO     Torch detected GPU: NVIDIA GeForce GTX 1660 Ti VRAM 6144 Arch (7, 5) Cores 24\r\n00:47:56-884855 INFO     Verifying modules instalation status from requirements_windows_torch2.txt...\r\n00:47:56-891393 INFO     Verifying modules instalation status from requirements.txt...\r\n00:48:00-668941 INFO     headless: False\r\n00:48:00-668941 INFO     Load CSS...\r\nRunning on local URL:  http://127.0.0.1:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\n00:48:39-635297 INFO     Captioning files in D:/Data/kj/KJLora/img/25_mykajal woman...\r\n00:48:39-637337 INFO     ./venv/Scripts/python.exe \"finetune/make_captions.py\" --batch_size=\"1\" --num_beams=\"12\"\r\n                         --top_p=\"0.9\" --max_length=\"75\" --min_length=\"25\" --beam_search --caption_extension=\".txt\"\r\n                         \"D:/Data/kj/KJLora/img/25_mykajal woman\"\r\n                         --caption_weights=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/mode\r\n                         l_large_caption.pth\"\r\nA matching Triton is not available, some optimizations will not be enabled.\r\nError caught was: No module named 'triton'\r\nCurrent Working Directory is:  D:\\Data\\kohya\\kohya_ss-21.8.3\r\nload images from D:\\Data\\kj\\KJLora\\img\\25_mykajal woman\r\nfound 58 images.\r\nloading BLIP caption: https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\r\nload checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\r\nBLIP loaded\r\n  0%|                                                                                           | 0/58 [00:07<?, ?it/s]\r\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\finetune\\make_captions.py:200 in <module>                          \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   197 \u2502   if args.caption_extention is not None:                                                 \u2502\r\n\u2502   198 \u2502   \u2502   args.caption_extension = args.caption_extention                                    \u2502\r\n\u2502   199 \u2502                                                                                          \u2502\r\n\u2502 \u2771 200 \u2502   main(args)                                                                             \u2502\r\n\u2502   201                                                                                            \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\finetune\\make_captions.py:144 in main                              \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   141 \u2502   \u2502   \u2502                                                                                  \u2502\r\n\u2502   142 \u2502   \u2502   \u2502   b_imgs.append((image_path, img_tensor))                                        \u2502\r\n\u2502   143 \u2502   \u2502   \u2502   if len(b_imgs) >= args.batch_size:                                             \u2502\r\n\u2502 \u2771 144 \u2502   \u2502   \u2502   \u2502   run_batch(b_imgs)                                                          \u2502\r\n\u2502   145 \u2502   \u2502   \u2502   \u2502   b_imgs.clear()                                                             \u2502\r\n\u2502   146 \u2502   if len(b_imgs) > 0:                                                                    \u2502\r\n\u2502   147 \u2502   \u2502   run_batch(b_imgs)                                                                  \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\finetune\\make_captions.py:97 in run_batch                          \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502    94 \u2502   \u2502                                                                                      \u2502\r\n\u2502    95 \u2502   \u2502   with torch.no_grad():                                                              \u2502\r\n\u2502    96 \u2502   \u2502   \u2502   if args.beam_search:                                                           \u2502\r\n\u2502 \u2771  97 \u2502   \u2502   \u2502   \u2502   captions = model.generate(                                                 \u2502\r\n\u2502    98 \u2502   \u2502   \u2502   \u2502   \u2502   imgs, sample=False, num_beams=args.num_beams, max_length=args.max_le   \u2502\r\n\u2502    99 \u2502   \u2502   \u2502   \u2502   )                                                                          \u2502\r\n\u2502   100 \u2502   \u2502   \u2502   else:                                                                          \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\finetune\\blip\\blip.py:158 in generate                              \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   155 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502     **model_kwargs)                          \u2502\r\n\u2502   156 \u2502   \u2502   else:                                                                              \u2502\r\n\u2502   157 \u2502   \u2502   \u2502   #beam search                                                                   \u2502\r\n\u2502 \u2771 158 \u2502   \u2502   \u2502   outputs = self.text_decoder.generate(input_ids=input_ids,                      \u2502\r\n\u2502   159 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502     max_length=max_length,                   \u2502\r\n\u2502   160 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502     min_length=min_length,                   \u2502\r\n\u2502   161 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502     num_beams=num_beams,                     \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:115 in           \u2502\r\n\u2502 decorate_context                                                                                 \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   112 \u2502   @functools.wraps(func)                                                                 \u2502\r\n\u2502   113 \u2502   def decorate_context(*args, **kwargs):                                                 \u2502\r\n\u2502   114 \u2502   \u2502   with ctx_factory():                                                                \u2502\r\n\u2502 \u2771 115 \u2502   \u2502   \u2502   return func(*args, **kwargs)                                                   \u2502\r\n\u2502   116 \u2502                                                                                          \u2502\r\n\u2502   117 \u2502   return decorate_context                                                                \u2502\r\n\u2502   118                                                                                            \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1611 in    \u2502\r\n\u2502 generate                                                                                         \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1608 \u2502   \u2502   \u2502   \u2502   **model_kwargs,                                                           \u2502\r\n\u2502   1609 \u2502   \u2502   \u2502   )                                                                             \u2502\r\n\u2502   1610 \u2502   \u2502   \u2502   # 13. run beam search                                                         \u2502\r\n\u2502 \u2771 1611 \u2502   \u2502   \u2502   return self.beam_search(                                                      \u2502\r\n\u2502   1612 \u2502   \u2502   \u2502   \u2502   input_ids,                                                                \u2502\r\n\u2502   1613 \u2502   \u2502   \u2502   \u2502   beam_scorer,                                                              \u2502\r\n\u2502   1614 \u2502   \u2502   \u2502   \u2502   logits_processor=logits_processor,                                        \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:2909 in    \u2502\r\n\u2502 beam_search                                                                                      \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   2906 \u2502   \u2502   \u2502                                                                                 \u2502\r\n\u2502   2907 \u2502   \u2502   \u2502   model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)  \u2502\r\n\u2502   2908 \u2502   \u2502   \u2502                                                                                 \u2502\r\n\u2502 \u2771 2909 \u2502   \u2502   \u2502   outputs = self(                                                               \u2502\r\n\u2502   2910 \u2502   \u2502   \u2502   \u2502   **model_inputs,                                                           \u2502\r\n\u2502   2911 \u2502   \u2502   \u2502   \u2502   return_dict=True,                                                         \u2502\r\n\u2502   2912 \u2502   \u2502   \u2502   \u2502   output_attentions=output_attentions,                                      \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501 in          \u2502\r\n\u2502 _call_impl                                                                                       \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks or _global_backward_hooks                   \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):                   \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)                                          \u2502\r\n\u2502   1502 \u2502   \u2502   # Do not call functions when jit is used                                          \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks = [], []                             \u2502\r\n\u2502   1504 \u2502   \u2502   backward_pre_hooks = []                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\finetune\\blip\\med.py:886 in forward                                \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   883 \u2502   \u2502   if labels is not None:                                                             \u2502\r\n\u2502   884 \u2502   \u2502   \u2502   use_cache = False                                                              \u2502\r\n\u2502   885 \u2502   \u2502                                                                                      \u2502\r\n\u2502 \u2771 886 \u2502   \u2502   outputs = self.bert(                                                               \u2502\r\n\u2502   887 \u2502   \u2502   \u2502   input_ids,                                                                     \u2502\r\n\u2502   888 \u2502   \u2502   \u2502   attention_mask=attention_mask,                                                 \u2502\r\n\u2502   889 \u2502   \u2502   \u2502   position_ids=position_ids,                                                     \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501 in          \u2502\r\n\u2502 _call_impl                                                                                       \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks or _global_backward_hooks                   \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):                   \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)                                          \u2502\r\n\u2502   1502 \u2502   \u2502   # Do not call functions when jit is used                                          \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks = [], []                             \u2502\r\n\u2502   1504 \u2502   \u2502   backward_pre_hooks = []                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\finetune\\blip\\med.py:781 in forward                                \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   778 \u2502   \u2502   else:                                                                              \u2502\r\n\u2502   779 \u2502   \u2502   \u2502   embedding_output = encoder_embeds                                              \u2502\r\n\u2502   780 \u2502   \u2502                                                                                      \u2502\r\n\u2502 \u2771 781 \u2502   \u2502   encoder_outputs = self.encoder(                                                    \u2502\r\n\u2502   782 \u2502   \u2502   \u2502   embedding_output,                                                              \u2502\r\n\u2502   783 \u2502   \u2502   \u2502   attention_mask=extended_attention_mask,                                        \u2502\r\n\u2502   784 \u2502   \u2502   \u2502   head_mask=head_mask,                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501 in          \u2502\r\n\u2502 _call_impl                                                                                       \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks or _global_backward_hooks                   \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):                   \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)                                          \u2502\r\n\u2502   1502 \u2502   \u2502   # Do not call functions when jit is used                                          \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks = [], []                             \u2502\r\n\u2502   1504 \u2502   \u2502   backward_pre_hooks = []                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\finetune\\blip\\med.py:445 in forward                                \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   442 \u2502   \u2502   \u2502   \u2502   \u2502   mode=mode,                                                             \u2502\r\n\u2502   443 \u2502   \u2502   \u2502   \u2502   )                                                                          \u2502\r\n\u2502   444 \u2502   \u2502   \u2502   else:                                                                          \u2502\r\n\u2502 \u2771 445 \u2502   \u2502   \u2502   \u2502   layer_outputs = layer_module(                                              \u2502\r\n\u2502   446 \u2502   \u2502   \u2502   \u2502   \u2502   hidden_states,                                                         \u2502\r\n\u2502   447 \u2502   \u2502   \u2502   \u2502   \u2502   attention_mask,                                                        \u2502\r\n\u2502   448 \u2502   \u2502   \u2502   \u2502   \u2502   layer_head_mask,                                                       \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501 in          \u2502\r\n\u2502 _call_impl                                                                                       \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks or _global_backward_hooks                   \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):                   \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)                                          \u2502\r\n\u2502   1502 \u2502   \u2502   # Do not call functions when jit is used                                          \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks = [], []                             \u2502\r\n\u2502   1504 \u2502   \u2502   backward_pre_hooks = []                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\finetune\\blip\\med.py:361 in forward                                \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   358 \u2502   \u2502   if mode=='multimodal':                                                             \u2502\r\n\u2502   359 \u2502   \u2502   \u2502   assert encoder_hidden_states is not None, \"encoder_hidden_states must be giv   \u2502\r\n\u2502   360 \u2502   \u2502   \u2502                                                                                  \u2502\r\n\u2502 \u2771 361 \u2502   \u2502   \u2502   cross_attention_outputs = self.crossattention(                                 \u2502\r\n\u2502   362 \u2502   \u2502   \u2502   \u2502   attention_output,                                                          \u2502\r\n\u2502   363 \u2502   \u2502   \u2502   \u2502   attention_mask,                                                            \u2502\r\n\u2502   364 \u2502   \u2502   \u2502   \u2502   head_mask,                                                                 \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501 in          \u2502\r\n\u2502 _call_impl                                                                                       \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks or _global_backward_hooks                   \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):                   \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)                                          \u2502\r\n\u2502   1502 \u2502   \u2502   # Do not call functions when jit is used                                          \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks = [], []                             \u2502\r\n\u2502   1504 \u2502   \u2502   backward_pre_hooks = []                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\finetune\\blip\\med.py:277 in forward                                \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   274 \u2502   \u2502   past_key_value=None,                                                               \u2502\r\n\u2502   275 \u2502   \u2502   output_attentions=False,                                                           \u2502\r\n\u2502   276 \u2502   ):                                                                                     \u2502\r\n\u2502 \u2771 277 \u2502   \u2502   self_outputs = self.self(                                                          \u2502\r\n\u2502   278 \u2502   \u2502   \u2502   hidden_states,                                                                 \u2502\r\n\u2502   279 \u2502   \u2502   \u2502   attention_mask,                                                                \u2502\r\n\u2502   280 \u2502   \u2502   \u2502   head_mask,                                                                     \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501 in          \u2502\r\n\u2502 _call_impl                                                                                       \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks or _global_backward_hooks                   \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):                   \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)                                          \u2502\r\n\u2502   1502 \u2502   \u2502   # Do not call functions when jit is used                                          \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks = [], []                             \u2502\r\n\u2502   1504 \u2502   \u2502   backward_pre_hooks = []                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\finetune\\blip\\med.py:178 in forward                                \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   175 \u2502   \u2502   past_key_value = (key_layer, value_layer)                                          \u2502\r\n\u2502   176 \u2502   \u2502                                                                                      \u2502\r\n\u2502   177 \u2502   \u2502   # Take the dot product between \"query\" and \"key\" to get the raw attention scores   \u2502\r\n\u2502 \u2771 178 \u2502   \u2502   attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))          \u2502\r\n\u2502   179 \u2502   \u2502                                                                                      \u2502\r\n\u2502   180 \u2502   \u2502   if self.position_embedding_type == \"relative_key\" or self.position_embedding_typ   \u2502\r\n\u2502   181 \u2502   \u2502   \u2502   seq_length = hidden_states.size()[1]                                           \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\nRuntimeError: The size of tensor a (12) must match the size of tensor b (144) at non-singleton dimension 0\r\n00:49:08-456631 INFO     ...captioning done\r\n\r\n",
                "reactions": {
                    "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1236/reactions",
                    "total_count": 2,
                    "+1": 2,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "timeline_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1236/timeline",
                "performed_via_github_app": null,
                "state_reason": null
            },
            "comment": {
                "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/comments/1781746677",
                "html_url": "https://github.com/bmaltais/kohya_ss/issues/1236#issuecomment-1781746677",
                "issue_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1236",
                "id": 1781746677,
                "node_id": "IC_kwDOIVqU3M5qM0v1",
                "user": {
                    "login": "iqddd",
                    "id": 37406615,
                    "node_id": "MDQ6VXNlcjM3NDA2NjE1",
                    "avatar_url": "https://avatars.githubusercontent.com/u/37406615?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/iqddd",
                    "html_url": "https://github.com/iqddd",
                    "followers_url": "https://api.github.com/users/iqddd/followers",
                    "following_url": "https://api.github.com/users/iqddd/following{/other_user}",
                    "gists_url": "https://api.github.com/users/iqddd/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/iqddd/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/iqddd/subscriptions",
                    "organizations_url": "https://api.github.com/users/iqddd/orgs",
                    "repos_url": "https://api.github.com/users/iqddd/repos",
                    "events_url": "https://api.github.com/users/iqddd/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/iqddd/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "created_at": "2023-10-26T19:09:12Z",
                "updated_at": "2023-10-26T19:09:12Z",
                "author_association": "NONE",
                "body": "Downgrade is by definition a so-so solution.",
                "reactions": {
                    "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/comments/1781746677/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "performed_via_github_app": null
            }
        },
        "public": true,
        "created_at": "2023-10-26T19:09:13Z"
    },
    {
        "id": "32879169975",
        "type": "IssueCommentEvent",
        "actor": {
            "id": 42456814,
            "login": "coderCK2",
            "display_login": "coderCK2",
            "gravatar_id": "",
            "url": "https://api.github.com/users/coderCK2",
            "avatar_url": "https://avatars.githubusercontent.com/u/42456814?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "created",
            "issue": {
                "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1236",
                "repository_url": "https://api.github.com/repos/bmaltais/kohya_ss",
                "labels_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1236/labels{/name}",
                "comments_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1236/comments",
                "events_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1236/events",
                "html_url": "https://github.com/bmaltais/kohya_ss/issues/1236",
                "id": 1814732108,
                "node_id": "I_kwDOIVqU3M5sKp1M",
                "number": 1236,
                "title": "Error while running BLIP captioning in Kohya_ss version kohya_ss-21.8.3",
                "user": {
                    "login": "jondoe231",
                    "id": 140114851,
                    "node_id": "U_kgDOCFn7ow",
                    "avatar_url": "https://avatars.githubusercontent.com/u/140114851?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/jondoe231",
                    "html_url": "https://github.com/jondoe231",
                    "followers_url": "https://api.github.com/users/jondoe231/followers",
                    "following_url": "https://api.github.com/users/jondoe231/following{/other_user}",
                    "gists_url": "https://api.github.com/users/jondoe231/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/jondoe231/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/jondoe231/subscriptions",
                    "organizations_url": "https://api.github.com/users/jondoe231/orgs",
                    "repos_url": "https://api.github.com/users/jondoe231/repos",
                    "events_url": "https://api.github.com/users/jondoe231/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/jondoe231/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "labels": [],
                "state": "open",
                "locked": false,
                "assignee": null,
                "assignees": [],
                "milestone": null,
                "comments": 16,
                "created_at": "2023-07-20T20:15:49Z",
                "updated_at": "2023-10-26T19:02:34Z",
                "closed_at": null,
                "author_association": "NONE",
                "active_lock_reason": null,
                "body": "I get the following error when trying to run BLIP captioning in Kohya_ss version kohya_ss-21.8.3. Please help.\r\n\r\n00:47:54-819080 INFO     Version: v21.8.3\r\n00:47:54-828177 INFO     nVidia toolkit detected\r\n00:47:56-805450 INFO     Torch 2.0.1+cu118\r\n00:47:56-881315 INFO     Torch backend: nVidia CUDA 11.8 cuDNN 8700\r\n00:47:56-883852 INFO     Torch detected GPU: NVIDIA GeForce GTX 1660 Ti VRAM 6144 Arch (7, 5) Cores 24\r\n00:47:56-884855 INFO     Verifying modules instalation status from requirements_windows_torch2.txt...\r\n00:47:56-891393 INFO     Verifying modules instalation status from requirements.txt...\r\n00:48:00-668941 INFO     headless: False\r\n00:48:00-668941 INFO     Load CSS...\r\nRunning on local URL:  http://127.0.0.1:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\n00:48:39-635297 INFO     Captioning files in D:/Data/kj/KJLora/img/25_mykajal woman...\r\n00:48:39-637337 INFO     ./venv/Scripts/python.exe \"finetune/make_captions.py\" --batch_size=\"1\" --num_beams=\"12\"\r\n                         --top_p=\"0.9\" --max_length=\"75\" --min_length=\"25\" --beam_search --caption_extension=\".txt\"\r\n                         \"D:/Data/kj/KJLora/img/25_mykajal woman\"\r\n                         --caption_weights=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/mode\r\n                         l_large_caption.pth\"\r\nA matching Triton is not available, some optimizations will not be enabled.\r\nError caught was: No module named 'triton'\r\nCurrent Working Directory is:  D:\\Data\\kohya\\kohya_ss-21.8.3\r\nload images from D:\\Data\\kj\\KJLora\\img\\25_mykajal woman\r\nfound 58 images.\r\nloading BLIP caption: https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\r\nload checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\r\nBLIP loaded\r\n  0%|                                                                                           | 0/58 [00:07<?, ?it/s]\r\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\finetune\\make_captions.py:200 in <module>                          \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   197 \u2502   if args.caption_extention is not None:                                                 \u2502\r\n\u2502   198 \u2502   \u2502   args.caption_extension = args.caption_extention                                    \u2502\r\n\u2502   199 \u2502                                                                                          \u2502\r\n\u2502 \u2771 200 \u2502   main(args)                                                                             \u2502\r\n\u2502   201                                                                                            \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\finetune\\make_captions.py:144 in main                              \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   141 \u2502   \u2502   \u2502                                                                                  \u2502\r\n\u2502   142 \u2502   \u2502   \u2502   b_imgs.append((image_path, img_tensor))                                        \u2502\r\n\u2502   143 \u2502   \u2502   \u2502   if len(b_imgs) >= args.batch_size:                                             \u2502\r\n\u2502 \u2771 144 \u2502   \u2502   \u2502   \u2502   run_batch(b_imgs)                                                          \u2502\r\n\u2502   145 \u2502   \u2502   \u2502   \u2502   b_imgs.clear()                                                             \u2502\r\n\u2502   146 \u2502   if len(b_imgs) > 0:                                                                    \u2502\r\n\u2502   147 \u2502   \u2502   run_batch(b_imgs)                                                                  \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\finetune\\make_captions.py:97 in run_batch                          \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502    94 \u2502   \u2502                                                                                      \u2502\r\n\u2502    95 \u2502   \u2502   with torch.no_grad():                                                              \u2502\r\n\u2502    96 \u2502   \u2502   \u2502   if args.beam_search:                                                           \u2502\r\n\u2502 \u2771  97 \u2502   \u2502   \u2502   \u2502   captions = model.generate(                                                 \u2502\r\n\u2502    98 \u2502   \u2502   \u2502   \u2502   \u2502   imgs, sample=False, num_beams=args.num_beams, max_length=args.max_le   \u2502\r\n\u2502    99 \u2502   \u2502   \u2502   \u2502   )                                                                          \u2502\r\n\u2502   100 \u2502   \u2502   \u2502   else:                                                                          \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\finetune\\blip\\blip.py:158 in generate                              \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   155 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502     **model_kwargs)                          \u2502\r\n\u2502   156 \u2502   \u2502   else:                                                                              \u2502\r\n\u2502   157 \u2502   \u2502   \u2502   #beam search                                                                   \u2502\r\n\u2502 \u2771 158 \u2502   \u2502   \u2502   outputs = self.text_decoder.generate(input_ids=input_ids,                      \u2502\r\n\u2502   159 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502     max_length=max_length,                   \u2502\r\n\u2502   160 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502     min_length=min_length,                   \u2502\r\n\u2502   161 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502     num_beams=num_beams,                     \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:115 in           \u2502\r\n\u2502 decorate_context                                                                                 \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   112 \u2502   @functools.wraps(func)                                                                 \u2502\r\n\u2502   113 \u2502   def decorate_context(*args, **kwargs):                                                 \u2502\r\n\u2502   114 \u2502   \u2502   with ctx_factory():                                                                \u2502\r\n\u2502 \u2771 115 \u2502   \u2502   \u2502   return func(*args, **kwargs)                                                   \u2502\r\n\u2502   116 \u2502                                                                                          \u2502\r\n\u2502   117 \u2502   return decorate_context                                                                \u2502\r\n\u2502   118                                                                                            \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1611 in    \u2502\r\n\u2502 generate                                                                                         \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1608 \u2502   \u2502   \u2502   \u2502   **model_kwargs,                                                           \u2502\r\n\u2502   1609 \u2502   \u2502   \u2502   )                                                                             \u2502\r\n\u2502   1610 \u2502   \u2502   \u2502   # 13. run beam search                                                         \u2502\r\n\u2502 \u2771 1611 \u2502   \u2502   \u2502   return self.beam_search(                                                      \u2502\r\n\u2502   1612 \u2502   \u2502   \u2502   \u2502   input_ids,                                                                \u2502\r\n\u2502   1613 \u2502   \u2502   \u2502   \u2502   beam_scorer,                                                              \u2502\r\n\u2502   1614 \u2502   \u2502   \u2502   \u2502   logits_processor=logits_processor,                                        \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:2909 in    \u2502\r\n\u2502 beam_search                                                                                      \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   2906 \u2502   \u2502   \u2502                                                                                 \u2502\r\n\u2502   2907 \u2502   \u2502   \u2502   model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)  \u2502\r\n\u2502   2908 \u2502   \u2502   \u2502                                                                                 \u2502\r\n\u2502 \u2771 2909 \u2502   \u2502   \u2502   outputs = self(                                                               \u2502\r\n\u2502   2910 \u2502   \u2502   \u2502   \u2502   **model_inputs,                                                           \u2502\r\n\u2502   2911 \u2502   \u2502   \u2502   \u2502   return_dict=True,                                                         \u2502\r\n\u2502   2912 \u2502   \u2502   \u2502   \u2502   output_attentions=output_attentions,                                      \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501 in          \u2502\r\n\u2502 _call_impl                                                                                       \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks or _global_backward_hooks                   \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):                   \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)                                          \u2502\r\n\u2502   1502 \u2502   \u2502   # Do not call functions when jit is used                                          \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks = [], []                             \u2502\r\n\u2502   1504 \u2502   \u2502   backward_pre_hooks = []                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\finetune\\blip\\med.py:886 in forward                                \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   883 \u2502   \u2502   if labels is not None:                                                             \u2502\r\n\u2502   884 \u2502   \u2502   \u2502   use_cache = False                                                              \u2502\r\n\u2502   885 \u2502   \u2502                                                                                      \u2502\r\n\u2502 \u2771 886 \u2502   \u2502   outputs = self.bert(                                                               \u2502\r\n\u2502   887 \u2502   \u2502   \u2502   input_ids,                                                                     \u2502\r\n\u2502   888 \u2502   \u2502   \u2502   attention_mask=attention_mask,                                                 \u2502\r\n\u2502   889 \u2502   \u2502   \u2502   position_ids=position_ids,                                                     \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501 in          \u2502\r\n\u2502 _call_impl                                                                                       \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks or _global_backward_hooks                   \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):                   \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)                                          \u2502\r\n\u2502   1502 \u2502   \u2502   # Do not call functions when jit is used                                          \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks = [], []                             \u2502\r\n\u2502   1504 \u2502   \u2502   backward_pre_hooks = []                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\finetune\\blip\\med.py:781 in forward                                \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   778 \u2502   \u2502   else:                                                                              \u2502\r\n\u2502   779 \u2502   \u2502   \u2502   embedding_output = encoder_embeds                                              \u2502\r\n\u2502   780 \u2502   \u2502                                                                                      \u2502\r\n\u2502 \u2771 781 \u2502   \u2502   encoder_outputs = self.encoder(                                                    \u2502\r\n\u2502   782 \u2502   \u2502   \u2502   embedding_output,                                                              \u2502\r\n\u2502   783 \u2502   \u2502   \u2502   attention_mask=extended_attention_mask,                                        \u2502\r\n\u2502   784 \u2502   \u2502   \u2502   head_mask=head_mask,                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501 in          \u2502\r\n\u2502 _call_impl                                                                                       \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks or _global_backward_hooks                   \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):                   \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)                                          \u2502\r\n\u2502   1502 \u2502   \u2502   # Do not call functions when jit is used                                          \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks = [], []                             \u2502\r\n\u2502   1504 \u2502   \u2502   backward_pre_hooks = []                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\finetune\\blip\\med.py:445 in forward                                \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   442 \u2502   \u2502   \u2502   \u2502   \u2502   mode=mode,                                                             \u2502\r\n\u2502   443 \u2502   \u2502   \u2502   \u2502   )                                                                          \u2502\r\n\u2502   444 \u2502   \u2502   \u2502   else:                                                                          \u2502\r\n\u2502 \u2771 445 \u2502   \u2502   \u2502   \u2502   layer_outputs = layer_module(                                              \u2502\r\n\u2502   446 \u2502   \u2502   \u2502   \u2502   \u2502   hidden_states,                                                         \u2502\r\n\u2502   447 \u2502   \u2502   \u2502   \u2502   \u2502   attention_mask,                                                        \u2502\r\n\u2502   448 \u2502   \u2502   \u2502   \u2502   \u2502   layer_head_mask,                                                       \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501 in          \u2502\r\n\u2502 _call_impl                                                                                       \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks or _global_backward_hooks                   \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):                   \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)                                          \u2502\r\n\u2502   1502 \u2502   \u2502   # Do not call functions when jit is used                                          \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks = [], []                             \u2502\r\n\u2502   1504 \u2502   \u2502   backward_pre_hooks = []                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\finetune\\blip\\med.py:361 in forward                                \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   358 \u2502   \u2502   if mode=='multimodal':                                                             \u2502\r\n\u2502   359 \u2502   \u2502   \u2502   assert encoder_hidden_states is not None, \"encoder_hidden_states must be giv   \u2502\r\n\u2502   360 \u2502   \u2502   \u2502                                                                                  \u2502\r\n\u2502 \u2771 361 \u2502   \u2502   \u2502   cross_attention_outputs = self.crossattention(                                 \u2502\r\n\u2502   362 \u2502   \u2502   \u2502   \u2502   attention_output,                                                          \u2502\r\n\u2502   363 \u2502   \u2502   \u2502   \u2502   attention_mask,                                                            \u2502\r\n\u2502   364 \u2502   \u2502   \u2502   \u2502   head_mask,                                                                 \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501 in          \u2502\r\n\u2502 _call_impl                                                                                       \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks or _global_backward_hooks                   \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):                   \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)                                          \u2502\r\n\u2502   1502 \u2502   \u2502   # Do not call functions when jit is used                                          \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks = [], []                             \u2502\r\n\u2502   1504 \u2502   \u2502   backward_pre_hooks = []                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\finetune\\blip\\med.py:277 in forward                                \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   274 \u2502   \u2502   past_key_value=None,                                                               \u2502\r\n\u2502   275 \u2502   \u2502   output_attentions=False,                                                           \u2502\r\n\u2502   276 \u2502   ):                                                                                     \u2502\r\n\u2502 \u2771 277 \u2502   \u2502   self_outputs = self.self(                                                          \u2502\r\n\u2502   278 \u2502   \u2502   \u2502   hidden_states,                                                                 \u2502\r\n\u2502   279 \u2502   \u2502   \u2502   attention_mask,                                                                \u2502\r\n\u2502   280 \u2502   \u2502   \u2502   head_mask,                                                                     \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501 in          \u2502\r\n\u2502 _call_impl                                                                                       \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks or _global_backward_hooks                   \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):                   \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)                                          \u2502\r\n\u2502   1502 \u2502   \u2502   # Do not call functions when jit is used                                          \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks = [], []                             \u2502\r\n\u2502   1504 \u2502   \u2502   backward_pre_hooks = []                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 D:\\Data\\kohya\\kohya_ss-21.8.3\\finetune\\blip\\med.py:178 in forward                                \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   175 \u2502   \u2502   past_key_value = (key_layer, value_layer)                                          \u2502\r\n\u2502   176 \u2502   \u2502                                                                                      \u2502\r\n\u2502   177 \u2502   \u2502   # Take the dot product between \"query\" and \"key\" to get the raw attention scores   \u2502\r\n\u2502 \u2771 178 \u2502   \u2502   attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))          \u2502\r\n\u2502   179 \u2502   \u2502                                                                                      \u2502\r\n\u2502   180 \u2502   \u2502   if self.position_embedding_type == \"relative_key\" or self.position_embedding_typ   \u2502\r\n\u2502   181 \u2502   \u2502   \u2502   seq_length = hidden_states.size()[1]                                           \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\nRuntimeError: The size of tensor a (12) must match the size of tensor b (144) at non-singleton dimension 0\r\n00:49:08-456631 INFO     ...captioning done\r\n\r\n",
                "reactions": {
                    "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1236/reactions",
                    "total_count": 2,
                    "+1": 2,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "timeline_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1236/timeline",
                "performed_via_github_app": null,
                "state_reason": null
            },
            "comment": {
                "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/comments/1781735410",
                "html_url": "https://github.com/bmaltais/kohya_ss/issues/1236#issuecomment-1781735410",
                "issue_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1236",
                "id": 1781735410,
                "node_id": "IC_kwDOIVqU3M5qMx_y",
                "user": {
                    "login": "coderCK2",
                    "id": 42456814,
                    "node_id": "MDQ6VXNlcjQyNDU2ODE0",
                    "avatar_url": "https://avatars.githubusercontent.com/u/42456814?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/coderCK2",
                    "html_url": "https://github.com/coderCK2",
                    "followers_url": "https://api.github.com/users/coderCK2/followers",
                    "following_url": "https://api.github.com/users/coderCK2/following{/other_user}",
                    "gists_url": "https://api.github.com/users/coderCK2/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/coderCK2/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/coderCK2/subscriptions",
                    "organizations_url": "https://api.github.com/users/coderCK2/orgs",
                    "repos_url": "https://api.github.com/users/coderCK2/repos",
                    "events_url": "https://api.github.com/users/coderCK2/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/coderCK2/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "created_at": "2023-10-26T19:02:33Z",
                "updated_at": "2023-10-26T19:02:33Z",
                "author_association": "NONE",
                "body": "> In requirements.txt, change `transformers==x.x.x` to `transformers==4.25.1`. You may have to run `pip install -r requirements.txt` as well. This will fix the issue by downgrading transformers to a working version.\r\n\r\nThank you for this! This issue is still happening, and your solution worked.",
                "reactions": {
                    "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/comments/1781735410/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "performed_via_github_app": null
            }
        },
        "public": true,
        "created_at": "2023-10-26T19:02:34Z"
    },
    {
        "id": "32874262813",
        "type": "WatchEvent",
        "actor": {
            "id": 47277141,
            "login": "Disty0",
            "display_login": "Disty0",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Disty0",
            "avatar_url": "https://avatars.githubusercontent.com/u/47277141?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-26T15:49:40Z"
    },
    {
        "id": "32873554490",
        "type": "WatchEvent",
        "actor": {
            "id": 111529329,
            "login": "luisdonginseo",
            "display_login": "luisdonginseo",
            "gravatar_id": "",
            "url": "https://api.github.com/users/luisdonginseo",
            "avatar_url": "https://avatars.githubusercontent.com/u/111529329?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-26T15:26:04Z"
    },
    {
        "id": "32871131732",
        "type": "IssueCommentEvent",
        "actor": {
            "id": 85022330,
            "login": "suede299",
            "display_login": "suede299",
            "gravatar_id": "",
            "url": "https://api.github.com/users/suede299",
            "avatar_url": "https://avatars.githubusercontent.com/u/85022330?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "created",
            "issue": {
                "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1636",
                "repository_url": "https://api.github.com/repos/bmaltais/kohya_ss",
                "labels_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1636/labels{/name}",
                "comments_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1636/comments",
                "events_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1636/events",
                "html_url": "https://github.com/bmaltais/kohya_ss/issues/1636",
                "id": 1962657006,
                "node_id": "I_kwDOIVqU3M50-8Tu",
                "number": 1636,
                "title": "Parameter settings of UI seems random, if not can someone please explain how the parameters are set especially the # epoch",
                "user": {
                    "login": "andrewtvuong",
                    "id": 15879426,
                    "node_id": "MDQ6VXNlcjE1ODc5NDI2",
                    "avatar_url": "https://avatars.githubusercontent.com/u/15879426?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/andrewtvuong",
                    "html_url": "https://github.com/andrewtvuong",
                    "followers_url": "https://api.github.com/users/andrewtvuong/followers",
                    "following_url": "https://api.github.com/users/andrewtvuong/following{/other_user}",
                    "gists_url": "https://api.github.com/users/andrewtvuong/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/andrewtvuong/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/andrewtvuong/subscriptions",
                    "organizations_url": "https://api.github.com/users/andrewtvuong/orgs",
                    "repos_url": "https://api.github.com/users/andrewtvuong/repos",
                    "events_url": "https://api.github.com/users/andrewtvuong/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/andrewtvuong/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "labels": [],
                "state": "open",
                "locked": false,
                "assignee": null,
                "assignees": [],
                "milestone": null,
                "comments": 1,
                "created_at": "2023-10-26T03:58:17Z",
                "updated_at": "2023-10-26T14:13:41Z",
                "closed_at": null,
                "author_association": "NONE",
                "active_lock_reason": null,
                "body": "![395577580_1251501482183311_6058618631504669907_n](https://github.com/bmaltais/kohya_ss/assets/15879426/0e431e98-9f10-4e61-9f6a-dd3a0ea8c06d)\r\n\r\nEverything has been retried and values changed to ensure not a one off case.\r\n\r\nFor example, on the UI I set for 5 epoch, but somehow I end up with 350 epoch, not sure how this value is determined.\r\n\r\nI have 150 images and its .txt caption placed in a folder that's named 5_apple. This should mean 5 repeated steps for 150 images for 5 epoch for a total of 3750 total steps, but somehow I have 261875 total steps.\r\n\r\nMy GPU is a 3070ti with 8gb vram, Driver 545.84, CUDA 12.3, with CUDNN optimization.\r\n\r\nWith a smaller image dataset of 15 files and 20 repeated steps the epoch parameter on the UI seems to respond correctly.\r\nI just don't understand where the number 350 epoch is coming from, not divisible into any parameters I actually inputted.\r\n\r\nThanks! I saw a similar issue here https://github.com/bmaltais/kohya_ss/issues/1618.",
                "reactions": {
                    "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1636/reactions",
                    "total_count": 1,
                    "+1": 1,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "timeline_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1636/timeline",
                "performed_via_github_app": null,
                "state_reason": null
            },
            "comment": {
                "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/comments/1781217830",
                "html_url": "https://github.com/bmaltais/kohya_ss/issues/1636#issuecomment-1781217830",
                "issue_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1636",
                "id": 1781217830,
                "node_id": "IC_kwDOIVqU3M5qKzom",
                "user": {
                    "login": "suede299",
                    "id": 85022330,
                    "node_id": "MDQ6VXNlcjg1MDIyMzMw",
                    "avatar_url": "https://avatars.githubusercontent.com/u/85022330?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/suede299",
                    "html_url": "https://github.com/suede299",
                    "followers_url": "https://api.github.com/users/suede299/followers",
                    "following_url": "https://api.github.com/users/suede299/following{/other_user}",
                    "gists_url": "https://api.github.com/users/suede299/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/suede299/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/suede299/subscriptions",
                    "organizations_url": "https://api.github.com/users/suede299/orgs",
                    "repos_url": "https://api.github.com/users/suede299/repos",
                    "events_url": "https://api.github.com/users/suede299/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/suede299/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "created_at": "2023-10-26T14:13:41Z",
                "updated_at": "2023-10-26T14:13:41Z",
                "author_association": "NONE",
                "body": "Is it possible that it's a problem with the versions of the various dependencies? I'm using pytorch 2.1+cu121 and today also set 64epoch and ended up running 56epoch. but mine only has this one parameter different from the setup.",
                "reactions": {
                    "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/comments/1781217830/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "performed_via_github_app": null
            }
        },
        "public": true,
        "created_at": "2023-10-26T14:13:41Z"
    },
    {
        "id": "32870217911",
        "type": "WatchEvent",
        "actor": {
            "id": 1930508,
            "login": "wing7",
            "display_login": "wing7",
            "gravatar_id": "",
            "url": "https://api.github.com/users/wing7",
            "avatar_url": "https://avatars.githubusercontent.com/u/1930508?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-26T13:47:08Z"
    },
    {
        "id": "32868444604",
        "type": "WatchEvent",
        "actor": {
            "id": 2506140,
            "login": "LittleCodingFox",
            "display_login": "LittleCodingFox",
            "gravatar_id": "",
            "url": "https://api.github.com/users/LittleCodingFox",
            "avatar_url": "https://avatars.githubusercontent.com/u/2506140?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-26T12:54:18Z"
    },
    {
        "id": "32867557755",
        "type": "ForkEvent",
        "actor": {
            "id": 131157896,
            "login": "zhaoganglxh",
            "display_login": "zhaoganglxh",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zhaoganglxh",
            "avatar_url": "https://avatars.githubusercontent.com/u/131157896?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "forkee": {
                "id": 710300344,
                "node_id": "R_kgDOKlZSuA",
                "name": "kohya_ss",
                "full_name": "zhaoganglxh/kohya_ss",
                "private": false,
                "owner": {
                    "login": "zhaoganglxh",
                    "id": 131157896,
                    "node_id": "U_kgDOB9FPiA",
                    "avatar_url": "https://avatars.githubusercontent.com/u/131157896?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/zhaoganglxh",
                    "html_url": "https://github.com/zhaoganglxh",
                    "followers_url": "https://api.github.com/users/zhaoganglxh/followers",
                    "following_url": "https://api.github.com/users/zhaoganglxh/following{/other_user}",
                    "gists_url": "https://api.github.com/users/zhaoganglxh/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/zhaoganglxh/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/zhaoganglxh/subscriptions",
                    "organizations_url": "https://api.github.com/users/zhaoganglxh/orgs",
                    "repos_url": "https://api.github.com/users/zhaoganglxh/repos",
                    "events_url": "https://api.github.com/users/zhaoganglxh/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/zhaoganglxh/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "html_url": "https://github.com/zhaoganglxh/kohya_ss",
                "description": null,
                "fork": true,
                "url": "https://api.github.com/repos/zhaoganglxh/kohya_ss",
                "forks_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/forks",
                "keys_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/keys{/key_id}",
                "collaborators_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/collaborators{/collaborator}",
                "teams_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/teams",
                "hooks_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/hooks",
                "issue_events_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/issues/events{/number}",
                "events_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/events",
                "assignees_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/assignees{/user}",
                "branches_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/branches{/branch}",
                "tags_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/tags",
                "blobs_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/git/blobs{/sha}",
                "git_tags_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/git/tags{/sha}",
                "git_refs_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/git/refs{/sha}",
                "trees_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/git/trees{/sha}",
                "statuses_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/statuses/{sha}",
                "languages_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/languages",
                "stargazers_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/stargazers",
                "contributors_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/contributors",
                "subscribers_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/subscribers",
                "subscription_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/subscription",
                "commits_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/commits{/sha}",
                "git_commits_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/git/commits{/sha}",
                "comments_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/comments{/number}",
                "issue_comment_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/issues/comments{/number}",
                "contents_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/contents/{+path}",
                "compare_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/compare/{base}...{head}",
                "merges_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/merges",
                "archive_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/{archive_format}{/ref}",
                "downloads_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/downloads",
                "issues_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/issues{/number}",
                "pulls_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/pulls{/number}",
                "milestones_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/milestones{/number}",
                "notifications_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/notifications{?since,all,participating}",
                "labels_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/labels{/name}",
                "releases_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/releases{/id}",
                "deployments_url": "https://api.github.com/repos/zhaoganglxh/kohya_ss/deployments",
                "created_at": "2023-10-26T12:25:53Z",
                "updated_at": "2023-10-26T12:25:53Z",
                "pushed_at": "2023-10-25T15:32:35Z",
                "git_url": "git://github.com/zhaoganglxh/kohya_ss.git",
                "ssh_url": "git@github.com:zhaoganglxh/kohya_ss.git",
                "clone_url": "https://github.com/zhaoganglxh/kohya_ss.git",
                "svn_url": "https://github.com/zhaoganglxh/kohya_ss",
                "homepage": null,
                "size": 11763,
                "stargazers_count": 0,
                "watchers_count": 0,
                "language": null,
                "has_issues": false,
                "has_projects": true,
                "has_downloads": true,
                "has_wiki": true,
                "has_pages": false,
                "has_discussions": false,
                "forks_count": 0,
                "mirror_url": null,
                "archived": false,
                "disabled": false,
                "open_issues_count": 0,
                "license": null,
                "allow_forking": true,
                "is_template": false,
                "web_commit_signoff_required": false,
                "topics": [],
                "visibility": "public",
                "forks": 0,
                "open_issues": 0,
                "watchers": 0,
                "default_branch": "main",
                "public": true
            }
        },
        "public": true,
        "created_at": "2023-10-26T12:25:54Z"
    },
    {
        "id": "32867048626",
        "type": "WatchEvent",
        "actor": {
            "id": 7423087,
            "login": "bianpratama",
            "display_login": "bianpratama",
            "gravatar_id": "",
            "url": "https://api.github.com/users/bianpratama",
            "avatar_url": "https://avatars.githubusercontent.com/u/7423087?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-26T12:08:55Z"
    },
    {
        "id": "32865431445",
        "type": "IssueCommentEvent",
        "actor": {
            "id": 85022330,
            "login": "suede299",
            "display_login": "suede299",
            "gravatar_id": "",
            "url": "https://api.github.com/users/suede299",
            "avatar_url": "https://avatars.githubusercontent.com/u/85022330?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "created",
            "issue": {
                "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1624",
                "repository_url": "https://api.github.com/repos/bmaltais/kohya_ss",
                "labels_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1624/labels{/name}",
                "comments_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1624/comments",
                "events_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1624/events",
                "html_url": "https://github.com/bmaltais/kohya_ss/issues/1624",
                "id": 1953449535,
                "node_id": "I_kwDOIVqU3M50b0Y_",
                "number": 1624,
                "title": "Cuda12.1 has achieved significant performance improvement and hopes to be adaptable",
                "user": {
                    "login": "wzgrx",
                    "id": 39661556,
                    "node_id": "MDQ6VXNlcjM5NjYxNTU2",
                    "avatar_url": "https://avatars.githubusercontent.com/u/39661556?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/wzgrx",
                    "html_url": "https://github.com/wzgrx",
                    "followers_url": "https://api.github.com/users/wzgrx/followers",
                    "following_url": "https://api.github.com/users/wzgrx/following{/other_user}",
                    "gists_url": "https://api.github.com/users/wzgrx/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/wzgrx/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/wzgrx/subscriptions",
                    "organizations_url": "https://api.github.com/users/wzgrx/orgs",
                    "repos_url": "https://api.github.com/users/wzgrx/repos",
                    "events_url": "https://api.github.com/users/wzgrx/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/wzgrx/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "labels": [],
                "state": "open",
                "locked": false,
                "assignee": null,
                "assignees": [],
                "milestone": null,
                "comments": 1,
                "created_at": "2023-10-20T03:36:38Z",
                "updated_at": "2023-10-26T11:07:28Z",
                "closed_at": null,
                "author_association": "NONE",
                "active_lock_reason": null,
                "body": "Cuda12.1 has achieved significant performance improvement and hopes to be adaptable",
                "reactions": {
                    "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1624/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "timeline_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1624/timeline",
                "performed_via_github_app": null,
                "state_reason": null
            },
            "comment": {
                "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/comments/1780903247",
                "html_url": "https://github.com/bmaltais/kohya_ss/issues/1624#issuecomment-1780903247",
                "issue_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1624",
                "id": 1780903247,
                "node_id": "IC_kwDOIVqU3M5qJm1P",
                "user": {
                    "login": "suede299",
                    "id": 85022330,
                    "node_id": "MDQ6VXNlcjg1MDIyMzMw",
                    "avatar_url": "https://avatars.githubusercontent.com/u/85022330?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/suede299",
                    "html_url": "https://github.com/suede299",
                    "followers_url": "https://api.github.com/users/suede299/followers",
                    "following_url": "https://api.github.com/users/suede299/following{/other_user}",
                    "gists_url": "https://api.github.com/users/suede299/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/suede299/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/suede299/subscriptions",
                    "organizations_url": "https://api.github.com/users/suede299/orgs",
                    "repos_url": "https://api.github.com/users/suede299/repos",
                    "events_url": "https://api.github.com/users/suede299/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/suede299/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "created_at": "2023-10-26T11:07:27Z",
                "updated_at": "2023-10-26T11:07:27Z",
                "author_association": "NONE",
                "body": "The current version does not have any compatibility issues with cuda12.1, at least with sdxl_train_network.py. You will only get a user warning that has no effect. you can even enable xformers, and the just released version 0.0.22.post7 I used without any problems.\r\nModify the version of the dependencies yourself, the whole process will only encounter errors in the accelerate configuration, you need to update the accelerate.\r\n![image](https://github.com/bmaltais/kohya_ss/assets/85022330/b3103d68-e36e-4b27-a8d8-f44f8b746c41)\r\n",
                "reactions": {
                    "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/comments/1780903247/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "performed_via_github_app": null
            }
        },
        "public": true,
        "created_at": "2023-10-26T11:07:28Z"
    },
    {
        "id": "32858689302",
        "type": "WatchEvent",
        "actor": {
            "id": 113626779,
            "login": "bibabao",
            "display_login": "bibabao",
            "gravatar_id": "",
            "url": "https://api.github.com/users/bibabao",
            "avatar_url": "https://avatars.githubusercontent.com/u/113626779?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-26T07:07:15Z"
    },
    {
        "id": "32855563303",
        "type": "WatchEvent",
        "actor": {
            "id": 6580675,
            "login": "entrpn",
            "display_login": "entrpn",
            "gravatar_id": "",
            "url": "https://api.github.com/users/entrpn",
            "avatar_url": "https://avatars.githubusercontent.com/u/6580675?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-26T04:02:40Z"
    },
    {
        "id": "32855505893",
        "type": "IssuesEvent",
        "actor": {
            "id": 15879426,
            "login": "andrewtvuong",
            "display_login": "andrewtvuong",
            "gravatar_id": "",
            "url": "https://api.github.com/users/andrewtvuong",
            "avatar_url": "https://avatars.githubusercontent.com/u/15879426?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "opened",
            "issue": {
                "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1636",
                "repository_url": "https://api.github.com/repos/bmaltais/kohya_ss",
                "labels_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1636/labels{/name}",
                "comments_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1636/comments",
                "events_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1636/events",
                "html_url": "https://github.com/bmaltais/kohya_ss/issues/1636",
                "id": 1962657006,
                "node_id": "I_kwDOIVqU3M50-8Tu",
                "number": 1636,
                "title": "Parameter settings of UI seems random, if not can someone please explain how the parameters are set especially the # epoch",
                "user": {
                    "login": "andrewtvuong",
                    "id": 15879426,
                    "node_id": "MDQ6VXNlcjE1ODc5NDI2",
                    "avatar_url": "https://avatars.githubusercontent.com/u/15879426?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/andrewtvuong",
                    "html_url": "https://github.com/andrewtvuong",
                    "followers_url": "https://api.github.com/users/andrewtvuong/followers",
                    "following_url": "https://api.github.com/users/andrewtvuong/following{/other_user}",
                    "gists_url": "https://api.github.com/users/andrewtvuong/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/andrewtvuong/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/andrewtvuong/subscriptions",
                    "organizations_url": "https://api.github.com/users/andrewtvuong/orgs",
                    "repos_url": "https://api.github.com/users/andrewtvuong/repos",
                    "events_url": "https://api.github.com/users/andrewtvuong/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/andrewtvuong/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "labels": [],
                "state": "open",
                "locked": false,
                "assignee": null,
                "assignees": [],
                "milestone": null,
                "comments": 0,
                "created_at": "2023-10-26T03:58:17Z",
                "updated_at": "2023-10-26T03:58:17Z",
                "closed_at": null,
                "author_association": "NONE",
                "active_lock_reason": null,
                "body": "![395577580_1251501482183311_6058618631504669907_n](https://github.com/bmaltais/kohya_ss/assets/15879426/0e431e98-9f10-4e61-9f6a-dd3a0ea8c06d)\r\n\r\nEverything has been retried and values changed to ensure not a one off case.\r\n\r\nFor example, on the UI I set for 5 epoch, but somehow I end up with 350 epoch, not sure how this value is determined.\r\n\r\nI have 150 images and its .txt caption placed in a folder that's named 5_apple. This should mean 5 repeated steps for 150 images for 5 epoch for a total of 3750 total steps, but somehow I have 261875 total steps.\r\n\r\nMy GPU is a 3070ti with 8gb vram, Driver 545.84, CUDA 12.3, with CUDNN optimization.\r\n\r\nWith a smaller image dataset of 15 files and 20 repeated steps the epoch parameter on the UI seems to respond correctly.\r\nI just don't understand where the number 350 epoch is coming from, not divisible into any parameters I actually inputted.\r\n\r\nThanks! I saw a similar issue here https://github.com/bmaltais/kohya_ss/issues/1618.",
                "reactions": {
                    "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1636/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "timeline_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1636/timeline",
                "performed_via_github_app": null,
                "state_reason": null
            }
        },
        "public": true,
        "created_at": "2023-10-26T03:58:19Z"
    },
    {
        "id": "32855393039",
        "type": "IssueCommentEvent",
        "actor": {
            "id": 15879426,
            "login": "andrewtvuong",
            "display_login": "andrewtvuong",
            "gravatar_id": "",
            "url": "https://api.github.com/users/andrewtvuong",
            "avatar_url": "https://avatars.githubusercontent.com/u/15879426?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "created",
            "issue": {
                "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1618",
                "repository_url": "https://api.github.com/repos/bmaltais/kohya_ss",
                "labels_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1618/labels{/name}",
                "comments_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1618/comments",
                "events_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1618/events",
                "html_url": "https://github.com/bmaltais/kohya_ss/issues/1618",
                "id": 1947341666,
                "node_id": "I_kwDOIVqU3M50EhNi",
                "number": 1618,
                "title": "Parameters settings not working in trainning.",
                "user": {
                    "login": "954114865",
                    "id": 121415253,
                    "node_id": "U_kgDOBzymVQ",
                    "avatar_url": "https://avatars.githubusercontent.com/u/121415253?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/954114865",
                    "html_url": "https://github.com/954114865",
                    "followers_url": "https://api.github.com/users/954114865/followers",
                    "following_url": "https://api.github.com/users/954114865/following{/other_user}",
                    "gists_url": "https://api.github.com/users/954114865/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/954114865/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/954114865/subscriptions",
                    "organizations_url": "https://api.github.com/users/954114865/orgs",
                    "repos_url": "https://api.github.com/users/954114865/repos",
                    "events_url": "https://api.github.com/users/954114865/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/954114865/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "labels": [],
                "state": "open",
                "locked": false,
                "assignee": null,
                "assignees": [],
                "milestone": null,
                "comments": 1,
                "created_at": "2023-10-17T12:45:11Z",
                "updated_at": "2023-10-26T03:49:17Z",
                "closed_at": null,
                "author_association": "NONE",
                "active_lock_reason": null,
                "body": "The parameters I set for trainning(Both Dreambooth and Finetune) don't really be used in trainning, the optimizer, the batch size, the max epoch...nothing is right. I had reinstalled the video driver, installed the old video driver, reinstalled python ,reinstalled several kohya gui version, nothing help.\r\nI did used the gui for a long time, but one day I found it not working properly, I don't know what makes it being like this.\r\nI'm using NVIDIA 3090, AMD Ryzen 3 3700X, 48G RAM, running in SSD, Windows 11.\r\nHere is the running output, you can see the parameters had been read, but not really used in the trainning, instead it gives the adamW and some wired trainning steps I never set:\r\n\r\n> 20:30:35-452667 INFO     Start Finetuning...\r\n20:30:35-453668 INFO     ./venv/Scripts/python.exe finetune/merge_captions_to_metadata.py --caption_extension=.txt\r\n                         \"G:\\Kohya\\Training\\appletest\\2\\img\" \"G:\\Kohya\\Training\\appletest\\2/meta_cap.json\"\r\nfound 3 images.\r\nloading existing metadata: G:\\Kohya\\Training\\appletest\\2/meta_cap.json\r\ncaptions for existing images will be overwritten / \u65e2\u5b58\u306e\u753b\u50cf\u306e\u30ad\u30e3\u30d7\u30b7\u30e7\u30f3\u306f\u4e0a\u66f8\u304d\u3055\u308c\u307e\u3059\r\nmerge caption texts to metadata json.\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 599.50it/s]\r\nwriting metadata: G:\\Kohya\\Training\\appletest\\2/meta_cap.json\r\ndone!\r\n20:30:42-988535 INFO     ./venv/Scripts/python.exe finetune/prepare_buckets_latents.py\r\n                         \"G:\\Kohya\\Training\\appletest\\2\\img\" \"G:\\Kohya\\Training\\appletest\\2/meta_cap.json\"\r\n                         \"G:\\Kohya\\Training\\appletest\\2/meta_lat.json\"\r\n                         \"G:/stable-diffusion-webui/models/Stable-diffusion/epicrealism_naturalSinRC1VAE.safetensors\"\r\n                         --batch_size=1 --max_resolution=512,512 --min_bucket_reso=64 --max_bucket_reso=512\r\n                         --mixed_precision=bf16\r\nfound 3 images.\r\nloading existing metadata: G:\\Kohya\\Training\\appletest\\2/meta_cap.json\r\nload VAE: G:/stable-diffusion-webui/models/Stable-diffusion/epicrealism_naturalSinRC1VAE.safetensors\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:04<00:00,  1.56s/it]\r\nbucket 0 (512, 512): 3\r\nmean ar error: 0.0\r\nwriting metadata: G:\\Kohya\\Training\\appletest\\2/meta_lat.json\r\ndone!\r\n20:30:56-026664 INFO     image_num = 3\r\n20:30:56-028666 INFO     repeats = 3\r\n20:30:56-028666 INFO     max_train_steps = 1\r\n20:30:56-029667 INFO     lr_warmup_steps = 0\r\n20:30:56-030668 INFO     Saving training config to G:\\Kohya\\Training\\appletest\\2\\2_20231017-203056.json...\r\n20:30:56-044682 INFO     accelerate launch --num_cpu_threads_per_process=2 \"./fine_tune.py\" --full_bf16\r\n                         --pretrained_model_name_or_path=\"G:/stable-diffusion-webui/models/Stable-diffusion/epicrealism_\r\n                         naturalSinRC1VAE.safetensors\" --in_json=\"G:\\Kohya\\Training\\appletest\\2/meta_cap.json\"\r\n                         --train_data_dir=\"G:\\Kohya\\Training\\appletest\\2\\img\"\r\n                         --output_dir=\"G:\\Kohya\\Training\\appletest\\2\\\" --logging_dir=\"G:\\Kohya\\Training\\appletest\\2\\log\"\r\n                         --dataset_repeats=1 --learning_rate=1.0 --enable_bucket --resolution=\"512,512\"\r\n                         --min_bucket_reso=64 --max_bucket_reso=512 --save_model_as=safetensors --output_name=\"2\"\r\n                         --learning_rate=\"1.0\" --lr_scheduler=\"constant\" --train_batch_size=\"4\" --max_train_steps=\"1\"\r\n                         --save_every_n_epochs=\"50\" --mixed_precision=\"bf16\" --save_precision=\"bf16\" --seed=\"1\"\r\n                         --caption_extension=\".txt\" --cache_latents --optimizer_type=\"DAdaptAdam\" --max_train_epochs=200\r\n                         --max_data_loader_n_workers=\"0\" --bucket_reso_steps=1 --gradient_checkpointing --xformers\r\n                         --bucket_no_upscale --noise_offset=0.0 --sample_sampler=euler_a\r\n                         --sample_prompts=\"G:\\Kohya\\Training\\appletest\\2\\sample\\prompt.txt\" --sample_every_n_epochs=\"20\"\r\nprepare tokenizer\r\nloading existing metadata: G:\\Kohya\\Training\\appletest\\2/meta_cap.json\r\nmetadata has bucket info, enable bucketing / \u30e1\u30bf\u30c7\u30fc\u30bf\u306bbucket\u60c5\u5831\u304c\u3042\u308b\u305f\u3081bucket\u3092\u6709\u52b9\u306b\u3057\u307e\u3059\r\nusing bucket info in metadata / \u30e1\u30bf\u30c7\u30fc\u30bf\u5185\u306ebucket\u60c5\u5831\u3092\u4f7f\u3044\u307e\u3059\r\n[Dataset 0]\r\n  batch_size: 1\r\n  resolution: (None, None)\r\n  enable_bucket: True\r\n  min_bucket_reso: None\r\n  max_bucket_reso: None\r\n  bucket_reso_steps: None\r\n  bucket_no_upscale: None\r\n  [Subset 0 of Dataset 0]\r\n    image_dir: \"G:\\Kohya\\Training\\appletest\\2\\img\"\r\n    image_count: 3\r\n    num_repeats: 1\r\n    shuffle_caption: False\r\n    keep_tokens: 0\r\n    caption_dropout_rate: 0.0\r\n    caption_dropout_every_n_epoches: 0\r\n    caption_tag_dropout_rate: 0.0\r\n    caption_prefix: None\r\n    caption_suffix: None\r\n    color_aug: False\r\n    flip_aug: False\r\n    face_crop_aug_range: None\r\n    random_crop: False\r\n    token_warmup_min: 1,\r\n    token_warmup_step: 0,\r\n    metadata_file: G:\\Kohya\\Training\\appletest\\2/meta_cap.json\r\n[Dataset 0]\r\nloading image sizes.\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<?, ?it/s]\r\nmake buckets\r\nnumber of images (including repeats) / \u5404bucket\u306e\u753b\u50cf\u679a\u6570\uff08\u7e70\u308a\u8fd4\u3057\u56de\u6570\u3092\u542b\u3080\uff09\r\nbucket 0: resolution (512, 512), count: 3\r\nmean ar error (without repeats): 0.0\r\nprepare accelerator\r\nloading model for process 0/1\r\nload StableDiffusion checkpoint: G:/stable-diffusion-webui/models/Stable-diffusion/epicrealism_naturalSinRC1VAE.safetensors\r\nUNet2DConditionModel: 64, 8, 768, False, False\r\nloading u-net: <All keys matched successfully>\r\nloading vae: <All keys matched successfully>\r\nloading text encoder: <All keys matched successfully>\r\nDisable Diffusers' xformers\r\nprepare optimizer, data loader etc.\r\nuse AdamW optimizer | {}\r\nrunning training / \u5b66\u7fd2\u958b\u59cb\r\n  num examples / \u30b5\u30f3\u30d7\u30eb\u6570: 3\r\n  num batches per epoch / 1epoch\u306e\u30d0\u30c3\u30c1\u6570: 3\r\n  num epochs / epoch\u6570: 534\r\n  batch size per device / \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba: 1\r\n  total train batch size (with parallel & distributed & accumulation) / \u7dcf\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\uff08\u4e26\u5217\u5b66\u7fd2\u3001\u52fe\u914d\u5408\u8a08\u542b\u3080\uff09: 1\r\n  gradient accumulation steps / \u52fe\u914d\u3092\u5408\u8a08\u3059\u308b\u30b9\u30c6\u30c3\u30d7\u6570 = 1\r\n  total optimization steps / \u5b66\u7fd2\u30b9\u30c6\u30c3\u30d7\u6570: 1600\r\nsteps:   0%|                                                                                  | 0/1600 [00:00<?, ?it/s]\r\nepoch 1/534\r\nsteps:   0%|                                                            | 3/1600 [01:02<9:17:15, 20.94s/it, loss=0.032]\r\nepoch 2/534\r\nsteps:   0%|\u258f                                                          | 6/1600 [02:02<9:02:20, 20.41s/it, loss=0.0173]\r\nepoch 3/534\r\n20:33:22-634398 INFO     The running process has been terminated.",
                "reactions": {
                    "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1618/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "timeline_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1618/timeline",
                "performed_via_github_app": null,
                "state_reason": null
            },
            "comment": {
                "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/comments/1780371325",
                "html_url": "https://github.com/bmaltais/kohya_ss/issues/1618#issuecomment-1780371325",
                "issue_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1618",
                "id": 1780371325,
                "node_id": "IC_kwDOIVqU3M5qHk99",
                "user": {
                    "login": "andrewtvuong",
                    "id": 15879426,
                    "node_id": "MDQ6VXNlcjE1ODc5NDI2",
                    "avatar_url": "https://avatars.githubusercontent.com/u/15879426?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/andrewtvuong",
                    "html_url": "https://github.com/andrewtvuong",
                    "followers_url": "https://api.github.com/users/andrewtvuong/followers",
                    "following_url": "https://api.github.com/users/andrewtvuong/following{/other_user}",
                    "gists_url": "https://api.github.com/users/andrewtvuong/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/andrewtvuong/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/andrewtvuong/subscriptions",
                    "organizations_url": "https://api.github.com/users/andrewtvuong/orgs",
                    "repos_url": "https://api.github.com/users/andrewtvuong/repos",
                    "events_url": "https://api.github.com/users/andrewtvuong/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/andrewtvuong/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "created_at": "2023-10-26T03:49:17Z",
                "updated_at": "2023-10-26T03:49:17Z",
                "author_association": "NONE",
                "body": "I have this same issue",
                "reactions": {
                    "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/comments/1780371325/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "performed_via_github_app": null
            }
        },
        "public": true,
        "created_at": "2023-10-26T03:49:17Z"
    },
    {
        "id": "32854999233",
        "type": "WatchEvent",
        "actor": {
            "id": 6216144,
            "login": "Jaid",
            "display_login": "Jaid",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Jaid",
            "avatar_url": "https://avatars.githubusercontent.com/u/6216144?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-26T03:18:31Z"
    },
    {
        "id": "32852196899",
        "type": "ForkEvent",
        "actor": {
            "id": 36973240,
            "login": "danruggi",
            "display_login": "danruggi",
            "gravatar_id": "",
            "url": "https://api.github.com/users/danruggi",
            "avatar_url": "https://avatars.githubusercontent.com/u/36973240?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "forkee": {
                "id": 710058317,
                "node_id": "R_kgDOKlKhTQ",
                "name": "kohya_ss",
                "full_name": "danruggi/kohya_ss",
                "private": false,
                "owner": {
                    "login": "danruggi",
                    "id": 36973240,
                    "node_id": "MDQ6VXNlcjM2OTczMjQw",
                    "avatar_url": "https://avatars.githubusercontent.com/u/36973240?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/danruggi",
                    "html_url": "https://github.com/danruggi",
                    "followers_url": "https://api.github.com/users/danruggi/followers",
                    "following_url": "https://api.github.com/users/danruggi/following{/other_user}",
                    "gists_url": "https://api.github.com/users/danruggi/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/danruggi/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/danruggi/subscriptions",
                    "organizations_url": "https://api.github.com/users/danruggi/orgs",
                    "repos_url": "https://api.github.com/users/danruggi/repos",
                    "events_url": "https://api.github.com/users/danruggi/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/danruggi/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "html_url": "https://github.com/danruggi/kohya_ss",
                "description": null,
                "fork": true,
                "url": "https://api.github.com/repos/danruggi/kohya_ss",
                "forks_url": "https://api.github.com/repos/danruggi/kohya_ss/forks",
                "keys_url": "https://api.github.com/repos/danruggi/kohya_ss/keys{/key_id}",
                "collaborators_url": "https://api.github.com/repos/danruggi/kohya_ss/collaborators{/collaborator}",
                "teams_url": "https://api.github.com/repos/danruggi/kohya_ss/teams",
                "hooks_url": "https://api.github.com/repos/danruggi/kohya_ss/hooks",
                "issue_events_url": "https://api.github.com/repos/danruggi/kohya_ss/issues/events{/number}",
                "events_url": "https://api.github.com/repos/danruggi/kohya_ss/events",
                "assignees_url": "https://api.github.com/repos/danruggi/kohya_ss/assignees{/user}",
                "branches_url": "https://api.github.com/repos/danruggi/kohya_ss/branches{/branch}",
                "tags_url": "https://api.github.com/repos/danruggi/kohya_ss/tags",
                "blobs_url": "https://api.github.com/repos/danruggi/kohya_ss/git/blobs{/sha}",
                "git_tags_url": "https://api.github.com/repos/danruggi/kohya_ss/git/tags{/sha}",
                "git_refs_url": "https://api.github.com/repos/danruggi/kohya_ss/git/refs{/sha}",
                "trees_url": "https://api.github.com/repos/danruggi/kohya_ss/git/trees{/sha}",
                "statuses_url": "https://api.github.com/repos/danruggi/kohya_ss/statuses/{sha}",
                "languages_url": "https://api.github.com/repos/danruggi/kohya_ss/languages",
                "stargazers_url": "https://api.github.com/repos/danruggi/kohya_ss/stargazers",
                "contributors_url": "https://api.github.com/repos/danruggi/kohya_ss/contributors",
                "subscribers_url": "https://api.github.com/repos/danruggi/kohya_ss/subscribers",
                "subscription_url": "https://api.github.com/repos/danruggi/kohya_ss/subscription",
                "commits_url": "https://api.github.com/repos/danruggi/kohya_ss/commits{/sha}",
                "git_commits_url": "https://api.github.com/repos/danruggi/kohya_ss/git/commits{/sha}",
                "comments_url": "https://api.github.com/repos/danruggi/kohya_ss/comments{/number}",
                "issue_comment_url": "https://api.github.com/repos/danruggi/kohya_ss/issues/comments{/number}",
                "contents_url": "https://api.github.com/repos/danruggi/kohya_ss/contents/{+path}",
                "compare_url": "https://api.github.com/repos/danruggi/kohya_ss/compare/{base}...{head}",
                "merges_url": "https://api.github.com/repos/danruggi/kohya_ss/merges",
                "archive_url": "https://api.github.com/repos/danruggi/kohya_ss/{archive_format}{/ref}",
                "downloads_url": "https://api.github.com/repos/danruggi/kohya_ss/downloads",
                "issues_url": "https://api.github.com/repos/danruggi/kohya_ss/issues{/number}",
                "pulls_url": "https://api.github.com/repos/danruggi/kohya_ss/pulls{/number}",
                "milestones_url": "https://api.github.com/repos/danruggi/kohya_ss/milestones{/number}",
                "notifications_url": "https://api.github.com/repos/danruggi/kohya_ss/notifications{?since,all,participating}",
                "labels_url": "https://api.github.com/repos/danruggi/kohya_ss/labels{/name}",
                "releases_url": "https://api.github.com/repos/danruggi/kohya_ss/releases{/id}",
                "deployments_url": "https://api.github.com/repos/danruggi/kohya_ss/deployments",
                "created_at": "2023-10-25T23:48:30Z",
                "updated_at": "2023-10-25T23:48:30Z",
                "pushed_at": "2023-10-25T15:32:35Z",
                "git_url": "git://github.com/danruggi/kohya_ss.git",
                "ssh_url": "git@github.com:danruggi/kohya_ss.git",
                "clone_url": "https://github.com/danruggi/kohya_ss.git",
                "svn_url": "https://github.com/danruggi/kohya_ss",
                "homepage": null,
                "size": 11763,
                "stargazers_count": 0,
                "watchers_count": 0,
                "language": null,
                "has_issues": false,
                "has_projects": true,
                "has_downloads": true,
                "has_wiki": true,
                "has_pages": false,
                "has_discussions": false,
                "forks_count": 0,
                "mirror_url": null,
                "archived": false,
                "disabled": false,
                "open_issues_count": 0,
                "license": null,
                "allow_forking": true,
                "is_template": false,
                "web_commit_signoff_required": false,
                "topics": [],
                "visibility": "public",
                "forks": 0,
                "open_issues": 0,
                "watchers": 0,
                "default_branch": "main",
                "public": true
            }
        },
        "public": true,
        "created_at": "2023-10-25T23:48:31Z"
    },
    {
        "id": "32852158163",
        "type": "IssuesEvent",
        "actor": {
            "id": 148998686,
            "login": "thekotfather",
            "display_login": "thekotfather",
            "gravatar_id": "",
            "url": "https://api.github.com/users/thekotfather",
            "avatar_url": "https://avatars.githubusercontent.com/u/148998686?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "opened",
            "issue": {
                "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1635",
                "repository_url": "https://api.github.com/repos/bmaltais/kohya_ss",
                "labels_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1635/labels{/name}",
                "comments_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1635/comments",
                "events_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1635/events",
                "html_url": "https://github.com/bmaltais/kohya_ss/issues/1635",
                "id": 1962465875,
                "node_id": "I_kwDOIVqU3M50-NpT",
                "number": 1635,
                "title": "Lora training (RUNPOD)",
                "user": {
                    "login": "thekotfather",
                    "id": 148998686,
                    "node_id": "U_kgDOCOGKHg",
                    "avatar_url": "https://avatars.githubusercontent.com/u/148998686?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/thekotfather",
                    "html_url": "https://github.com/thekotfather",
                    "followers_url": "https://api.github.com/users/thekotfather/followers",
                    "following_url": "https://api.github.com/users/thekotfather/following{/other_user}",
                    "gists_url": "https://api.github.com/users/thekotfather/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/thekotfather/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/thekotfather/subscriptions",
                    "organizations_url": "https://api.github.com/users/thekotfather/orgs",
                    "repos_url": "https://api.github.com/users/thekotfather/repos",
                    "events_url": "https://api.github.com/users/thekotfather/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/thekotfather/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "labels": [],
                "state": "open",
                "locked": false,
                "assignee": null,
                "assignees": [],
                "milestone": null,
                "comments": 0,
                "created_at": "2023-10-25T23:45:13Z",
                "updated_at": "2023-10-25T23:45:13Z",
                "closed_at": null,
                "author_association": "NONE",
                "active_lock_reason": null,
                "body": "Help me, pleeeease!\r\n\r\nI've tried this installation 500 times, but no result.\r\nEverything is starting good, but..\r\n\r\n****\"ModuleNotFoundError: No module named 'scipy'\"**\r\nand then...\r\n...During handling of the above exception, another exception occurred:**\r\nblablablah(attached the full log)\r\n\r\n**scipy** is installed, of course.\r\n\r\nHELP ME UNDERSTAND, PLEASE =(\r\n\r\n\r\n\r\nLog file:\r\n\r\nroot@34890166845e:/workspace/kohya_ss# ./gui.sh --share --headless\r\n23:33:18-830649 INFO     Version: v22.1.0                                                                                       \r\n                                                                                                                                \r\n23:33:18-843460 INFO     nVidia toolkit detected                                                                                \r\n23:33:20-275862 INFO     Torch 2.0.1+cu118                                                                                      \r\n23:33:20-313302 INFO     Torch backend: nVidia CUDA 11.8 cuDNN 8904                                                             \r\n23:33:20-329518 INFO     Torch detected GPU: Tesla V100-SXM2-16GB VRAM 16151 Arch (7, 0) Cores 80                               \r\n23:33:20-330969 INFO     Verifying modules installation status from /workspace/kohya_ss/requirements_runpod.txt...              \r\n23:33:20-334579 INFO     Verifying modules installation status from requirements.txt...                                         \r\n23:33:20-340323 WARNING  Package wrong version: huggingface-hub 0.17.3 required 0.15.1                                          \r\n23:33:20-341864 INFO     Installing package: huggingface-hub==0.15.1                                                            \r\n23:33:26-377521 INFO     headless: True                                                                                         \r\n23:33:26-382540 INFO     Load CSS...                                                                                            \r\nRunning on local URL:  http://127.0.0.1:7863\r\nRunning on public URL: https://09bdb39e27a344aa7b.gradio.live\r\n\r\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\r\n23:34:59-397707 INFO     Loading config...                                                                                      \r\n23:35:15-756463 INFO     Loading config...                                                                                      \r\n23:35:59-035797 INFO     Start training LoRA Standard ...                                                                       \r\n23:35:59-038059 INFO     Checking for duplicate image filenames in training data directory...                                   \r\n23:35:59-040859 INFO     Valid image folder names found in: /workspace/test1/img                                                \r\n23:35:59-043044 INFO     Headless mode, skipping verification if model already exist... if model already exist it will be       \r\n                         overwritten...                                                                                         \r\n23:35:59-045912 INFO     Folder 40_natusyavelikovnagross woman: 13 images found                                                 \r\n23:35:59-048047 INFO     Folder 40_natusyavelikovnagross woman: 520 steps                                                       \r\n23:35:59-049271 INFO     Total steps: 520                                                                                       \r\n23:35:59-050185 INFO     Train batch size: 1                                                                                    \r\n23:35:59-051122 INFO     Gradient accumulation steps: 1                                                                         \r\n23:35:59-052064 INFO     Epoch: 1                                                                                               \r\n23:35:59-052973 INFO     Regulatization factor: 1                                                                               \r\n23:35:59-053918 INFO     max_train_steps (520 / 1 / 1 * 1 * 1) = 520                                                            \r\n23:35:59-055174 INFO     stop_text_encoder_training = 0                                                                         \r\n23:35:59-056143 INFO     lr_warmup_steps = 52                                                                                   \r\n23:35:59-057189 INFO     Saving training config to /workspace/test1/model/last_20231025-233559.json...                          \r\n23:35:59-058623 INFO     accelerate launch --num_cpu_threads_per_process=2 \"./train_network.py\" --enable_bucket                 \r\n                         --min_bucket_reso=256 --max_bucket_reso=2048                                                           \r\n                         --pretrained_model_name_or_path=\"/workspace/stable-diffusion-webui/models/Stable-diffusion/deliberate.s\r\n                         afetensors\" --train_data_dir=\"/workspace/test1/img\" --resolution=\"512,512\"                             \r\n                         --output_dir=\"/workspace/test1/model\" --logging_dir=\"/workspace/test1/log\" --network_alpha=\"1\"         \r\n                         --save_model_as=safetensors --network_module=networks.lora --text_encoder_lr=5e-05 --unet_lr=0.0001    \r\n                         --network_dim=256 --output_name=\"last\" --lr_scheduler_num_cycles=\"1\" --no_half_vae                     \r\n                         --learning_rate=\"0.0001\" --lr_scheduler=\"cosine\" --lr_warmup_steps=\"52\" --train_batch_size=\"1\"         \r\n                         --max_train_steps=\"520\" --save_every_n_epochs=\"1\" --mixed_precision=\"fp16\" --save_precision=\"fp16\"     \r\n                         --cache_latents --optimizer_type=\"AdamW8bit\" --max_data_loader_n_workers=\"0\" --bucket_reso_steps=64    \r\n                         --xformers --bucket_no_upscale --noise_offset=0.0                                                      \r\n2023-10-25 23:36:03.826827: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2023-10-25 23:36:03.826886: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2023-10-25 23:36:03.826914: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2023-10-25 23:36:03.835111: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-10-25 23:36:04.783174: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\nprepare tokenizer\r\nUsing DreamBooth method.\r\nprepare images.\r\nfound directory /workspace/test1/img/40_natusyavelikovnagross woman contains 13 image files\r\nNo caption file found for 13 images. Training will continue without captions for these images. If class token exists, it will be used. / 13\u679a\u306e\u753b\u50cf\u306b\u30ad\u30e3\u30d7\u30b7\u30e7\u30f3\u30d5\u30a1\u30a4\u30eb\u304c\u898b\u3064\u304b\u308a\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u3053\u308c\u3089\u306e\u753b\u50cf\u306b\u3064\u3044\u3066\u306f\u30ad\u30e3\u30d7\u30b7\u30e7\u30f3\u306a\u3057\u3067\u5b66\u7fd2\u3092\u7d9a\u884c\u3057\u307e\u3059\u3002class token\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306f\u305d\u308c\u3092\u4f7f\u3044\u307e\u3059\u3002\r\n/workspace/test1/img/40_natusyavelikovnagross woman/image (10).png\r\n/workspace/test1/img/40_natusyavelikovnagross woman/image (11).png\r\n/workspace/test1/img/40_natusyavelikovnagross woman/image (12).png\r\n/workspace/test1/img/40_natusyavelikovnagross woman/image (13).png\r\n/workspace/test1/img/40_natusyavelikovnagross woman/image (14).png\r\n/workspace/test1/img/40_natusyavelikovnagross woman/image (15).png... and 8 more\r\n520 train images with repeating.\r\n0 reg images.\r\nno regularization images / \u6b63\u5247\u5316\u753b\u50cf\u304c\u898b\u3064\u304b\u308a\u307e\u305b\u3093\u3067\u3057\u305f\r\n[Dataset 0]\r\n  batch_size: 1\r\n  resolution: (512, 512)\r\n  enable_bucket: True\r\n  min_bucket_reso: 256\r\n  max_bucket_reso: 2048\r\n  bucket_reso_steps: 64\r\n  bucket_no_upscale: True\r\n\r\n  [Subset 0 of Dataset 0]\r\n    image_dir: \"/workspace/test1/img/40_natusyavelikovnagross woman\"\r\n    image_count: 13\r\n    num_repeats: 40\r\n    shuffle_caption: False\r\n    keep_tokens: 0\r\n    caption_dropout_rate: 0.0\r\n    caption_dropout_every_n_epoches: 0\r\n    caption_tag_dropout_rate: 0.0\r\n    caption_prefix: None\r\n    caption_suffix: None\r\n    color_aug: False\r\n    flip_aug: False\r\n    face_crop_aug_range: None\r\n    random_crop: False\r\n    token_warmup_min: 1,\r\n    token_warmup_step: 0,\r\n    is_reg: False\r\n    class_tokens: natusyavelikovnagross woman\r\n    caption_extension: .caption\r\n\r\n\r\n[Dataset 0]\r\nloading image sizes.\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00<00:00, 1611.33it/s]\r\nmake buckets\r\nmin_bucket_reso and max_bucket_reso are ignored if bucket_no_upscale is set, because bucket reso is defined by image size automatically / bucket_no_upscale\u304c\u6307\u5b9a\u3055\u308c\u305f\u5834\u5408\u306f\u3001bucket\u306e\u89e3\u50cf\u5ea6\u306f\u753b\u50cf\u30b5\u30a4\u30ba\u304b\u3089\u81ea\u52d5\u8a08\u7b97\u3055\u308c\u308b\u305f\u3081\u3001min_bucket_reso\u3068max_bucket_reso\u306f\u7121\u8996\u3055\u308c\u307e\u3059\r\nnumber of images (including repeats) / \u5404bucket\u306e\u753b\u50cf\u679a\u6570\uff08\u7e70\u308a\u8fd4\u3057\u56de\u6570\u3092\u542b\u3080\uff09\r\nbucket 0: resolution (512, 512), count: 520\r\nmean ar error (without repeats): 0.0\r\npreparing accelerator\r\nloading model for process 0/1\r\nload StableDiffusion checkpoint: /workspace/stable-diffusion-webui/models/Stable-diffusion/deliberate.safetensors\r\nUNet2DConditionModel: 64, 8, 768, False, False\r\nloading u-net: <All keys matched successfully>\r\nloading vae: <All keys matched successfully>\r\nloading text encoder: <All keys matched successfully>\r\nEnable xformers for U-Net\r\nimport network module: networks.lora\r\n[Dataset 0]\r\ncaching latents.\r\nchecking cache validity...\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00<00:00, 254794.17it/s]\r\ncaching latents...\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:02<00:00,  5.96it/s]\r\ncreate LoRA network. base dim (rank): 256, alpha: 1.0\r\nneuron dropout: p=None, rank dropout: p=None, module dropout: p=None\r\ncreate LoRA for Text Encoder:\r\ncreate LoRA for Text Encoder: 72 modules.\r\ncreate LoRA for U-Net: 192 modules.\r\nenable LoRA for text encoder\r\nenable LoRA for U-Net\r\nprepare optimizer, data loader etc.\r\nTraceback (most recent call last):\r\n  File \"/workspace/kohya_ss/library/train_util.py\", line 3419, in get_optimizer\r\n    import bitsandbytes as bnb\r\n  File \"/workspace/kohya_ss/venv/lib/python3.10/site-packages/bitsandbytes/__init__.py\", line 6, in <module>\r\n    from . import cuda_setup, utils, research\r\n  File \"/workspace/kohya_ss/venv/lib/python3.10/site-packages/bitsandbytes/research/__init__.py\", line 1, in <module>\r\n    from . import nn\r\n  File \"/workspace/kohya_ss/venv/lib/python3.10/site-packages/bitsandbytes/research/nn/__init__.py\", line 1, in <module>\r\n    from .modules import LinearFP8Mixed, LinearFP8Global\r\n  File \"/workspace/kohya_ss/venv/lib/python3.10/site-packages/bitsandbytes/research/nn/modules.py\", line 8, in <module>\r\n    from bitsandbytes.optim import GlobalOptimManager\r\n  File \"/workspace/kohya_ss/venv/lib/python3.10/site-packages/bitsandbytes/optim/__init__.py\", line 8, in <module>\r\n    from .adagrad import Adagrad, Adagrad8bit, Adagrad32bit\r\n  File \"/workspace/kohya_ss/venv/lib/python3.10/site-packages/bitsandbytes/optim/adagrad.py\", line 5, in <module>\r\n    from bitsandbytes.optim.optimizer import Optimizer1State\r\n  File \"/workspace/kohya_ss/venv/lib/python3.10/site-packages/bitsandbytes/optim/optimizer.py\", line 12, in <module>\r\n    import bitsandbytes.functional as F\r\n  File \"/workspace/kohya_ss/venv/lib/python3.10/site-packages/bitsandbytes/functional.py\", line 12, in <module>\r\n    from scipy.stats import norm\r\nModuleNotFoundError: No module named 'scipy'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/workspace/kohya_ss/./train_network.py\", line 1009, in <module>\r\n    trainer.train(args)\r\n  File \"/workspace/kohya_ss/./train_network.py\", line 338, in train\r\n    optimizer_name, optimizer_args, optimizer = train_util.get_optimizer(args, trainable_params)\r\n  File \"/workspace/kohya_ss/library/train_util.py\", line 3421, in get_optimizer\r\n    raise ImportError(\"No bitsandbytes / bitsandbytes\u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u306a\u3044\u3088\u3046\u3067\u3059\")\r\nImportError: No bitsandbytes / bitsandbytes\u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u306a\u3044\u3088\u3046\u3067\u3059\r\nTraceback (most recent call last):\r\n  File \"/workspace/kohya_ss/venv/bin/accelerate\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/workspace/kohya_ss/venv/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py\", line 47, in main\r\n    args.func(args)\r\n  File \"/workspace/kohya_ss/venv/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 986, in launch_command\r\n    simple_launcher(args)\r\n  File \"/workspace/kohya_ss/venv/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 628, in simple_launcher\r\n    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\r\nsubprocess.CalledProcessError: Command '['/workspace/kohya_ss/venv/bin/python', './train_network.py', '--enable_bucket', '--min_bucket_reso=256', '--max_bucket_reso=2048', '--pretrained_model_name_or_path=/workspace/stable-diffusion-webui/models/Stable-diffusion/deliberate.safetensors', '--train_data_dir=/workspace/test1/img', '--resolution=512,512', '--output_dir=/workspace/test1/model', '--logging_dir=/workspace/test1/log', '--network_alpha=1', '--save_model_as=safetensors', '--network_module=networks.lora', '--text_encoder_lr=5e-05', '--unet_lr=0.0001', '--network_dim=256', '--output_name=last', '--lr_scheduler_num_cycles=1', '--no_half_vae', '--learning_rate=0.0001', '--lr_scheduler=cosine', '--lr_warmup_steps=52', '--train_batch_size=1', '--max_train_steps=520', '--save_every_n_epochs=1', '--mixed_precision=fp16', '--save_precision=fp16', '--cache_latents', '--optimizer_type=AdamW8bit', '--max_data_loader_n_workers=0', '--bucket_reso_steps=64', '--xformers', '--bucket_no_upscale', '--noise_offset=0.0']' returned non-zero exit status 1.",
                "reactions": {
                    "url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1635/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "timeline_url": "https://api.github.com/repos/bmaltais/kohya_ss/issues/1635/timeline",
                "performed_via_github_app": null,
                "state_reason": null
            }
        },
        "public": true,
        "created_at": "2023-10-25T23:45:15Z"
    },
    {
        "id": "32850266500",
        "type": "WatchEvent",
        "actor": {
            "id": 11469924,
            "login": "ardywibowo",
            "display_login": "ardywibowo",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ardywibowo",
            "avatar_url": "https://avatars.githubusercontent.com/u/11469924?"
        },
        "repo": {
            "id": 559584476,
            "name": "bmaltais/kohya_ss",
            "url": "https://api.github.com/repos/bmaltais/kohya_ss"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-25T21:49:26Z"
    }
]