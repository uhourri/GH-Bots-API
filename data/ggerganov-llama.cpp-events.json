[
    {
        "id": "32897858541",
        "type": "IssueCommentEvent",
        "actor": {
            "id": 59298527,
            "login": "Ph0rk0z",
            "display_login": "Ph0rk0z",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Ph0rk0z",
            "avatar_url": "https://avatars.githubusercontent.com/u/59298527?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "action": "created",
            "issue": {
                "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3776",
                "repository_url": "https://api.github.com/repos/ggerganov/llama.cpp",
                "labels_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3776/labels{/name}",
                "comments_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3776/comments",
                "events_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3776/events",
                "html_url": "https://github.com/ggerganov/llama.cpp/pull/3776",
                "id": 1961304396,
                "node_id": "PR_kwDOJH_K4M5dv5bW",
                "number": 3776,
                "title": "cuda : improve text-generation and batched decoding performance",
                "user": {
                    "login": "ggerganov",
                    "id": 1991296,
                    "node_id": "MDQ6VXNlcjE5OTEyOTY=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/1991296?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/ggerganov",
                    "html_url": "https://github.com/ggerganov",
                    "followers_url": "https://api.github.com/users/ggerganov/followers",
                    "following_url": "https://api.github.com/users/ggerganov/following{/other_user}",
                    "gists_url": "https://api.github.com/users/ggerganov/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/ggerganov/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/ggerganov/subscriptions",
                    "organizations_url": "https://api.github.com/users/ggerganov/orgs",
                    "repos_url": "https://api.github.com/users/ggerganov/repos",
                    "events_url": "https://api.github.com/users/ggerganov/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/ggerganov/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "labels": [],
                "state": "open",
                "locked": false,
                "assignee": null,
                "assignees": [],
                "milestone": null,
                "comments": 20,
                "created_at": "2023-10-25T12:26:34Z",
                "updated_at": "2023-10-27T11:59:41Z",
                "closed_at": null,
                "author_association": "OWNER",
                "active_lock_reason": null,
                "draft": false,
                "pull_request": {
                    "url": "https://api.github.com/repos/ggerganov/llama.cpp/pulls/3776",
                    "html_url": "https://github.com/ggerganov/llama.cpp/pull/3776",
                    "diff_url": "https://github.com/ggerganov/llama.cpp/pull/3776.diff",
                    "patch_url": "https://github.com/ggerganov/llama.cpp/pull/3776.patch",
                    "merged_at": null
                },
                "body": "ref #3479 \r\nref #3771 \r\n\r\n# Description\r\n\r\nThis PR should improve significantly the text-generation, prompt processing and batched decoding speed for all models for NVIDIA cards with tensor cores (i.e. VOLTA, AMPERE, etc).\r\n\r\n## Prompt processing\r\n\r\nBy default `llama.cpp` uses `MMQ=1` which means that the matrix-matrix multiplications for quantized models are performed with custom kernel for integer multiplications. Recently (#3412), we found out that for large batch dimension (which is the case when processing prompts), `MMQ=0` offers significant performance boost by first dequantizing `src0` to F16 and performing the GEMM using cublas. This PR essentially enables the same optimization for `MMQ=1` by not using the custom kernel for batch size > 32.\r\n\r\n## Batched decoding\r\n\r\nIn this mode, the batch size is larger than 1, but typically small (for example not more than 32). In #3545 we found out that the currently used constants `MMQ_X`, `MMQ_Y` and `NWARPS` are not optimal for small batch sizes. Probably they have been optimized for prompt processing. However, since we now fallback to cuBLAS for prompt processing, the constants can be adjusted for small batch sizes.\r\n\r\n## Text-generation\r\n\r\nSo far, for the KV cache related ops (`KQ` and `KQV`) we have been using custom matrix-vector kernels. For small sequence lengths (~128) and no prompt, these kernels are quite efficient. However, as the KV cache grows with the sequence length it is more efficient to use the tensor cores via cuBLAS GEMM. This PR applies this change to achieve TG improvements for all models when the context is big\r\n\r\nIn summary, we now have the following strategy for matrix multiplications:\r\n\r\n- batch size == 1:\r\n  - non-attention ops && quantized `src0`: use custom matrix-vector kernel\r\n  - otherwise: use cuBLAS GEMM\r\n- batch size <= 32: use custom matrix-matrix kernel\r\n- batch size > 32: use cuBLAS GEMM\r\n\r\n# Results\r\n\r\n## RTX 3090\r\n\r\n```bash\r\nLLAMA_CUBLAS=1 make -j batched batched-bench && ./batched-bench ./models/codellama-7b/ggml-model-q4_0.gguf 8704 1 99 1 512 128 1,2,3,4,5,6,7,8,16,32,64\r\n```\r\n\r\n### master\r\n\r\nmain: n_kv_max = 8704, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.248 |  2063.97 |    1.098 |   116.60 |    1.346 |   475.54 |\r\n|   512 |    128 |    2 |    768 |    0.234 |  2191.11 |    7.435 |    34.43 |    7.669 |   100.15 |\r\n|   512 |    128 |    3 |    896 |    0.233 |  2198.40 |    7.515 |    51.09 |    7.748 |   115.64 |\r\n|   512 |    128 |    4 |   1024 |    0.234 |  2189.40 |    7.553 |    67.78 |    7.787 |   131.50 |\r\n|   512 |    128 |    5 |   1152 |    0.236 |  2168.46 |    7.635 |    83.83 |    7.871 |   146.37 |\r\n|   512 |    128 |    6 |   1280 |    0.236 |  2167.85 |    7.700 |    99.74 |    7.936 |   161.28 |\r\n|   512 |    128 |    7 |   1408 |    0.238 |  2152.19 |    7.768 |   115.34 |    8.006 |   175.86 |\r\n|   512 |    128 |    8 |   1536 |    0.242 |  2115.82 |    7.832 |   130.75 |    8.074 |   190.25 |\r\n|   512 |    128 |   16 |   2560 |    0.244 |  2095.92 |    8.391 |   244.06 |    8.636 |   296.45 |\r\n|   512 |    128 |   32 |   4608 |    0.264 |  1937.69 |    8.852 |   462.71 |    9.116 |   505.46 |\r\n|   512 |    128 |   64 |   8704 |    0.245 |  2085.75 |   11.646 |   703.42 |   11.891 |   731.95 |\r\n\r\n### PR\r\n\r\nmain: n_kv_max = 8704, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.173 |  2953.07 |    1.073 |   119.32 |    1.246 |   513.57 |\r\n|   512 |    128 |    2 |    768 |    0.165 |  3109.78 |    1.715 |   149.23 |    1.880 |   408.49 |\r\n|   512 |    128 |    3 |    896 |    0.162 |  3155.39 |    1.768 |   217.16 |    1.931 |   464.11 |\r\n|   512 |    128 |    4 |   1024 |    0.164 |  3123.65 |    1.803 |   283.98 |    1.967 |   520.63 |\r\n|   512 |    128 |    5 |   1152 |    0.163 |  3138.81 |    2.403 |   266.36 |    2.566 |   448.96 |\r\n|   512 |    128 |    6 |   1280 |    0.166 |  3077.79 |    2.433 |   315.70 |    2.599 |   492.49 |\r\n|   512 |    128 |    7 |   1408 |    0.166 |  3082.24 |    2.515 |   356.21 |    2.681 |   525.08 |\r\n|   512 |    128 |    8 |   1536 |    0.166 |  3080.13 |    2.551 |   401.48 |    2.717 |   565.37 |\r\n|   512 |    128 |   16 |   2560 |    0.169 |  3028.26 |    4.723 |   433.60 |    4.892 |   523.27 |\r\n|   512 |    128 |   32 |   4608 |    0.167 |  3059.51 |    7.973 |   513.75 |    8.140 |   566.09 |\r\n|   512 |    128 |   64 |   8704 |    0.164 |  3126.79 |   11.462 |   714.74 |   11.625 |   748.71 |\r\n\r\n```bash\r\nLLAMA_CUBLAS=1 make -j batched-bench && ./batched-bench ./models/codellama-13b/ggml-model-q4_k.gguf 4096 1 99 1 512,3200 128,800 1\r\n```\r\n\r\n### master\r\n\r\nmain: n_kv_max = 4096, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.437 |  1171.55 |    2.124 |    60.28 |    2.561 |   249.95 |\r\n|   512 |    800 |    1 |   1312 |    0.435 |  1176.46 |   14.269 |    56.07 |   14.704 |    89.23 |\r\n|  3200 |    128 |    1 |   3328 |    3.233 |   989.79 |    3.422 |    37.40 |    6.655 |   500.07 |\r\n|  3200 |    800 |    1 |   4000 |    3.219 |   994.22 |   22.392 |    35.73 |   25.611 |   156.19 |\r\n\r\n### PR\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.246 |  2084.35 |    2.021 |    63.32 |    2.267 |   282.30 |\r\n|   512 |    800 |    1 |   1312 |    0.247 |  2074.85 |   13.436 |    59.54 |   13.683 |    95.88 |\r\n|  3200 |    128 |    1 |   3328 |    1.977 |  1618.55 |    2.501 |    51.18 |    4.478 |   743.16 |\r\n|  3200 |    800 |    1 |   4000 |    1.984 |  1613.30 |   16.140 |    49.57 |   18.123 |   220.71 |\r\n\r\n```bash\r\nmake -j && ../scripts/run-all-perf.sh codellama-7b \"f16 q8_0 q4_0 q4_k\" \"-ngl 999 -t 1 -n 128,512 -p 512\"\r\n```\r\n\r\n### master\r\n\r\n| model                   |       size |     params | backend    | ngl |    threads | test       |              t/s |\r\n| ----------------------- | ---------: | ---------: | ---------- | --: | ---------: | ---------- | ---------------: |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  4493.64 \u00b1 25.65 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     54.67 \u00b1 0.04 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     52.65 \u00b1 0.24 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  2038.44 \u00b1 73.46 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     87.10 \u00b1 0.09 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     82.33 \u00b1 0.25 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     | 1901.29 \u00b1 154.12 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    133.48 \u00b1 0.17 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    123.13 \u00b1 0.38 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  1998.98 \u00b1 52.16 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    110.47 \u00b1 0.18 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    101.51 \u00b1 0.29 |\r\n\r\n### PR\r\n\r\n| model                   |       size |     params | backend    | ngl |    threads | test       |              t/s |\r\n| ----------------------- | ---------: | ---------: | ---------- | --: | ---------: | ---------- | ---------------: |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  4513.66 \u00b1 32.46 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     54.14 \u00b1 0.13 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     53.00 \u00b1 0.26 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  3183.63 \u00b1 16.25 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     85.08 \u00b1 0.15 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     82.86 \u00b1 0.14 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  3250.48 \u00b1 22.76 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    130.02 \u00b1 0.09 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    126.12 \u00b1 0.29 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  3661.56 \u00b1 53.75 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    109.18 \u00b1 0.19 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    104.73 \u00b1 0.08 |\r\n\r\n## RTX 4090\r\n\r\n```bash\r\nLLAMA_CUBLAS=1 make -j batched batched-bench && ./batched-bench ./models/codellama-7b/ggml-model-q4_0.gguf 8704 1 99 1 512 128 1,2,3,4,5,6,7,8,16,32,64\r\n```\r\n\r\n### master\r\n\r\nmain: n_kv_max = 8704, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.142 |  3615.61 |    0.915 |   139.85 |    1.057 |   605.55 |\r\n|   512 |    128 |    2 |    768 |    0.107 |  4779.37 |    5.170 |    49.52 |    5.277 |   145.55 |\r\n|   512 |    128 |    3 |    896 |    0.126 |  4050.31 |    5.201 |    73.83 |    5.328 |   168.17 |\r\n|   512 |    128 |    4 |   1024 |    0.120 |  4268.98 |    5.210 |    98.27 |    5.330 |   192.12 |\r\n|   512 |    128 |    5 |   1152 |    0.128 |  3988.04 |    5.240 |   122.15 |    5.368 |   214.60 |\r\n|   512 |    128 |    6 |   1280 |    0.126 |  4062.23 |    5.242 |   146.51 |    5.368 |   238.45 |\r\n|   512 |    128 |    7 |   1408 |    0.138 |  3721.15 |    5.336 |   167.91 |    5.474 |   257.22 |\r\n|   512 |    128 |    8 |   1536 |    0.114 |  4493.71 |    5.302 |   193.12 |    5.416 |   283.59 |\r\n|   512 |    128 |   16 |   2560 |    0.115 |  4435.90 |    5.558 |   368.49 |    5.673 |   451.24 |\r\n|   512 |    128 |   32 |   4608 |    0.118 |  4325.20 |    5.853 |   699.78 |    5.972 |   771.65 |\r\n|   512 |    128 |   64 |   8704 |    0.120 |  4251.54 |    7.116 |  1151.21 |    7.236 |  1202.81 |\r\n\r\n### PR\r\n\r\nmain: n_kv_max = 8704, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.109 |  4697.51 |    0.943 |   135.73 |    1.052 |   608.36 |\r\n|   512 |    128 |    2 |    768 |    0.089 |  5738.82 |    1.389 |   184.31 |    1.478 |   519.57 |\r\n|   512 |    128 |    3 |    896 |    0.099 |  5188.49 |    1.410 |   272.26 |    1.509 |   593.72 |\r\n|   512 |    128 |    4 |   1024 |    0.091 |  5633.43 |    1.438 |   355.94 |    1.529 |   669.57 |\r\n|   512 |    128 |    5 |   1152 |    0.093 |  5476.76 |    1.508 |   424.27 |    1.602 |   719.12 |\r\n|   512 |    128 |    6 |   1280 |    0.086 |  5968.90 |    1.520 |   505.42 |    1.605 |   797.35 |\r\n|   512 |    128 |    7 |   1408 |    0.092 |  5567.34 |    1.546 |   579.54 |    1.638 |   859.57 |\r\n|   512 |    128 |    8 |   1536 |    0.091 |  5653.09 |    1.574 |   650.69 |    1.664 |   922.91 |\r\n|   512 |    128 |   16 |   2560 |    0.084 |  6129.17 |    2.196 |   932.58 |    2.280 |  1123.01 |\r\n|   512 |    128 |   32 |   4608 |    0.099 |  5172.66 |    3.436 |  1192.04 |    3.535 |  1303.50 |\r\n|   512 |    128 |   64 |   8704 |    0.097 |  5279.28 |    6.336 |  1293.00 |    6.433 |  1353.10 |\r\n\r\n\r\n```bash\r\nLLAMA_CUBLAS=1 make -j batched-bench && ./batched-bench ./models/codellama-13b/ggml-model-q4_k.gguf 4096 1 99 1 512,3200 128,800 1\r\n```\r\n\r\n### master\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.226 |  2269.96 |    1.607 |    79.66 |    1.832 |   349.29 |\r\n|   512 |    800 |    1 |   1312 |    0.214 |  2387.60 |   10.557 |    75.78 |   10.771 |   121.80 |\r\n|  3200 |    128 |    1 |   3328 |    1.640 |  1950.80 |    2.294 |    55.80 |    3.934 |   845.87 |\r\n|  3200 |    800 |    1 |   4000 |    1.626 |  1968.49 |   14.876 |    53.78 |   16.501 |   242.41 |\r\n\r\n### PR\r\n\r\nmain: n_kv_max = 4096, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.152 |  3363.09 |    1.612 |    79.40 |    1.764 |   362.73 |\r\n|   512 |    800 |    1 |   1312 |    0.145 |  3522.46 |   10.201 |    78.42 |   10.347 |   126.80 |\r\n|  3200 |    128 |    1 |   3328 |    1.269 |  2521.20 |    1.986 |    64.45 |    3.255 |  1022.30 |\r\n|  3200 |    800 |    1 |   4000 |    1.268 |  2523.72 |   12.692 |    63.03 |   13.960 |   286.53 |\r\n\r\n```bash\r\nmake -j && ../scripts/run-all-perf.sh codellama-7b \"f16 q8_0 q4_0 q4_k\" \"-ngl 999 -t 1 -n 128,512 -p 512\"\r\n```\r\n\r\n### master\r\n\r\n| model                   |       size |     params | backend    | ngl |    threads | test       |              t/s |\r\n| ----------------------- | ---------: | ---------: | ---------- | --: | ---------: | ---------- | ---------------: |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     | 8972.06 \u00b1 345.72 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     62.27 \u00b1 0.19 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     61.54 \u00b1 0.04 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  5272.91 \u00b1 99.01 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    101.73 \u00b1 0.15 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     99.43 \u00b1 0.05 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     | 5017.87 \u00b1 132.87 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    158.38 \u00b1 0.13 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    152.41 \u00b1 0.85 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     | 4763.11 \u00b1 163.93 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    147.54 \u00b1 0.30 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    142.81 \u00b1 0.15 |\r\n\r\n### PR\r\n\r\n| model                   |       size |     params | backend    | ngl |    threads | test       |              t/s |\r\n| ----------------------- | ---------: | ---------: | ---------- | --: | ---------: | ---------- | ---------------: |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     | 9473.31 \u00b1 227.98 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     61.76 \u00b1 0.29 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     61.27 \u00b1 0.01 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   6473.99 \u00b1 4.55 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    100.22 \u00b1 0.14 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     98.65 \u00b1 0.07 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   6693.25 \u00b1 6.18 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    154.33 \u00b1 0.36 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    150.87 \u00b1 0.12 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   7277.83 \u00b1 4.73 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    144.40 \u00b1 0.25 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    141.19 \u00b1 0.07 |\r\n\r\n\r\n## V100\r\n\r\n```bash\r\nLLAMA_CUBLAS=1 make -j batched-bench && ./batched-bench ./models/openllama-7b-v2/ggml-model-q4_k.gguf 4096 1 99 1 512,3200 128,800 1\r\n```\r\n\r\n### master\r\n\r\nmain: n_kv_max = 4096, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.353 |  1451.95 |    1.388 |    92.23 |    1.740 |   367.73 |\r\n|   512 |    800 |    1 |   1312 |    0.351 |  1457.49 |    9.431 |    84.83 |    9.782 |   134.12 |\r\n|  3200 |    128 |    1 |   3328 |    2.648 |  1208.35 |    2.329 |    54.97 |    4.977 |   668.71 |\r\n|  3200 |    800 |    1 |   4000 |    2.653 |  1206.38 |   15.285 |    52.34 |   17.937 |   223.00 |\r\n\r\n### PR\r\n\r\nmain: n_kv_max = 4096, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.211 |  2421.42 |    1.376 |    92.99 |    1.588 |   403.05 |\r\n|   512 |    800 |    1 |   1312 |    0.212 |  2419.59 |    9.145 |    87.48 |    9.357 |   140.22 |\r\n|  3200 |    128 |    1 |   3328 |    1.767 |  1811.34 |    2.026 |    63.17 |    3.793 |   877.43 |\r\n|  3200 |    800 |    1 |   4000 |    1.765 |  1812.75 |   13.127 |    60.94 |   14.892 |   268.60 |\r\n\r\n## A100 80GB\r\n\r\n```bash\r\nLLAMA_CUBLAS=1 make -j batched batched-bench && ./batched-bench ./models/codellama-7b/ggml-model-q4_0.gguf 8704 1 99 1 512 128 1,2,3,4,5,6,7,8,16,32,64\r\n```\r\n\r\n### master\r\n\r\nmain: n_kv_max = 8704, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.234 |  2185.63 |    1.031 |   124.18 |    1.265 |   505.93 |\r\n|   512 |    128 |    2 |    768 |    0.217 |  2363.62 |   10.829 |    23.64 |   11.045 |    69.53 |\r\n|   512 |    128 |    3 |    896 |    0.217 |  2360.95 |   10.870 |    35.33 |   11.087 |    80.81 |\r\n|   512 |    128 |    4 |   1024 |    0.217 |  2361.65 |   10.904 |    46.95 |   11.121 |    92.08 |\r\n|   512 |    128 |    5 |   1152 |    0.217 |  2359.50 |   10.967 |    58.36 |   11.184 |   103.01 |\r\n|   512 |    128 |    6 |   1280 |    0.217 |  2360.19 |   10.993 |    69.86 |   11.210 |   114.19 |\r\n|   512 |    128 |    7 |   1408 |    0.217 |  2360.06 |   11.044 |    81.13 |   11.261 |   125.04 |\r\n|   512 |    128 |    8 |   1536 |    0.217 |  2360.97 |   11.081 |    92.41 |   11.297 |   135.96 |\r\n|   512 |    128 |   16 |   2560 |    0.217 |  2355.54 |   11.536 |   177.53 |   11.754 |   217.81 |\r\n|   512 |    128 |   32 |   4608 |    0.218 |  2346.86 |   11.580 |   353.72 |   11.798 |   390.58 |\r\n|   512 |    128 |   64 |   8704 |    0.220 |  2331.26 |   13.576 |   603.41 |   13.796 |   630.92 |\r\n\r\n### PR\r\n\r\nmain: n_kv_max = 8704, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.115 |  4469.78 |    1.006 |   127.18 |    1.121 |   570.93 |\r\n|   512 |    128 |    2 |    768 |    0.097 |  5258.29 |    1.853 |   138.13 |    1.951 |   393.72 |\r\n|   512 |    128 |    3 |    896 |    0.097 |  5298.23 |    1.893 |   202.84 |    1.990 |   450.31 |\r\n|   512 |    128 |    4 |   1024 |    0.098 |  5247.73 |    1.931 |   265.14 |    2.029 |   504.78 |\r\n|   512 |    128 |    5 |   1152 |    0.097 |  5277.15 |    2.147 |   298.05 |    2.244 |   513.31 |\r\n|   512 |    128 |    6 |   1280 |    0.097 |  5276.45 |    2.176 |   352.95 |    2.273 |   563.13 |\r\n|   512 |    128 |    7 |   1408 |    0.097 |  5289.42 |    2.228 |   402.20 |    2.325 |   605.71 |\r\n|   512 |    128 |    8 |   1536 |    0.097 |  5282.33 |    2.274 |   450.38 |    2.371 |   647.94 |\r\n|   512 |    128 |   16 |   2560 |    0.097 |  5259.21 |    3.386 |   604.89 |    3.483 |   734.98 |\r\n|   512 |    128 |   32 |   4608 |    0.098 |  5212.26 |    5.141 |   796.67 |    5.240 |   879.46 |\r\n|   512 |    128 |   64 |   8704 |    0.100 |  5132.47 |    8.464 |   967.89 |    8.564 |  1016.40 |\r\n\r\n```bash\r\nLLAMA_CUBLAS=1 make -j batched-bench && ./batched-bench ./models/codellama-13b/ggml-model-q4_k.gguf 4096 1 99 1 512,3200 128,800 1\r\n```\r\n\r\n### master\r\n\r\nmain: n_kv_max = 4096, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.462 |  1108.58 |    1.879 |    68.13 |    2.341 |   273.42 |\r\n|   512 |    800 |    1 |   1312 |    0.444 |  1153.87 |   12.526 |    63.87 |   12.970 |   101.16 |\r\n|  3200 |    128 |    1 |   3328 |    3.100 |  1032.42 |    2.914 |    43.92 |    6.014 |   553.38 |\r\n|  3200 |    800 |    1 |   4000 |    3.101 |  1032.07 |   19.025 |    42.05 |   22.126 |   180.78 |\r\n\r\n### PR\r\n\r\nmain: n_kv_max = 4096, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.163 |  3150.34 |    1.842 |    69.50 |    2.004 |   319.31 |\r\n|   512 |    800 |    1 |   1312 |    0.145 |  3532.96 |   12.018 |    66.57 |   12.163 |   107.87 |\r\n|  3200 |    128 |    1 |   3328 |    1.239 |  2582.01 |    2.445 |    52.36 |    3.684 |   903.34 |\r\n|  3200 |    800 |    1 |   4000 |    1.241 |  2579.48 |   15.755 |    50.78 |   16.995 |   235.36 |\r\n\r\n```bash\r\nmake -j && ../scripts/run-all-perf.sh codellama-7b \"f16 q8_0 q4_0 q4_k\" \"-ngl 999 -t 1 -n 128,512 -p 512\"\r\n```\r\n\r\n| model                   |       size |     params | backend    | ngl |    threads | test       |              t/s |\r\n| ----------------------- | ---------: | ---------: | ---------- | --: | ---------: | ---------- | ---------------: |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   7883.90 \u00b1 3.58 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     77.14 \u00b1 0.05 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     74.74 \u00b1 0.03 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   2316.34 \u00b1 1.83 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     98.67 \u00b1 0.04 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     94.81 \u00b1 0.04 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   2389.03 \u00b1 1.81 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    146.72 \u00b1 0.07 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    138.33 \u00b1 0.14 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   2034.51 \u00b1 4.29 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    115.35 \u00b1 0.06 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    110.23 \u00b1 0.12 |\r\n\r\n### PR\r\n\r\n| model                   |       size |     params | backend    | ngl |    threads | test       |              t/s |\r\n| ----------------------- | ---------: | ---------: | ---------- | --: | ---------: | ---------- | ---------------: |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  7912.18 \u00b1 32.14 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     75.84 \u00b1 0.05 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     74.30 \u00b1 0.02 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  5356.38 \u00b1 15.04 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     96.57 \u00b1 0.02 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     94.09 \u00b1 0.14 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   5412.66 \u00b1 8.32 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    141.65 \u00b1 0.06 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    136.63 \u00b1 0.62 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  6044.39 \u00b1 24.43 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    112.27 \u00b1 0.14 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    109.27 \u00b1 0.02 |\r\n\r\n\r\n\r\n# TODO\r\n\r\n- [x] Perform ppl tests to make sure I didn't break something\r\n- [x] Run tests on other cards\r\n- [x] Run tests on other models\r\n- [x] Try to add full MMQ fallback\r\n- [x] Fix `BACKEND_SPLIT` support for `src0`\r\n- [ ] Tune compile-time constants for other CUDA / AMD architectures",
                "reactions": {
                    "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3776/reactions",
                    "total_count": 3,
                    "+1": 2,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 1,
                    "eyes": 0
                },
                "timeline_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3776/timeline",
                "performed_via_github_app": null,
                "state_reason": null
            },
            "comment": {
                "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/comments/1782793367",
                "html_url": "https://github.com/ggerganov/llama.cpp/pull/3776#issuecomment-1782793367",
                "issue_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3776",
                "id": 1782793367,
                "node_id": "IC_kwDOJH_K4M5qQ0SX",
                "user": {
                    "login": "Ph0rk0z",
                    "id": 59298527,
                    "node_id": "MDQ6VXNlcjU5Mjk4NTI3",
                    "avatar_url": "https://avatars.githubusercontent.com/u/59298527?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/Ph0rk0z",
                    "html_url": "https://github.com/Ph0rk0z",
                    "followers_url": "https://api.github.com/users/Ph0rk0z/followers",
                    "following_url": "https://api.github.com/users/Ph0rk0z/following{/other_user}",
                    "gists_url": "https://api.github.com/users/Ph0rk0z/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/Ph0rk0z/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/Ph0rk0z/subscriptions",
                    "organizations_url": "https://api.github.com/users/Ph0rk0z/orgs",
                    "repos_url": "https://api.github.com/users/Ph0rk0z/repos",
                    "events_url": "https://api.github.com/users/Ph0rk0z/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/Ph0rk0z/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "created_at": "2023-10-27T11:59:41Z",
                "updated_at": "2023-10-27T11:59:41Z",
                "author_association": "NONE",
                "body": "> @Ph0rk0z you have one use case, but to seriously use this software in a business context, good batched performance is necessary for quite a few use cases. I do hope they maintain good compatibility though.\r\n\r\nIt hobbles a lot of accessible hardware that people invested money into. I'm not the only one. There are no cheaper 24G cards available. Enjoy running a lot of tiny ineffectual models really really fast I guess.  The v100/A100 folks will be using vllm and TGI as they currently do. You could say stay with the old version if the format didn't change so often but that hasn't been the case. So much for good Ml being accessible rather than big business oriented.",
                "reactions": {
                    "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/comments/1782793367/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "performed_via_github_app": null
            }
        },
        "public": true,
        "created_at": "2023-10-27T11:59:41Z"
    },
    {
        "id": "32897710072",
        "type": "IssueCommentEvent",
        "actor": {
            "id": 44031344,
            "login": "KerfuffleV2",
            "display_login": "KerfuffleV2",
            "gravatar_id": "",
            "url": "https://api.github.com/users/KerfuffleV2",
            "avatar_url": "https://avatars.githubusercontent.com/u/44031344?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "action": "created",
            "issue": {
                "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3796",
                "repository_url": "https://api.github.com/repos/ggerganov/llama.cpp",
                "labels_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3796/labels{/name}",
                "comments_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3796/comments",
                "events_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3796/events",
                "html_url": "https://github.com/ggerganov/llama.cpp/issues/3796",
                "id": 1963584507,
                "node_id": "I_kwDOJH_K4M51Cev7",
                "number": 3796,
                "title": "How to stock prompt's result into txt file ?",
                "user": {
                    "login": "oceanedruenne",
                    "id": 78699361,
                    "node_id": "MDQ6VXNlcjc4Njk5MzYx",
                    "avatar_url": "https://avatars.githubusercontent.com/u/78699361?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/oceanedruenne",
                    "html_url": "https://github.com/oceanedruenne",
                    "followers_url": "https://api.github.com/users/oceanedruenne/followers",
                    "following_url": "https://api.github.com/users/oceanedruenne/following{/other_user}",
                    "gists_url": "https://api.github.com/users/oceanedruenne/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/oceanedruenne/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/oceanedruenne/subscriptions",
                    "organizations_url": "https://api.github.com/users/oceanedruenne/orgs",
                    "repos_url": "https://api.github.com/users/oceanedruenne/repos",
                    "events_url": "https://api.github.com/users/oceanedruenne/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/oceanedruenne/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "labels": [
                    {
                        "id": 5250901855,
                        "node_id": "LA_kwDOJH_K4M8AAAABOPpnXw",
                        "url": "https://api.github.com/repos/ggerganov/llama.cpp/labels/enhancement",
                        "name": "enhancement",
                        "color": "a2eeef",
                        "default": true,
                        "description": "New feature or request"
                    }
                ],
                "state": "open",
                "locked": false,
                "assignee": null,
                "assignees": [],
                "milestone": null,
                "comments": 13,
                "created_at": "2023-10-26T13:30:50Z",
                "updated_at": "2023-10-27T11:53:38Z",
                "closed_at": null,
                "author_association": "NONE",
                "active_lock_reason": null,
                "body": "# Prerequisites\r\n\r\nPlease answer the following questions for yourself before submitting an issue.\r\n\r\n- [X] I am running the latest code. Development is very rapid so there are no tagged versions as of now.\r\n- [X] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [X] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) to make sure that I am creating a new issue that is not already open (or closed).\r\n- [X] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions), and have a new bug or useful enhancement to share.\r\n\r\n# Feature Description\r\nHello ! I would like to retrieve the answer from my prompt and store it in a txt file but I don't see how to do it.\r\nI tried this command : \r\n`./main -m ./models/7B/ggml-model-q4_0.gguf -n 256 --repeat_penalty 1.0 --color -i -r \"User:\" -f prompts/chat-with-bob.txt --prompt \"can you write me a story about a dog\" -n 128  > output.txt\r\n`\r\n\r\nbut it returns Bob talking to himself, instead of answering my question. Sometimes he speaks to me in German or Russian.\r\n\r\nI've also tried to create another program to send the prompt and save the answer, but I get another error saying : \r\n`Use of undeclared identifier 'llama'`\r\n\r\n```cpp\r\n#define LLAMA_API_INTERNAL\r\n#include \"llama.h\"\r\n#include <iostream>\r\n#include \"ggml.h\"\r\n\r\n#include \"ggml-alloc.h\"\r\n\r\nint main() {\r\n    llama::Llama2 llama;\r\n\r\n    llama.sendPrompt(\"Votre prompt ici\");\r\n\r\n    std::string response = llama.recvResponse();\r\n\r\n    std::cout << \"R\u00e9ponse de Llama2 : \" << response << std::endl;\r\n\r\n    return 0;\r\n}\r\n```\r\n\r\n\r\n\r\n\r\n# Motivation\r\n\r\nI would like to do this for a project at school (we are 10 in the group but we have so many work to do lol)\r\n\r\n\r\n\r\n",
                "reactions": {
                    "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3796/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "timeline_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3796/timeline",
                "performed_via_github_app": null,
                "state_reason": null
            },
            "comment": {
                "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/comments/1782785651",
                "html_url": "https://github.com/ggerganov/llama.cpp/issues/3796#issuecomment-1782785651",
                "issue_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3796",
                "id": 1782785651,
                "node_id": "IC_kwDOJH_K4M5qQyZz",
                "user": {
                    "login": "KerfuffleV2",
                    "id": 44031344,
                    "node_id": "MDQ6VXNlcjQ0MDMxMzQ0",
                    "avatar_url": "https://avatars.githubusercontent.com/u/44031344?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/KerfuffleV2",
                    "html_url": "https://github.com/KerfuffleV2",
                    "followers_url": "https://api.github.com/users/KerfuffleV2/followers",
                    "following_url": "https://api.github.com/users/KerfuffleV2/following{/other_user}",
                    "gists_url": "https://api.github.com/users/KerfuffleV2/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/KerfuffleV2/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/KerfuffleV2/subscriptions",
                    "organizations_url": "https://api.github.com/users/KerfuffleV2/orgs",
                    "repos_url": "https://api.github.com/users/KerfuffleV2/repos",
                    "events_url": "https://api.github.com/users/KerfuffleV2/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/KerfuffleV2/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "created_at": "2023-10-27T11:53:38Z",
                "updated_at": "2023-10-27T11:53:38Z",
                "author_association": "COLLABORATOR",
                "body": "> I have a last question: if I want a short story, I'll have to use less tokens, right ?\r\n\r\nKind of. If you make the context shorter, it will just stop generating when the size exceeds what you set. So the model might start telling a long story, and just get interrupted when it hits that limit. (The model doesn't know what the limit is.) If you want the model to tell you a short story, then you'd want to include that in the prompt. Like \"Write me a short two paragraph story about a dog\" or something like that.",
                "reactions": {
                    "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/comments/1782785651/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "performed_via_github_app": null
            }
        },
        "public": true,
        "created_at": "2023-10-27T11:53:39Z"
    },
    {
        "id": "32897611890",
        "type": "IssuesEvent",
        "actor": {
            "id": 2708520,
            "login": "Johnz86",
            "display_login": "Johnz86",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Johnz86",
            "avatar_url": "https://avatars.githubusercontent.com/u/2708520?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "action": "opened",
            "issue": {
                "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3811",
                "repository_url": "https://api.github.com/repos/ggerganov/llama.cpp",
                "labels_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3811/labels{/name}",
                "comments_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3811/comments",
                "events_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3811/events",
                "html_url": "https://github.com/ggerganov/llama.cpp/issues/3811",
                "id": 1965356876,
                "node_id": "I_kwDOJH_K4M51JPdM",
                "number": 3811,
                "title": "Server completion endpoint receive embeddings",
                "user": {
                    "login": "Johnz86",
                    "id": 2708520,
                    "node_id": "MDQ6VXNlcjI3MDg1MjA=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/2708520?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/Johnz86",
                    "html_url": "https://github.com/Johnz86",
                    "followers_url": "https://api.github.com/users/Johnz86/followers",
                    "following_url": "https://api.github.com/users/Johnz86/following{/other_user}",
                    "gists_url": "https://api.github.com/users/Johnz86/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/Johnz86/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/Johnz86/subscriptions",
                    "organizations_url": "https://api.github.com/users/Johnz86/orgs",
                    "repos_url": "https://api.github.com/users/Johnz86/repos",
                    "events_url": "https://api.github.com/users/Johnz86/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/Johnz86/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "labels": [
                    {
                        "id": 5250901855,
                        "node_id": "LA_kwDOJH_K4M8AAAABOPpnXw",
                        "url": "https://api.github.com/repos/ggerganov/llama.cpp/labels/enhancement",
                        "name": "enhancement",
                        "color": "a2eeef",
                        "default": true,
                        "description": "New feature or request"
                    }
                ],
                "state": "open",
                "locked": false,
                "assignee": null,
                "assignees": [],
                "milestone": null,
                "comments": 0,
                "created_at": "2023-10-27T11:49:34Z",
                "updated_at": "2023-10-27T11:49:34Z",
                "closed_at": null,
                "author_association": "NONE",
                "active_lock_reason": null,
                "body": "# Prerequisites\r\n\r\n- [x] I am running the latest code.\r\n- [x] I carefully followed the [README.md](https://github.com/ggerganov/llama.cpp/blob/master/README.md).\r\n- [x] I [searched using keywords relevant to my issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/filtering-and-searching-issues-and-pull-requests) and found no existing open or closed issues.\r\n- [x] I reviewed the [Discussions](https://github.com/ggerganov/llama.cpp/discussions) and decided to open a new issue.\r\n\r\n# Feature Description\r\n\r\nI propose an enhancement to allow `llama.cpp` server to accept document embeddings along with prompts to provide more contextual information when generating responses.\r\n\r\n# Motivation\r\n\r\nThis feature should allow to POST embeddings, along with prompt content to provide contextual information for text generation. It will enrich the context, leading to more accurate and relevant completions.\r\n\r\n# Possible Implementation\r\n\r\nA new endpoint or an extension to the existing `/completion` endpoint could be created to accept embeddings. The embeddings could be sent as a separate property in the JSON request, and be utilized in the generation process to provide context.\r\n",
                "reactions": {
                    "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3811/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "timeline_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3811/timeline",
                "performed_via_github_app": null,
                "state_reason": null
            }
        },
        "public": true,
        "created_at": "2023-10-27T11:49:35Z"
    },
    {
        "id": "32896285341",
        "type": "ForkEvent",
        "actor": {
            "id": 49484167,
            "login": "vincentsider",
            "display_login": "vincentsider",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vincentsider",
            "avatar_url": "https://avatars.githubusercontent.com/u/49484167?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "forkee": {
                "id": 710746074,
                "node_id": "R_kgDOKl0f2g",
                "name": "llama.cpp",
                "full_name": "vincentsider/llama.cpp",
                "private": false,
                "owner": {
                    "login": "vincentsider",
                    "id": 49484167,
                    "node_id": "MDQ6VXNlcjQ5NDg0MTY3",
                    "avatar_url": "https://avatars.githubusercontent.com/u/49484167?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/vincentsider",
                    "html_url": "https://github.com/vincentsider",
                    "followers_url": "https://api.github.com/users/vincentsider/followers",
                    "following_url": "https://api.github.com/users/vincentsider/following{/other_user}",
                    "gists_url": "https://api.github.com/users/vincentsider/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/vincentsider/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/vincentsider/subscriptions",
                    "organizations_url": "https://api.github.com/users/vincentsider/orgs",
                    "repos_url": "https://api.github.com/users/vincentsider/repos",
                    "events_url": "https://api.github.com/users/vincentsider/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/vincentsider/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "html_url": "https://github.com/vincentsider/llama.cpp",
                "description": "Port of Facebook's LLaMA model in C/C++",
                "fork": true,
                "url": "https://api.github.com/repos/vincentsider/llama.cpp",
                "forks_url": "https://api.github.com/repos/vincentsider/llama.cpp/forks",
                "keys_url": "https://api.github.com/repos/vincentsider/llama.cpp/keys{/key_id}",
                "collaborators_url": "https://api.github.com/repos/vincentsider/llama.cpp/collaborators{/collaborator}",
                "teams_url": "https://api.github.com/repos/vincentsider/llama.cpp/teams",
                "hooks_url": "https://api.github.com/repos/vincentsider/llama.cpp/hooks",
                "issue_events_url": "https://api.github.com/repos/vincentsider/llama.cpp/issues/events{/number}",
                "events_url": "https://api.github.com/repos/vincentsider/llama.cpp/events",
                "assignees_url": "https://api.github.com/repos/vincentsider/llama.cpp/assignees{/user}",
                "branches_url": "https://api.github.com/repos/vincentsider/llama.cpp/branches{/branch}",
                "tags_url": "https://api.github.com/repos/vincentsider/llama.cpp/tags",
                "blobs_url": "https://api.github.com/repos/vincentsider/llama.cpp/git/blobs{/sha}",
                "git_tags_url": "https://api.github.com/repos/vincentsider/llama.cpp/git/tags{/sha}",
                "git_refs_url": "https://api.github.com/repos/vincentsider/llama.cpp/git/refs{/sha}",
                "trees_url": "https://api.github.com/repos/vincentsider/llama.cpp/git/trees{/sha}",
                "statuses_url": "https://api.github.com/repos/vincentsider/llama.cpp/statuses/{sha}",
                "languages_url": "https://api.github.com/repos/vincentsider/llama.cpp/languages",
                "stargazers_url": "https://api.github.com/repos/vincentsider/llama.cpp/stargazers",
                "contributors_url": "https://api.github.com/repos/vincentsider/llama.cpp/contributors",
                "subscribers_url": "https://api.github.com/repos/vincentsider/llama.cpp/subscribers",
                "subscription_url": "https://api.github.com/repos/vincentsider/llama.cpp/subscription",
                "commits_url": "https://api.github.com/repos/vincentsider/llama.cpp/commits{/sha}",
                "git_commits_url": "https://api.github.com/repos/vincentsider/llama.cpp/git/commits{/sha}",
                "comments_url": "https://api.github.com/repos/vincentsider/llama.cpp/comments{/number}",
                "issue_comment_url": "https://api.github.com/repos/vincentsider/llama.cpp/issues/comments{/number}",
                "contents_url": "https://api.github.com/repos/vincentsider/llama.cpp/contents/{+path}",
                "compare_url": "https://api.github.com/repos/vincentsider/llama.cpp/compare/{base}...{head}",
                "merges_url": "https://api.github.com/repos/vincentsider/llama.cpp/merges",
                "archive_url": "https://api.github.com/repos/vincentsider/llama.cpp/{archive_format}{/ref}",
                "downloads_url": "https://api.github.com/repos/vincentsider/llama.cpp/downloads",
                "issues_url": "https://api.github.com/repos/vincentsider/llama.cpp/issues{/number}",
                "pulls_url": "https://api.github.com/repos/vincentsider/llama.cpp/pulls{/number}",
                "milestones_url": "https://api.github.com/repos/vincentsider/llama.cpp/milestones{/number}",
                "notifications_url": "https://api.github.com/repos/vincentsider/llama.cpp/notifications{?since,all,participating}",
                "labels_url": "https://api.github.com/repos/vincentsider/llama.cpp/labels{/name}",
                "releases_url": "https://api.github.com/repos/vincentsider/llama.cpp/releases{/id}",
                "deployments_url": "https://api.github.com/repos/vincentsider/llama.cpp/deployments",
                "created_at": "2023-10-27T10:54:11Z",
                "updated_at": "2023-10-27T10:54:11Z",
                "pushed_at": "2023-10-27T10:21:25Z",
                "git_url": "git://github.com/vincentsider/llama.cpp.git",
                "ssh_url": "git@github.com:vincentsider/llama.cpp.git",
                "clone_url": "https://github.com/vincentsider/llama.cpp.git",
                "svn_url": "https://github.com/vincentsider/llama.cpp",
                "homepage": null,
                "size": 12522,
                "stargazers_count": 0,
                "watchers_count": 0,
                "language": null,
                "has_issues": false,
                "has_projects": true,
                "has_downloads": true,
                "has_wiki": true,
                "has_pages": false,
                "has_discussions": false,
                "forks_count": 0,
                "mirror_url": null,
                "archived": false,
                "disabled": false,
                "open_issues_count": 0,
                "license": null,
                "allow_forking": true,
                "is_template": false,
                "web_commit_signoff_required": false,
                "topics": [],
                "visibility": "public",
                "forks": 0,
                "open_issues": 0,
                "watchers": 0,
                "default_branch": "main",
                "public": true
            }
        },
        "public": true,
        "created_at": "2023-10-27T10:54:12Z"
    },
    {
        "id": "32896261441",
        "type": "WatchEvent",
        "actor": {
            "id": 108573093,
            "login": "scooomaker",
            "display_login": "scooomaker",
            "gravatar_id": "",
            "url": "https://api.github.com/users/scooomaker",
            "avatar_url": "https://avatars.githubusercontent.com/u/108573093?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-27T10:53:11Z"
    },
    {
        "id": "32896229857",
        "type": "WatchEvent",
        "actor": {
            "id": 43937008,
            "login": "fxeqxmulfx",
            "display_login": "fxeqxmulfx",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fxeqxmulfx",
            "avatar_url": "https://avatars.githubusercontent.com/u/43937008?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-27T10:51:51Z"
    },
    {
        "id": "32896105774",
        "type": "WatchEvent",
        "actor": {
            "id": 39304339,
            "login": "Wehzie",
            "display_login": "Wehzie",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Wehzie",
            "avatar_url": "https://avatars.githubusercontent.com/u/39304339?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-27T10:46:39Z"
    },
    {
        "id": "32895859624",
        "type": "IssueCommentEvent",
        "actor": {
            "id": 18492268,
            "login": "JohannesGaessler",
            "display_login": "JohannesGaessler",
            "gravatar_id": "",
            "url": "https://api.github.com/users/JohannesGaessler",
            "avatar_url": "https://avatars.githubusercontent.com/u/18492268?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "action": "created",
            "issue": {
                "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3776",
                "repository_url": "https://api.github.com/repos/ggerganov/llama.cpp",
                "labels_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3776/labels{/name}",
                "comments_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3776/comments",
                "events_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3776/events",
                "html_url": "https://github.com/ggerganov/llama.cpp/pull/3776",
                "id": 1961304396,
                "node_id": "PR_kwDOJH_K4M5dv5bW",
                "number": 3776,
                "title": "cuda : improve text-generation and batched decoding performance",
                "user": {
                    "login": "ggerganov",
                    "id": 1991296,
                    "node_id": "MDQ6VXNlcjE5OTEyOTY=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/1991296?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/ggerganov",
                    "html_url": "https://github.com/ggerganov",
                    "followers_url": "https://api.github.com/users/ggerganov/followers",
                    "following_url": "https://api.github.com/users/ggerganov/following{/other_user}",
                    "gists_url": "https://api.github.com/users/ggerganov/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/ggerganov/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/ggerganov/subscriptions",
                    "organizations_url": "https://api.github.com/users/ggerganov/orgs",
                    "repos_url": "https://api.github.com/users/ggerganov/repos",
                    "events_url": "https://api.github.com/users/ggerganov/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/ggerganov/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "labels": [],
                "state": "open",
                "locked": false,
                "assignee": null,
                "assignees": [],
                "milestone": null,
                "comments": 19,
                "created_at": "2023-10-25T12:26:34Z",
                "updated_at": "2023-10-27T10:36:15Z",
                "closed_at": null,
                "author_association": "OWNER",
                "active_lock_reason": null,
                "draft": false,
                "pull_request": {
                    "url": "https://api.github.com/repos/ggerganov/llama.cpp/pulls/3776",
                    "html_url": "https://github.com/ggerganov/llama.cpp/pull/3776",
                    "diff_url": "https://github.com/ggerganov/llama.cpp/pull/3776.diff",
                    "patch_url": "https://github.com/ggerganov/llama.cpp/pull/3776.patch",
                    "merged_at": null
                },
                "body": "ref #3479 \r\nref #3771 \r\n\r\n# Description\r\n\r\nThis PR should improve significantly the text-generation, prompt processing and batched decoding speed for all models for NVIDIA cards with tensor cores (i.e. VOLTA, AMPERE, etc).\r\n\r\n## Prompt processing\r\n\r\nBy default `llama.cpp` uses `MMQ=1` which means that the matrix-matrix multiplications for quantized models are performed with custom kernel for integer multiplications. Recently (#3412), we found out that for large batch dimension (which is the case when processing prompts), `MMQ=0` offers significant performance boost by first dequantizing `src0` to F16 and performing the GEMM using cublas. This PR essentially enables the same optimization for `MMQ=1` by not using the custom kernel for batch size > 32.\r\n\r\n## Batched decoding\r\n\r\nIn this mode, the batch size is larger than 1, but typically small (for example not more than 32). In #3545 we found out that the currently used constants `MMQ_X`, `MMQ_Y` and `NWARPS` are not optimal for small batch sizes. Probably they have been optimized for prompt processing. However, since we now fallback to cuBLAS for prompt processing, the constants can be adjusted for small batch sizes.\r\n\r\n## Text-generation\r\n\r\nSo far, for the KV cache related ops (`KQ` and `KQV`) we have been using custom matrix-vector kernels. For small sequence lengths (~128) and no prompt, these kernels are quite efficient. However, as the KV cache grows with the sequence length it is more efficient to use the tensor cores via cuBLAS GEMM. This PR applies this change to achieve TG improvements for all models when the context is big\r\n\r\nIn summary, we now have the following strategy for matrix multiplications:\r\n\r\n- batch size == 1:\r\n  - non-attention ops && quantized `src0`: use custom matrix-vector kernel\r\n  - otherwise: use cuBLAS GEMM\r\n- batch size <= 32: use custom matrix-matrix kernel\r\n- batch size > 32: use cuBLAS GEMM\r\n\r\n# Results\r\n\r\n## RTX 3090\r\n\r\n```bash\r\nLLAMA_CUBLAS=1 make -j batched batched-bench && ./batched-bench ./models/codellama-7b/ggml-model-q4_0.gguf 8704 1 99 1 512 128 1,2,3,4,5,6,7,8,16,32,64\r\n```\r\n\r\n### master\r\n\r\nmain: n_kv_max = 8704, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.248 |  2063.97 |    1.098 |   116.60 |    1.346 |   475.54 |\r\n|   512 |    128 |    2 |    768 |    0.234 |  2191.11 |    7.435 |    34.43 |    7.669 |   100.15 |\r\n|   512 |    128 |    3 |    896 |    0.233 |  2198.40 |    7.515 |    51.09 |    7.748 |   115.64 |\r\n|   512 |    128 |    4 |   1024 |    0.234 |  2189.40 |    7.553 |    67.78 |    7.787 |   131.50 |\r\n|   512 |    128 |    5 |   1152 |    0.236 |  2168.46 |    7.635 |    83.83 |    7.871 |   146.37 |\r\n|   512 |    128 |    6 |   1280 |    0.236 |  2167.85 |    7.700 |    99.74 |    7.936 |   161.28 |\r\n|   512 |    128 |    7 |   1408 |    0.238 |  2152.19 |    7.768 |   115.34 |    8.006 |   175.86 |\r\n|   512 |    128 |    8 |   1536 |    0.242 |  2115.82 |    7.832 |   130.75 |    8.074 |   190.25 |\r\n|   512 |    128 |   16 |   2560 |    0.244 |  2095.92 |    8.391 |   244.06 |    8.636 |   296.45 |\r\n|   512 |    128 |   32 |   4608 |    0.264 |  1937.69 |    8.852 |   462.71 |    9.116 |   505.46 |\r\n|   512 |    128 |   64 |   8704 |    0.245 |  2085.75 |   11.646 |   703.42 |   11.891 |   731.95 |\r\n\r\n### PR\r\n\r\nmain: n_kv_max = 8704, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.173 |  2953.07 |    1.073 |   119.32 |    1.246 |   513.57 |\r\n|   512 |    128 |    2 |    768 |    0.165 |  3109.78 |    1.715 |   149.23 |    1.880 |   408.49 |\r\n|   512 |    128 |    3 |    896 |    0.162 |  3155.39 |    1.768 |   217.16 |    1.931 |   464.11 |\r\n|   512 |    128 |    4 |   1024 |    0.164 |  3123.65 |    1.803 |   283.98 |    1.967 |   520.63 |\r\n|   512 |    128 |    5 |   1152 |    0.163 |  3138.81 |    2.403 |   266.36 |    2.566 |   448.96 |\r\n|   512 |    128 |    6 |   1280 |    0.166 |  3077.79 |    2.433 |   315.70 |    2.599 |   492.49 |\r\n|   512 |    128 |    7 |   1408 |    0.166 |  3082.24 |    2.515 |   356.21 |    2.681 |   525.08 |\r\n|   512 |    128 |    8 |   1536 |    0.166 |  3080.13 |    2.551 |   401.48 |    2.717 |   565.37 |\r\n|   512 |    128 |   16 |   2560 |    0.169 |  3028.26 |    4.723 |   433.60 |    4.892 |   523.27 |\r\n|   512 |    128 |   32 |   4608 |    0.167 |  3059.51 |    7.973 |   513.75 |    8.140 |   566.09 |\r\n|   512 |    128 |   64 |   8704 |    0.164 |  3126.79 |   11.462 |   714.74 |   11.625 |   748.71 |\r\n\r\n```bash\r\nLLAMA_CUBLAS=1 make -j batched-bench && ./batched-bench ./models/codellama-13b/ggml-model-q4_k.gguf 4096 1 99 1 512,3200 128,800 1\r\n```\r\n\r\n### master\r\n\r\nmain: n_kv_max = 4096, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.437 |  1171.55 |    2.124 |    60.28 |    2.561 |   249.95 |\r\n|   512 |    800 |    1 |   1312 |    0.435 |  1176.46 |   14.269 |    56.07 |   14.704 |    89.23 |\r\n|  3200 |    128 |    1 |   3328 |    3.233 |   989.79 |    3.422 |    37.40 |    6.655 |   500.07 |\r\n|  3200 |    800 |    1 |   4000 |    3.219 |   994.22 |   22.392 |    35.73 |   25.611 |   156.19 |\r\n\r\n### PR\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.246 |  2084.35 |    2.021 |    63.32 |    2.267 |   282.30 |\r\n|   512 |    800 |    1 |   1312 |    0.247 |  2074.85 |   13.436 |    59.54 |   13.683 |    95.88 |\r\n|  3200 |    128 |    1 |   3328 |    1.977 |  1618.55 |    2.501 |    51.18 |    4.478 |   743.16 |\r\n|  3200 |    800 |    1 |   4000 |    1.984 |  1613.30 |   16.140 |    49.57 |   18.123 |   220.71 |\r\n\r\n```bash\r\nmake -j && ../scripts/run-all-perf.sh codellama-7b \"f16 q8_0 q4_0 q4_k\" \"-ngl 999 -t 1 -n 128,512 -p 512\"\r\n```\r\n\r\n### master\r\n\r\n| model                   |       size |     params | backend    | ngl |    threads | test       |              t/s |\r\n| ----------------------- | ---------: | ---------: | ---------- | --: | ---------: | ---------- | ---------------: |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  4493.64 \u00b1 25.65 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     54.67 \u00b1 0.04 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     52.65 \u00b1 0.24 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  2038.44 \u00b1 73.46 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     87.10 \u00b1 0.09 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     82.33 \u00b1 0.25 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     | 1901.29 \u00b1 154.12 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    133.48 \u00b1 0.17 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    123.13 \u00b1 0.38 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  1998.98 \u00b1 52.16 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    110.47 \u00b1 0.18 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    101.51 \u00b1 0.29 |\r\n\r\n### PR\r\n\r\n| model                   |       size |     params | backend    | ngl |    threads | test       |              t/s |\r\n| ----------------------- | ---------: | ---------: | ---------- | --: | ---------: | ---------- | ---------------: |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  4513.66 \u00b1 32.46 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     54.14 \u00b1 0.13 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     53.00 \u00b1 0.26 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  3183.63 \u00b1 16.25 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     85.08 \u00b1 0.15 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     82.86 \u00b1 0.14 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  3250.48 \u00b1 22.76 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    130.02 \u00b1 0.09 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    126.12 \u00b1 0.29 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  3661.56 \u00b1 53.75 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    109.18 \u00b1 0.19 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    104.73 \u00b1 0.08 |\r\n\r\n## RTX 4090\r\n\r\n```bash\r\nLLAMA_CUBLAS=1 make -j batched batched-bench && ./batched-bench ./models/codellama-7b/ggml-model-q4_0.gguf 8704 1 99 1 512 128 1,2,3,4,5,6,7,8,16,32,64\r\n```\r\n\r\n### master\r\n\r\nmain: n_kv_max = 8704, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.142 |  3615.61 |    0.915 |   139.85 |    1.057 |   605.55 |\r\n|   512 |    128 |    2 |    768 |    0.107 |  4779.37 |    5.170 |    49.52 |    5.277 |   145.55 |\r\n|   512 |    128 |    3 |    896 |    0.126 |  4050.31 |    5.201 |    73.83 |    5.328 |   168.17 |\r\n|   512 |    128 |    4 |   1024 |    0.120 |  4268.98 |    5.210 |    98.27 |    5.330 |   192.12 |\r\n|   512 |    128 |    5 |   1152 |    0.128 |  3988.04 |    5.240 |   122.15 |    5.368 |   214.60 |\r\n|   512 |    128 |    6 |   1280 |    0.126 |  4062.23 |    5.242 |   146.51 |    5.368 |   238.45 |\r\n|   512 |    128 |    7 |   1408 |    0.138 |  3721.15 |    5.336 |   167.91 |    5.474 |   257.22 |\r\n|   512 |    128 |    8 |   1536 |    0.114 |  4493.71 |    5.302 |   193.12 |    5.416 |   283.59 |\r\n|   512 |    128 |   16 |   2560 |    0.115 |  4435.90 |    5.558 |   368.49 |    5.673 |   451.24 |\r\n|   512 |    128 |   32 |   4608 |    0.118 |  4325.20 |    5.853 |   699.78 |    5.972 |   771.65 |\r\n|   512 |    128 |   64 |   8704 |    0.120 |  4251.54 |    7.116 |  1151.21 |    7.236 |  1202.81 |\r\n\r\n### PR\r\n\r\nmain: n_kv_max = 8704, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.109 |  4697.51 |    0.943 |   135.73 |    1.052 |   608.36 |\r\n|   512 |    128 |    2 |    768 |    0.089 |  5738.82 |    1.389 |   184.31 |    1.478 |   519.57 |\r\n|   512 |    128 |    3 |    896 |    0.099 |  5188.49 |    1.410 |   272.26 |    1.509 |   593.72 |\r\n|   512 |    128 |    4 |   1024 |    0.091 |  5633.43 |    1.438 |   355.94 |    1.529 |   669.57 |\r\n|   512 |    128 |    5 |   1152 |    0.093 |  5476.76 |    1.508 |   424.27 |    1.602 |   719.12 |\r\n|   512 |    128 |    6 |   1280 |    0.086 |  5968.90 |    1.520 |   505.42 |    1.605 |   797.35 |\r\n|   512 |    128 |    7 |   1408 |    0.092 |  5567.34 |    1.546 |   579.54 |    1.638 |   859.57 |\r\n|   512 |    128 |    8 |   1536 |    0.091 |  5653.09 |    1.574 |   650.69 |    1.664 |   922.91 |\r\n|   512 |    128 |   16 |   2560 |    0.084 |  6129.17 |    2.196 |   932.58 |    2.280 |  1123.01 |\r\n|   512 |    128 |   32 |   4608 |    0.099 |  5172.66 |    3.436 |  1192.04 |    3.535 |  1303.50 |\r\n|   512 |    128 |   64 |   8704 |    0.097 |  5279.28 |    6.336 |  1293.00 |    6.433 |  1353.10 |\r\n\r\n\r\n```bash\r\nLLAMA_CUBLAS=1 make -j batched-bench && ./batched-bench ./models/codellama-13b/ggml-model-q4_k.gguf 4096 1 99 1 512,3200 128,800 1\r\n```\r\n\r\n### master\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.226 |  2269.96 |    1.607 |    79.66 |    1.832 |   349.29 |\r\n|   512 |    800 |    1 |   1312 |    0.214 |  2387.60 |   10.557 |    75.78 |   10.771 |   121.80 |\r\n|  3200 |    128 |    1 |   3328 |    1.640 |  1950.80 |    2.294 |    55.80 |    3.934 |   845.87 |\r\n|  3200 |    800 |    1 |   4000 |    1.626 |  1968.49 |   14.876 |    53.78 |   16.501 |   242.41 |\r\n\r\n### PR\r\n\r\nmain: n_kv_max = 4096, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.152 |  3363.09 |    1.612 |    79.40 |    1.764 |   362.73 |\r\n|   512 |    800 |    1 |   1312 |    0.145 |  3522.46 |   10.201 |    78.42 |   10.347 |   126.80 |\r\n|  3200 |    128 |    1 |   3328 |    1.269 |  2521.20 |    1.986 |    64.45 |    3.255 |  1022.30 |\r\n|  3200 |    800 |    1 |   4000 |    1.268 |  2523.72 |   12.692 |    63.03 |   13.960 |   286.53 |\r\n\r\n```bash\r\nmake -j && ../scripts/run-all-perf.sh codellama-7b \"f16 q8_0 q4_0 q4_k\" \"-ngl 999 -t 1 -n 128,512 -p 512\"\r\n```\r\n\r\n### master\r\n\r\n| model                   |       size |     params | backend    | ngl |    threads | test       |              t/s |\r\n| ----------------------- | ---------: | ---------: | ---------- | --: | ---------: | ---------- | ---------------: |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     | 8972.06 \u00b1 345.72 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     62.27 \u00b1 0.19 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     61.54 \u00b1 0.04 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  5272.91 \u00b1 99.01 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    101.73 \u00b1 0.15 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     99.43 \u00b1 0.05 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     | 5017.87 \u00b1 132.87 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    158.38 \u00b1 0.13 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    152.41 \u00b1 0.85 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     | 4763.11 \u00b1 163.93 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    147.54 \u00b1 0.30 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    142.81 \u00b1 0.15 |\r\n\r\n### PR\r\n\r\n| model                   |       size |     params | backend    | ngl |    threads | test       |              t/s |\r\n| ----------------------- | ---------: | ---------: | ---------- | --: | ---------: | ---------- | ---------------: |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     | 9473.31 \u00b1 227.98 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     61.76 \u00b1 0.29 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     61.27 \u00b1 0.01 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   6473.99 \u00b1 4.55 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    100.22 \u00b1 0.14 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     98.65 \u00b1 0.07 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   6693.25 \u00b1 6.18 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    154.33 \u00b1 0.36 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    150.87 \u00b1 0.12 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   7277.83 \u00b1 4.73 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    144.40 \u00b1 0.25 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    141.19 \u00b1 0.07 |\r\n\r\n\r\n## V100\r\n\r\n```bash\r\nLLAMA_CUBLAS=1 make -j batched-bench && ./batched-bench ./models/openllama-7b-v2/ggml-model-q4_k.gguf 4096 1 99 1 512,3200 128,800 1\r\n```\r\n\r\n### master\r\n\r\nmain: n_kv_max = 4096, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.353 |  1451.95 |    1.388 |    92.23 |    1.740 |   367.73 |\r\n|   512 |    800 |    1 |   1312 |    0.351 |  1457.49 |    9.431 |    84.83 |    9.782 |   134.12 |\r\n|  3200 |    128 |    1 |   3328 |    2.648 |  1208.35 |    2.329 |    54.97 |    4.977 |   668.71 |\r\n|  3200 |    800 |    1 |   4000 |    2.653 |  1206.38 |   15.285 |    52.34 |   17.937 |   223.00 |\r\n\r\n### PR\r\n\r\nmain: n_kv_max = 4096, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.211 |  2421.42 |    1.376 |    92.99 |    1.588 |   403.05 |\r\n|   512 |    800 |    1 |   1312 |    0.212 |  2419.59 |    9.145 |    87.48 |    9.357 |   140.22 |\r\n|  3200 |    128 |    1 |   3328 |    1.767 |  1811.34 |    2.026 |    63.17 |    3.793 |   877.43 |\r\n|  3200 |    800 |    1 |   4000 |    1.765 |  1812.75 |   13.127 |    60.94 |   14.892 |   268.60 |\r\n\r\n## A100 80GB\r\n\r\n```bash\r\nLLAMA_CUBLAS=1 make -j batched batched-bench && ./batched-bench ./models/codellama-7b/ggml-model-q4_0.gguf 8704 1 99 1 512 128 1,2,3,4,5,6,7,8,16,32,64\r\n```\r\n\r\n### master\r\n\r\nmain: n_kv_max = 8704, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.234 |  2185.63 |    1.031 |   124.18 |    1.265 |   505.93 |\r\n|   512 |    128 |    2 |    768 |    0.217 |  2363.62 |   10.829 |    23.64 |   11.045 |    69.53 |\r\n|   512 |    128 |    3 |    896 |    0.217 |  2360.95 |   10.870 |    35.33 |   11.087 |    80.81 |\r\n|   512 |    128 |    4 |   1024 |    0.217 |  2361.65 |   10.904 |    46.95 |   11.121 |    92.08 |\r\n|   512 |    128 |    5 |   1152 |    0.217 |  2359.50 |   10.967 |    58.36 |   11.184 |   103.01 |\r\n|   512 |    128 |    6 |   1280 |    0.217 |  2360.19 |   10.993 |    69.86 |   11.210 |   114.19 |\r\n|   512 |    128 |    7 |   1408 |    0.217 |  2360.06 |   11.044 |    81.13 |   11.261 |   125.04 |\r\n|   512 |    128 |    8 |   1536 |    0.217 |  2360.97 |   11.081 |    92.41 |   11.297 |   135.96 |\r\n|   512 |    128 |   16 |   2560 |    0.217 |  2355.54 |   11.536 |   177.53 |   11.754 |   217.81 |\r\n|   512 |    128 |   32 |   4608 |    0.218 |  2346.86 |   11.580 |   353.72 |   11.798 |   390.58 |\r\n|   512 |    128 |   64 |   8704 |    0.220 |  2331.26 |   13.576 |   603.41 |   13.796 |   630.92 |\r\n\r\n### PR\r\n\r\nmain: n_kv_max = 8704, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.115 |  4469.78 |    1.006 |   127.18 |    1.121 |   570.93 |\r\n|   512 |    128 |    2 |    768 |    0.097 |  5258.29 |    1.853 |   138.13 |    1.951 |   393.72 |\r\n|   512 |    128 |    3 |    896 |    0.097 |  5298.23 |    1.893 |   202.84 |    1.990 |   450.31 |\r\n|   512 |    128 |    4 |   1024 |    0.098 |  5247.73 |    1.931 |   265.14 |    2.029 |   504.78 |\r\n|   512 |    128 |    5 |   1152 |    0.097 |  5277.15 |    2.147 |   298.05 |    2.244 |   513.31 |\r\n|   512 |    128 |    6 |   1280 |    0.097 |  5276.45 |    2.176 |   352.95 |    2.273 |   563.13 |\r\n|   512 |    128 |    7 |   1408 |    0.097 |  5289.42 |    2.228 |   402.20 |    2.325 |   605.71 |\r\n|   512 |    128 |    8 |   1536 |    0.097 |  5282.33 |    2.274 |   450.38 |    2.371 |   647.94 |\r\n|   512 |    128 |   16 |   2560 |    0.097 |  5259.21 |    3.386 |   604.89 |    3.483 |   734.98 |\r\n|   512 |    128 |   32 |   4608 |    0.098 |  5212.26 |    5.141 |   796.67 |    5.240 |   879.46 |\r\n|   512 |    128 |   64 |   8704 |    0.100 |  5132.47 |    8.464 |   967.89 |    8.564 |  1016.40 |\r\n\r\n```bash\r\nLLAMA_CUBLAS=1 make -j batched-bench && ./batched-bench ./models/codellama-13b/ggml-model-q4_k.gguf 4096 1 99 1 512,3200 128,800 1\r\n```\r\n\r\n### master\r\n\r\nmain: n_kv_max = 4096, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.462 |  1108.58 |    1.879 |    68.13 |    2.341 |   273.42 |\r\n|   512 |    800 |    1 |   1312 |    0.444 |  1153.87 |   12.526 |    63.87 |   12.970 |   101.16 |\r\n|  3200 |    128 |    1 |   3328 |    3.100 |  1032.42 |    2.914 |    43.92 |    6.014 |   553.38 |\r\n|  3200 |    800 |    1 |   4000 |    3.101 |  1032.07 |   19.025 |    42.05 |   22.126 |   180.78 |\r\n\r\n### PR\r\n\r\nmain: n_kv_max = 4096, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.163 |  3150.34 |    1.842 |    69.50 |    2.004 |   319.31 |\r\n|   512 |    800 |    1 |   1312 |    0.145 |  3532.96 |   12.018 |    66.57 |   12.163 |   107.87 |\r\n|  3200 |    128 |    1 |   3328 |    1.239 |  2582.01 |    2.445 |    52.36 |    3.684 |   903.34 |\r\n|  3200 |    800 |    1 |   4000 |    1.241 |  2579.48 |   15.755 |    50.78 |   16.995 |   235.36 |\r\n\r\n```bash\r\nmake -j && ../scripts/run-all-perf.sh codellama-7b \"f16 q8_0 q4_0 q4_k\" \"-ngl 999 -t 1 -n 128,512 -p 512\"\r\n```\r\n\r\n| model                   |       size |     params | backend    | ngl |    threads | test       |              t/s |\r\n| ----------------------- | ---------: | ---------: | ---------- | --: | ---------: | ---------- | ---------------: |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   7883.90 \u00b1 3.58 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     77.14 \u00b1 0.05 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     74.74 \u00b1 0.03 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   2316.34 \u00b1 1.83 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     98.67 \u00b1 0.04 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     94.81 \u00b1 0.04 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   2389.03 \u00b1 1.81 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    146.72 \u00b1 0.07 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    138.33 \u00b1 0.14 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   2034.51 \u00b1 4.29 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    115.35 \u00b1 0.06 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    110.23 \u00b1 0.12 |\r\n\r\n### PR\r\n\r\n| model                   |       size |     params | backend    | ngl |    threads | test       |              t/s |\r\n| ----------------------- | ---------: | ---------: | ---------- | --: | ---------: | ---------- | ---------------: |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  7912.18 \u00b1 32.14 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     75.84 \u00b1 0.05 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     74.30 \u00b1 0.02 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  5356.38 \u00b1 15.04 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     96.57 \u00b1 0.02 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     94.09 \u00b1 0.14 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   5412.66 \u00b1 8.32 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    141.65 \u00b1 0.06 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    136.63 \u00b1 0.62 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  6044.39 \u00b1 24.43 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    112.27 \u00b1 0.14 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    109.27 \u00b1 0.02 |\r\n\r\n\r\n\r\n# TODO\r\n\r\n- [x] Perform ppl tests to make sure I didn't break something\r\n- [x] Run tests on other cards\r\n- [x] Run tests on other models\r\n- [x] Try to add full MMQ fallback\r\n- [x] Fix `BACKEND_SPLIT` support for `src0`\r\n- [ ] Tune compile-time constants for other CUDA / AMD architectures",
                "reactions": {
                    "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3776/reactions",
                    "total_count": 3,
                    "+1": 2,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 1,
                    "eyes": 0
                },
                "timeline_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3776/timeline",
                "performed_via_github_app": null,
                "state_reason": null
            },
            "comment": {
                "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/comments/1782687658",
                "html_url": "https://github.com/ggerganov/llama.cpp/pull/3776#issuecomment-1782687658",
                "issue_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3776",
                "id": 1782687658,
                "node_id": "IC_kwDOJH_K4M5qQaeq",
                "user": {
                    "login": "JohannesGaessler",
                    "id": 18492268,
                    "node_id": "MDQ6VXNlcjE4NDkyMjY4",
                    "avatar_url": "https://avatars.githubusercontent.com/u/18492268?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/JohannesGaessler",
                    "html_url": "https://github.com/JohannesGaessler",
                    "followers_url": "https://api.github.com/users/JohannesGaessler/followers",
                    "following_url": "https://api.github.com/users/JohannesGaessler/following{/other_user}",
                    "gists_url": "https://api.github.com/users/JohannesGaessler/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/JohannesGaessler/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/JohannesGaessler/subscriptions",
                    "organizations_url": "https://api.github.com/users/JohannesGaessler/orgs",
                    "repos_url": "https://api.github.com/users/JohannesGaessler/repos",
                    "events_url": "https://api.github.com/users/JohannesGaessler/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/JohannesGaessler/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "created_at": "2023-10-27T10:36:15Z",
                "updated_at": "2023-10-27T10:36:15Z",
                "author_association": "COLLABORATOR",
                "body": "To clarify my perspective: if the increase in VRAM usage is ~1% as previously suggested I am completely fine with this PR and do not think a compilation option for mmq only is necessary. However, to my understanding mmq is currently still used for the output tensor which is by far the largest and therefore requires the most VRAM. So prior to merging I would like there to be a final VRAM measurement.\r\n\r\nAlso the multi GPU performance should be checked. Currently with mmq the hidden state is converted to q8_1 prior to being distributed from the main GPU to the other GPUs. This significantly reduces latency and bandwidth and therefore improves performance. So cuBLAS may still be slower in multi GPU settings even with the presence of tensor cores. Although for batched decoding in setting with one server and multiple clients a different parallelization scheme where the GPUs run sequentially rather than in parallel would be much more efficient anyways (it is extremely unlikely that I will implement this because it is not a use case that I care about).",
                "reactions": {
                    "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/comments/1782687658/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "performed_via_github_app": null
            }
        },
        "public": true,
        "created_at": "2023-10-27T10:36:16Z"
    },
    {
        "id": "32895801518",
        "type": "WatchEvent",
        "actor": {
            "id": 43927507,
            "login": "archetipico",
            "display_login": "archetipico",
            "gravatar_id": "",
            "url": "https://api.github.com/users/archetipico",
            "avatar_url": "https://avatars.githubusercontent.com/u/43927507?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-27T10:33:50Z"
    },
    {
        "id": "32895475727",
        "type": "PushEvent",
        "actor": {
            "id": 1991296,
            "login": "ggerganov",
            "display_login": "ggerganov",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ggerganov",
            "avatar_url": "https://avatars.githubusercontent.com/u/1991296?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "repository_id": 612354784,
            "push_id": 15590140929,
            "size": 1,
            "distinct_size": 1,
            "ref": "refs/heads/cuda-quantum-batch",
            "head": "49af767fadfc44dd079d49038089b4ee99d77e0c",
            "before": "a4e15a36e4cd7120945eef560e591efaeb5fbd2b",
            "commits": [
                {
                    "sha": "49af767fadfc44dd079d49038089b4ee99d77e0c",
                    "author": {
                        "email": "ggerganov@gmail.com",
                        "name": "Georgi Gerganov"
                    },
                    "message": "build : add compile option to force use of MMQ kernels",
                    "distinct": true,
                    "url": "https://api.github.com/repos/ggerganov/llama.cpp/commits/49af767fadfc44dd079d49038089b4ee99d77e0c"
                }
            ]
        },
        "public": true,
        "created_at": "2023-10-27T10:21:24Z"
    },
    {
        "id": "32895387123",
        "type": "ForkEvent",
        "actor": {
            "id": 95281313,
            "login": "virtuanista",
            "display_login": "virtuanista",
            "gravatar_id": "",
            "url": "https://api.github.com/users/virtuanista",
            "avatar_url": "https://avatars.githubusercontent.com/u/95281313?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "forkee": {
                "id": 710733096,
                "node_id": "R_kgDOKlztKA",
                "name": "llama.cpp",
                "full_name": "virtuanista/llama.cpp",
                "private": false,
                "owner": {
                    "login": "virtuanista",
                    "id": 95281313,
                    "node_id": "U_kgDOBa3goQ",
                    "avatar_url": "https://avatars.githubusercontent.com/u/95281313?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/virtuanista",
                    "html_url": "https://github.com/virtuanista",
                    "followers_url": "https://api.github.com/users/virtuanista/followers",
                    "following_url": "https://api.github.com/users/virtuanista/following{/other_user}",
                    "gists_url": "https://api.github.com/users/virtuanista/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/virtuanista/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/virtuanista/subscriptions",
                    "organizations_url": "https://api.github.com/users/virtuanista/orgs",
                    "repos_url": "https://api.github.com/users/virtuanista/repos",
                    "events_url": "https://api.github.com/users/virtuanista/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/virtuanista/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "html_url": "https://github.com/virtuanista/llama.cpp",
                "description": "Port del modelo LLaMA de Facebook en C/C++",
                "fork": true,
                "url": "https://api.github.com/repos/virtuanista/llama.cpp",
                "forks_url": "https://api.github.com/repos/virtuanista/llama.cpp/forks",
                "keys_url": "https://api.github.com/repos/virtuanista/llama.cpp/keys{/key_id}",
                "collaborators_url": "https://api.github.com/repos/virtuanista/llama.cpp/collaborators{/collaborator}",
                "teams_url": "https://api.github.com/repos/virtuanista/llama.cpp/teams",
                "hooks_url": "https://api.github.com/repos/virtuanista/llama.cpp/hooks",
                "issue_events_url": "https://api.github.com/repos/virtuanista/llama.cpp/issues/events{/number}",
                "events_url": "https://api.github.com/repos/virtuanista/llama.cpp/events",
                "assignees_url": "https://api.github.com/repos/virtuanista/llama.cpp/assignees{/user}",
                "branches_url": "https://api.github.com/repos/virtuanista/llama.cpp/branches{/branch}",
                "tags_url": "https://api.github.com/repos/virtuanista/llama.cpp/tags",
                "blobs_url": "https://api.github.com/repos/virtuanista/llama.cpp/git/blobs{/sha}",
                "git_tags_url": "https://api.github.com/repos/virtuanista/llama.cpp/git/tags{/sha}",
                "git_refs_url": "https://api.github.com/repos/virtuanista/llama.cpp/git/refs{/sha}",
                "trees_url": "https://api.github.com/repos/virtuanista/llama.cpp/git/trees{/sha}",
                "statuses_url": "https://api.github.com/repos/virtuanista/llama.cpp/statuses/{sha}",
                "languages_url": "https://api.github.com/repos/virtuanista/llama.cpp/languages",
                "stargazers_url": "https://api.github.com/repos/virtuanista/llama.cpp/stargazers",
                "contributors_url": "https://api.github.com/repos/virtuanista/llama.cpp/contributors",
                "subscribers_url": "https://api.github.com/repos/virtuanista/llama.cpp/subscribers",
                "subscription_url": "https://api.github.com/repos/virtuanista/llama.cpp/subscription",
                "commits_url": "https://api.github.com/repos/virtuanista/llama.cpp/commits{/sha}",
                "git_commits_url": "https://api.github.com/repos/virtuanista/llama.cpp/git/commits{/sha}",
                "comments_url": "https://api.github.com/repos/virtuanista/llama.cpp/comments{/number}",
                "issue_comment_url": "https://api.github.com/repos/virtuanista/llama.cpp/issues/comments{/number}",
                "contents_url": "https://api.github.com/repos/virtuanista/llama.cpp/contents/{+path}",
                "compare_url": "https://api.github.com/repos/virtuanista/llama.cpp/compare/{base}...{head}",
                "merges_url": "https://api.github.com/repos/virtuanista/llama.cpp/merges",
                "archive_url": "https://api.github.com/repos/virtuanista/llama.cpp/{archive_format}{/ref}",
                "downloads_url": "https://api.github.com/repos/virtuanista/llama.cpp/downloads",
                "issues_url": "https://api.github.com/repos/virtuanista/llama.cpp/issues{/number}",
                "pulls_url": "https://api.github.com/repos/virtuanista/llama.cpp/pulls{/number}",
                "milestones_url": "https://api.github.com/repos/virtuanista/llama.cpp/milestones{/number}",
                "notifications_url": "https://api.github.com/repos/virtuanista/llama.cpp/notifications{?since,all,participating}",
                "labels_url": "https://api.github.com/repos/virtuanista/llama.cpp/labels{/name}",
                "releases_url": "https://api.github.com/repos/virtuanista/llama.cpp/releases{/id}",
                "deployments_url": "https://api.github.com/repos/virtuanista/llama.cpp/deployments",
                "created_at": "2023-10-27T10:18:15Z",
                "updated_at": "2023-10-27T10:18:15Z",
                "pushed_at": "2023-10-27T10:06:07Z",
                "git_url": "git://github.com/virtuanista/llama.cpp.git",
                "ssh_url": "git@github.com:virtuanista/llama.cpp.git",
                "clone_url": "https://github.com/virtuanista/llama.cpp.git",
                "svn_url": "https://github.com/virtuanista/llama.cpp",
                "homepage": null,
                "size": 12522,
                "stargazers_count": 0,
                "watchers_count": 0,
                "language": null,
                "has_issues": false,
                "has_projects": true,
                "has_downloads": true,
                "has_wiki": true,
                "has_pages": false,
                "has_discussions": false,
                "forks_count": 0,
                "mirror_url": null,
                "archived": false,
                "disabled": false,
                "open_issues_count": 0,
                "license": null,
                "allow_forking": true,
                "is_template": false,
                "web_commit_signoff_required": false,
                "topics": [],
                "visibility": "public",
                "forks": 0,
                "open_issues": 0,
                "watchers": 0,
                "default_branch": "main",
                "public": true
            }
        },
        "public": true,
        "created_at": "2023-10-27T10:18:16Z"
    },
    {
        "id": "32895186683",
        "type": "IssueCommentEvent",
        "actor": {
            "id": 242550,
            "login": "jonastemplestein",
            "display_login": "jonastemplestein",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jonastemplestein",
            "avatar_url": "https://avatars.githubusercontent.com/u/242550?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "action": "created",
            "issue": {
                "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3801",
                "repository_url": "https://api.github.com/repos/ggerganov/llama.cpp",
                "labels_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3801/labels{/name}",
                "comments_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3801/comments",
                "events_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3801/events",
                "html_url": "https://github.com/ggerganov/llama.cpp/issues/3801",
                "id": 1964223167,
                "node_id": "I_kwDOJH_K4M51E6q_",
                "number": 3801,
                "title": "Segfault when using simple self-referential grammar on M2 Mac",
                "user": {
                    "login": "jonastemplestein",
                    "id": 242550,
                    "node_id": "MDQ6VXNlcjI0MjU1MA==",
                    "avatar_url": "https://avatars.githubusercontent.com/u/242550?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/jonastemplestein",
                    "html_url": "https://github.com/jonastemplestein",
                    "followers_url": "https://api.github.com/users/jonastemplestein/followers",
                    "following_url": "https://api.github.com/users/jonastemplestein/following{/other_user}",
                    "gists_url": "https://api.github.com/users/jonastemplestein/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/jonastemplestein/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/jonastemplestein/subscriptions",
                    "organizations_url": "https://api.github.com/users/jonastemplestein/orgs",
                    "repos_url": "https://api.github.com/users/jonastemplestein/repos",
                    "events_url": "https://api.github.com/users/jonastemplestein/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/jonastemplestein/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "labels": [
                    {
                        "id": 5250901848,
                        "node_id": "LA_kwDOJH_K4M8AAAABOPpnWA",
                        "url": "https://api.github.com/repos/ggerganov/llama.cpp/labels/bug",
                        "name": "bug",
                        "color": "d73a4a",
                        "default": true,
                        "description": "Something isn't working"
                    }
                ],
                "state": "open",
                "locked": false,
                "assignee": null,
                "assignees": [],
                "milestone": null,
                "comments": 4,
                "created_at": "2023-10-26T19:18:20Z",
                "updated_at": "2023-10-27T10:10:30Z",
                "closed_at": null,
                "author_association": "NONE",
                "active_lock_reason": null,
                "body": "Hey folks, first of all massive thanks for this amazing project and in particular grammar support - it honestly feels like magic <3\r\n\r\nI was inspired by [this tweet](https://x.com/josevalim/status/1717201320799535578) that @ggerganov retweeted to try converting tree sitter grammars to GBNF form. After some hiccups I had a script that created grammar's that could be parsed by llama.cpp, but unfortunately I'm now getting segmentation faults.\r\n\r\nI've isolated it to this simple example grammar:\r\n\r\n```\r\nroot ::=  expression\r\nexpression ::= (integer | binary-operator)\r\ninteger ::=  [0-9]+\r\nbinary-operator ::=  expression  (\"+\"  |  \"-\")  expression\r\n```\r\n\r\nHere's the segfault. I've included the full arguments and shortened output of llama.cpp:\r\n```\r\n% ./main --grammar-file elixir.gbnf --model mistral-7b-instruct-v0.1.Q6_K.gguf -t 7 -b 24 -n -1 --temp 0 -ngl 1 -ins\r\n\r\nLog start\r\nmain: build = 1428 (6961c4b)\r\nmain: built with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.6.0\r\nmain: seed  = 1698347229\r\n\r\n[... the usual loading of the model output...]\r\n\r\n== Running in interactive mode. ==\r\n - Press Ctrl+C to interject at any time.\r\n - Press Return to return control to LLaMa.\r\n - To return control without starting a new line, end your input with '/'.\r\n - If you want to submit another line, end your input with '\\'.\r\n\r\nzsh: segmentation fault  ./main --grammar-file  --color --model  -t 7 -b 24 -n -1 --temp 0 -ngl 1 -ins\r\n```\r\n\r\nA few other observations:\r\n\r\n- The shell hangs for a couple of seconds before the segfault, suggesting there might be an infinite loop\r\n- This doesn't happen if the grammar isn't self-referential (e.g. by replacing the last line with `binary-operator ::=  integer  (\"+\"  |  \"-\")  integer`\r\n- This happens irrespective of which model I use and doesn't happen without using a grammar\r\n- It's not a grammar parse error. Those would happen before model loading and print specific error messages\r\n\r\nAny help would be much appreciated. It'd be really cool if we could force llama.cpp to output any programming language that tree sitter supports. But most of them contain this sort of self-referential symbols.",
                "reactions": {
                    "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3801/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "timeline_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3801/timeline",
                "performed_via_github_app": null,
                "state_reason": null
            },
            "comment": {
                "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/comments/1782654102",
                "html_url": "https://github.com/ggerganov/llama.cpp/issues/3801#issuecomment-1782654102",
                "issue_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3801",
                "id": 1782654102,
                "node_id": "IC_kwDOJH_K4M5qQSSW",
                "user": {
                    "login": "jonastemplestein",
                    "id": 242550,
                    "node_id": "MDQ6VXNlcjI0MjU1MA==",
                    "avatar_url": "https://avatars.githubusercontent.com/u/242550?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/jonastemplestein",
                    "html_url": "https://github.com/jonastemplestein",
                    "followers_url": "https://api.github.com/users/jonastemplestein/followers",
                    "following_url": "https://api.github.com/users/jonastemplestein/following{/other_user}",
                    "gists_url": "https://api.github.com/users/jonastemplestein/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/jonastemplestein/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/jonastemplestein/subscriptions",
                    "organizations_url": "https://api.github.com/users/jonastemplestein/orgs",
                    "repos_url": "https://api.github.com/users/jonastemplestein/repos",
                    "events_url": "https://api.github.com/users/jonastemplestein/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/jonastemplestein/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "created_at": "2023-10-27T10:10:30Z",
                "updated_at": "2023-10-27T10:10:30Z",
                "author_association": "NONE",
                "body": "Oh amazing, thank you so much. I can just insert a whitespace symbol on the left and the toy grammar works:\r\n```\r\nroot ::=  expression\r\nexpression ::= (integer | binary-operator)\r\ninteger ::=  [0-9]+\r\nbinary-operator ::=  ws expression  (\"+\"  |  \"-\")  expression\r\nws ::= [ \\n\\t]+\r\n```\r\n\r\nI've prepended whitespace symbols to various definitions in my Elixir grammar and I can now get to the input prompt! Exciting!\r\n\r\nUnfortunately llama.cpp still doesn't generate Elixir code. It just keeps giving either single word answers or printing an infinite series of newlines.\r\n\r\nAnd each subsequent token that llama.cpp generates takes longer than the first one, suggesting to me that the whole output string is parsed in the grammar each time, as opposed to tagging the output as we go along and storing that state. Is that true? \r\n\r\nIn that case we are probably some way off from using formal grammars to force llama.cpp to output only valid code in high level programming languages.\r\n\r\nIn case you're interested, you can find the full elixir grammar that is causing this behaviour in [this repo](https://github.com/jonastemplestein/tree_sitter_grammar_to_gbnf) (elixir_no_left_recursion.gbnf) alongside some more notes\r\n\r\n",
                "reactions": {
                    "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/comments/1782654102/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "performed_via_github_app": null
            }
        },
        "public": true,
        "created_at": "2023-10-27T10:10:31Z"
    },
    {
        "id": "32895074258",
        "type": "CreateEvent",
        "actor": {
            "id": 1991296,
            "login": "ggerganov",
            "display_login": "ggerganov",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ggerganov",
            "avatar_url": "https://avatars.githubusercontent.com/u/1991296?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "ref": "cuda-multi-gpu",
            "ref_type": "branch",
            "master_branch": "master",
            "description": "Port of Facebook's LLaMA model in C/C++",
            "pusher_type": "user"
        },
        "public": true,
        "created_at": "2023-10-27T10:06:10Z"
    },
    {
        "id": "32894853105",
        "type": "ForkEvent",
        "actor": {
            "id": 13460680,
            "login": "xuetuyic1",
            "display_login": "xuetuyic1",
            "gravatar_id": "",
            "url": "https://api.github.com/users/xuetuyic1",
            "avatar_url": "https://avatars.githubusercontent.com/u/13460680?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "forkee": {
                "id": 710725707,
                "node_id": "R_kgDOKlzQSw",
                "name": "llama",
                "full_name": "xuetuyic1/llama",
                "private": false,
                "owner": {
                    "login": "xuetuyic1",
                    "id": 13460680,
                    "node_id": "MDQ6VXNlcjEzNDYwNjgw",
                    "avatar_url": "https://avatars.githubusercontent.com/u/13460680?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/xuetuyic1",
                    "html_url": "https://github.com/xuetuyic1",
                    "followers_url": "https://api.github.com/users/xuetuyic1/followers",
                    "following_url": "https://api.github.com/users/xuetuyic1/following{/other_user}",
                    "gists_url": "https://api.github.com/users/xuetuyic1/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/xuetuyic1/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/xuetuyic1/subscriptions",
                    "organizations_url": "https://api.github.com/users/xuetuyic1/orgs",
                    "repos_url": "https://api.github.com/users/xuetuyic1/repos",
                    "events_url": "https://api.github.com/users/xuetuyic1/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/xuetuyic1/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "html_url": "https://github.com/xuetuyic1/llama",
                "description": "Port of Facebook's LLaMA model in C/C++",
                "fork": true,
                "url": "https://api.github.com/repos/xuetuyic1/llama",
                "forks_url": "https://api.github.com/repos/xuetuyic1/llama/forks",
                "keys_url": "https://api.github.com/repos/xuetuyic1/llama/keys{/key_id}",
                "collaborators_url": "https://api.github.com/repos/xuetuyic1/llama/collaborators{/collaborator}",
                "teams_url": "https://api.github.com/repos/xuetuyic1/llama/teams",
                "hooks_url": "https://api.github.com/repos/xuetuyic1/llama/hooks",
                "issue_events_url": "https://api.github.com/repos/xuetuyic1/llama/issues/events{/number}",
                "events_url": "https://api.github.com/repos/xuetuyic1/llama/events",
                "assignees_url": "https://api.github.com/repos/xuetuyic1/llama/assignees{/user}",
                "branches_url": "https://api.github.com/repos/xuetuyic1/llama/branches{/branch}",
                "tags_url": "https://api.github.com/repos/xuetuyic1/llama/tags",
                "blobs_url": "https://api.github.com/repos/xuetuyic1/llama/git/blobs{/sha}",
                "git_tags_url": "https://api.github.com/repos/xuetuyic1/llama/git/tags{/sha}",
                "git_refs_url": "https://api.github.com/repos/xuetuyic1/llama/git/refs{/sha}",
                "trees_url": "https://api.github.com/repos/xuetuyic1/llama/git/trees{/sha}",
                "statuses_url": "https://api.github.com/repos/xuetuyic1/llama/statuses/{sha}",
                "languages_url": "https://api.github.com/repos/xuetuyic1/llama/languages",
                "stargazers_url": "https://api.github.com/repos/xuetuyic1/llama/stargazers",
                "contributors_url": "https://api.github.com/repos/xuetuyic1/llama/contributors",
                "subscribers_url": "https://api.github.com/repos/xuetuyic1/llama/subscribers",
                "subscription_url": "https://api.github.com/repos/xuetuyic1/llama/subscription",
                "commits_url": "https://api.github.com/repos/xuetuyic1/llama/commits{/sha}",
                "git_commits_url": "https://api.github.com/repos/xuetuyic1/llama/git/commits{/sha}",
                "comments_url": "https://api.github.com/repos/xuetuyic1/llama/comments{/number}",
                "issue_comment_url": "https://api.github.com/repos/xuetuyic1/llama/issues/comments{/number}",
                "contents_url": "https://api.github.com/repos/xuetuyic1/llama/contents/{+path}",
                "compare_url": "https://api.github.com/repos/xuetuyic1/llama/compare/{base}...{head}",
                "merges_url": "https://api.github.com/repos/xuetuyic1/llama/merges",
                "archive_url": "https://api.github.com/repos/xuetuyic1/llama/{archive_format}{/ref}",
                "downloads_url": "https://api.github.com/repos/xuetuyic1/llama/downloads",
                "issues_url": "https://api.github.com/repos/xuetuyic1/llama/issues{/number}",
                "pulls_url": "https://api.github.com/repos/xuetuyic1/llama/pulls{/number}",
                "milestones_url": "https://api.github.com/repos/xuetuyic1/llama/milestones{/number}",
                "notifications_url": "https://api.github.com/repos/xuetuyic1/llama/notifications{?since,all,participating}",
                "labels_url": "https://api.github.com/repos/xuetuyic1/llama/labels{/name}",
                "releases_url": "https://api.github.com/repos/xuetuyic1/llama/releases{/id}",
                "deployments_url": "https://api.github.com/repos/xuetuyic1/llama/deployments",
                "created_at": "2023-10-27T09:57:47Z",
                "updated_at": "2023-10-27T09:57:47Z",
                "pushed_at": "2023-10-26T21:46:39Z",
                "git_url": "git://github.com/xuetuyic1/llama.git",
                "ssh_url": "git@github.com:xuetuyic1/llama.git",
                "clone_url": "https://github.com/xuetuyic1/llama.git",
                "svn_url": "https://github.com/xuetuyic1/llama",
                "homepage": null,
                "size": 12522,
                "stargazers_count": 0,
                "watchers_count": 0,
                "language": null,
                "has_issues": false,
                "has_projects": true,
                "has_downloads": true,
                "has_wiki": true,
                "has_pages": false,
                "has_discussions": false,
                "forks_count": 0,
                "mirror_url": null,
                "archived": false,
                "disabled": false,
                "open_issues_count": 0,
                "license": null,
                "allow_forking": true,
                "is_template": false,
                "web_commit_signoff_required": false,
                "topics": [],
                "visibility": "public",
                "forks": 0,
                "open_issues": 0,
                "watchers": 0,
                "default_branch": "main",
                "public": true
            }
        },
        "public": true,
        "created_at": "2023-10-27T09:57:48Z"
    },
    {
        "id": "32894848445",
        "type": "WatchEvent",
        "actor": {
            "id": 13460680,
            "login": "xuetuyic1",
            "display_login": "xuetuyic1",
            "gravatar_id": "",
            "url": "https://api.github.com/users/xuetuyic1",
            "avatar_url": "https://avatars.githubusercontent.com/u/13460680?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-27T09:57:37Z"
    },
    {
        "id": "32894799373",
        "type": "WatchEvent",
        "actor": {
            "id": 115035186,
            "login": "Caycoding",
            "display_login": "Caycoding",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Caycoding",
            "avatar_url": "https://avatars.githubusercontent.com/u/115035186?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-27T09:55:44Z"
    },
    {
        "id": "32894103323",
        "type": "IssueCommentEvent",
        "actor": {
            "id": 59751859,
            "login": "Dampfinchen",
            "display_login": "Dampfinchen",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Dampfinchen",
            "avatar_url": "https://avatars.githubusercontent.com/u/59751859?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "action": "created",
            "issue": {
                "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3776",
                "repository_url": "https://api.github.com/repos/ggerganov/llama.cpp",
                "labels_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3776/labels{/name}",
                "comments_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3776/comments",
                "events_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3776/events",
                "html_url": "https://github.com/ggerganov/llama.cpp/pull/3776",
                "id": 1961304396,
                "node_id": "PR_kwDOJH_K4M5dv5bW",
                "number": 3776,
                "title": "cuda : improve text-generation and batched decoding performance",
                "user": {
                    "login": "ggerganov",
                    "id": 1991296,
                    "node_id": "MDQ6VXNlcjE5OTEyOTY=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/1991296?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/ggerganov",
                    "html_url": "https://github.com/ggerganov",
                    "followers_url": "https://api.github.com/users/ggerganov/followers",
                    "following_url": "https://api.github.com/users/ggerganov/following{/other_user}",
                    "gists_url": "https://api.github.com/users/ggerganov/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/ggerganov/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/ggerganov/subscriptions",
                    "organizations_url": "https://api.github.com/users/ggerganov/orgs",
                    "repos_url": "https://api.github.com/users/ggerganov/repos",
                    "events_url": "https://api.github.com/users/ggerganov/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/ggerganov/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "labels": [],
                "state": "open",
                "locked": false,
                "assignee": null,
                "assignees": [],
                "milestone": null,
                "comments": 18,
                "created_at": "2023-10-25T12:26:34Z",
                "updated_at": "2023-10-27T09:29:25Z",
                "closed_at": null,
                "author_association": "OWNER",
                "active_lock_reason": null,
                "draft": false,
                "pull_request": {
                    "url": "https://api.github.com/repos/ggerganov/llama.cpp/pulls/3776",
                    "html_url": "https://github.com/ggerganov/llama.cpp/pull/3776",
                    "diff_url": "https://github.com/ggerganov/llama.cpp/pull/3776.diff",
                    "patch_url": "https://github.com/ggerganov/llama.cpp/pull/3776.patch",
                    "merged_at": null
                },
                "body": "ref #3479 \r\nref #3771 \r\n\r\n# Description\r\n\r\nThis PR should improve significantly the text-generation, prompt processing and batched decoding speed for all models for NVIDIA cards with tensor cores (i.e. VOLTA, AMPERE, etc).\r\n\r\n## Prompt processing\r\n\r\nBy default `llama.cpp` uses `MMQ=1` which means that the matrix-matrix multiplications for quantized models are performed with custom kernel for integer multiplications. Recently (#3412), we found out that for large batch dimension (which is the case when processing prompts), `MMQ=0` offers significant performance boost by first dequantizing `src0` to F16 and performing the GEMM using cublas. This PR essentially enables the same optimization for `MMQ=1` by not using the custom kernel for batch size > 32.\r\n\r\n## Batched decoding\r\n\r\nIn this mode, the batch size is larger than 1, but typically small (for example not more than 32). In #3545 we found out that the currently used constants `MMQ_X`, `MMQ_Y` and `NWARPS` are not optimal for small batch sizes. Probably they have been optimized for prompt processing. However, since we now fallback to cuBLAS for prompt processing, the constants can be adjusted for small batch sizes.\r\n\r\n## Text-generation\r\n\r\nSo far, for the KV cache related ops (`KQ` and `KQV`) we have been using custom matrix-vector kernels. For small sequence lengths (~128) and no prompt, these kernels are quite efficient. However, as the KV cache grows with the sequence length it is more efficient to use the tensor cores via cuBLAS GEMM. This PR applies this change to achieve TG improvements for all models when the context is big\r\n\r\nIn summary, we now have the following strategy for matrix multiplications:\r\n\r\n- batch size == 1:\r\n  - non-attention ops && quantized `src0`: use custom matrix-vector kernel\r\n  - otherwise: use cuBLAS GEMM\r\n- batch size <= 32: use custom matrix-matrix kernel\r\n- batch size > 32: use cuBLAS GEMM\r\n\r\n# Results\r\n\r\n## RTX 3090\r\n\r\n```bash\r\nLLAMA_CUBLAS=1 make -j batched batched-bench && ./batched-bench ./models/codellama-7b/ggml-model-q4_0.gguf 8704 1 99 1 512 128 1,2,3,4,5,6,7,8,16,32,64\r\n```\r\n\r\n### master\r\n\r\nmain: n_kv_max = 8704, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.248 |  2063.97 |    1.098 |   116.60 |    1.346 |   475.54 |\r\n|   512 |    128 |    2 |    768 |    0.234 |  2191.11 |    7.435 |    34.43 |    7.669 |   100.15 |\r\n|   512 |    128 |    3 |    896 |    0.233 |  2198.40 |    7.515 |    51.09 |    7.748 |   115.64 |\r\n|   512 |    128 |    4 |   1024 |    0.234 |  2189.40 |    7.553 |    67.78 |    7.787 |   131.50 |\r\n|   512 |    128 |    5 |   1152 |    0.236 |  2168.46 |    7.635 |    83.83 |    7.871 |   146.37 |\r\n|   512 |    128 |    6 |   1280 |    0.236 |  2167.85 |    7.700 |    99.74 |    7.936 |   161.28 |\r\n|   512 |    128 |    7 |   1408 |    0.238 |  2152.19 |    7.768 |   115.34 |    8.006 |   175.86 |\r\n|   512 |    128 |    8 |   1536 |    0.242 |  2115.82 |    7.832 |   130.75 |    8.074 |   190.25 |\r\n|   512 |    128 |   16 |   2560 |    0.244 |  2095.92 |    8.391 |   244.06 |    8.636 |   296.45 |\r\n|   512 |    128 |   32 |   4608 |    0.264 |  1937.69 |    8.852 |   462.71 |    9.116 |   505.46 |\r\n|   512 |    128 |   64 |   8704 |    0.245 |  2085.75 |   11.646 |   703.42 |   11.891 |   731.95 |\r\n\r\n### PR\r\n\r\nmain: n_kv_max = 8704, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.173 |  2953.07 |    1.073 |   119.32 |    1.246 |   513.57 |\r\n|   512 |    128 |    2 |    768 |    0.165 |  3109.78 |    1.715 |   149.23 |    1.880 |   408.49 |\r\n|   512 |    128 |    3 |    896 |    0.162 |  3155.39 |    1.768 |   217.16 |    1.931 |   464.11 |\r\n|   512 |    128 |    4 |   1024 |    0.164 |  3123.65 |    1.803 |   283.98 |    1.967 |   520.63 |\r\n|   512 |    128 |    5 |   1152 |    0.163 |  3138.81 |    2.403 |   266.36 |    2.566 |   448.96 |\r\n|   512 |    128 |    6 |   1280 |    0.166 |  3077.79 |    2.433 |   315.70 |    2.599 |   492.49 |\r\n|   512 |    128 |    7 |   1408 |    0.166 |  3082.24 |    2.515 |   356.21 |    2.681 |   525.08 |\r\n|   512 |    128 |    8 |   1536 |    0.166 |  3080.13 |    2.551 |   401.48 |    2.717 |   565.37 |\r\n|   512 |    128 |   16 |   2560 |    0.169 |  3028.26 |    4.723 |   433.60 |    4.892 |   523.27 |\r\n|   512 |    128 |   32 |   4608 |    0.167 |  3059.51 |    7.973 |   513.75 |    8.140 |   566.09 |\r\n|   512 |    128 |   64 |   8704 |    0.164 |  3126.79 |   11.462 |   714.74 |   11.625 |   748.71 |\r\n\r\n```bash\r\nLLAMA_CUBLAS=1 make -j batched-bench && ./batched-bench ./models/codellama-13b/ggml-model-q4_k.gguf 4096 1 99 1 512,3200 128,800 1\r\n```\r\n\r\n### master\r\n\r\nmain: n_kv_max = 4096, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.437 |  1171.55 |    2.124 |    60.28 |    2.561 |   249.95 |\r\n|   512 |    800 |    1 |   1312 |    0.435 |  1176.46 |   14.269 |    56.07 |   14.704 |    89.23 |\r\n|  3200 |    128 |    1 |   3328 |    3.233 |   989.79 |    3.422 |    37.40 |    6.655 |   500.07 |\r\n|  3200 |    800 |    1 |   4000 |    3.219 |   994.22 |   22.392 |    35.73 |   25.611 |   156.19 |\r\n\r\n### PR\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.246 |  2084.35 |    2.021 |    63.32 |    2.267 |   282.30 |\r\n|   512 |    800 |    1 |   1312 |    0.247 |  2074.85 |   13.436 |    59.54 |   13.683 |    95.88 |\r\n|  3200 |    128 |    1 |   3328 |    1.977 |  1618.55 |    2.501 |    51.18 |    4.478 |   743.16 |\r\n|  3200 |    800 |    1 |   4000 |    1.984 |  1613.30 |   16.140 |    49.57 |   18.123 |   220.71 |\r\n\r\n```bash\r\nmake -j && ../scripts/run-all-perf.sh codellama-7b \"f16 q8_0 q4_0 q4_k\" \"-ngl 999 -t 1 -n 128,512 -p 512\"\r\n```\r\n\r\n### master\r\n\r\n| model                   |       size |     params | backend    | ngl |    threads | test       |              t/s |\r\n| ----------------------- | ---------: | ---------: | ---------- | --: | ---------: | ---------- | ---------------: |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  4493.64 \u00b1 25.65 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     54.67 \u00b1 0.04 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     52.65 \u00b1 0.24 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  2038.44 \u00b1 73.46 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     87.10 \u00b1 0.09 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     82.33 \u00b1 0.25 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     | 1901.29 \u00b1 154.12 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    133.48 \u00b1 0.17 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    123.13 \u00b1 0.38 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  1998.98 \u00b1 52.16 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    110.47 \u00b1 0.18 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    101.51 \u00b1 0.29 |\r\n\r\n### PR\r\n\r\n| model                   |       size |     params | backend    | ngl |    threads | test       |              t/s |\r\n| ----------------------- | ---------: | ---------: | ---------- | --: | ---------: | ---------- | ---------------: |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  4513.66 \u00b1 32.46 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     54.14 \u00b1 0.13 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     53.00 \u00b1 0.26 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  3183.63 \u00b1 16.25 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     85.08 \u00b1 0.15 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     82.86 \u00b1 0.14 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  3250.48 \u00b1 22.76 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    130.02 \u00b1 0.09 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    126.12 \u00b1 0.29 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  3661.56 \u00b1 53.75 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    109.18 \u00b1 0.19 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    104.73 \u00b1 0.08 |\r\n\r\n## RTX 4090\r\n\r\n```bash\r\nLLAMA_CUBLAS=1 make -j batched batched-bench && ./batched-bench ./models/codellama-7b/ggml-model-q4_0.gguf 8704 1 99 1 512 128 1,2,3,4,5,6,7,8,16,32,64\r\n```\r\n\r\n### master\r\n\r\nmain: n_kv_max = 8704, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.142 |  3615.61 |    0.915 |   139.85 |    1.057 |   605.55 |\r\n|   512 |    128 |    2 |    768 |    0.107 |  4779.37 |    5.170 |    49.52 |    5.277 |   145.55 |\r\n|   512 |    128 |    3 |    896 |    0.126 |  4050.31 |    5.201 |    73.83 |    5.328 |   168.17 |\r\n|   512 |    128 |    4 |   1024 |    0.120 |  4268.98 |    5.210 |    98.27 |    5.330 |   192.12 |\r\n|   512 |    128 |    5 |   1152 |    0.128 |  3988.04 |    5.240 |   122.15 |    5.368 |   214.60 |\r\n|   512 |    128 |    6 |   1280 |    0.126 |  4062.23 |    5.242 |   146.51 |    5.368 |   238.45 |\r\n|   512 |    128 |    7 |   1408 |    0.138 |  3721.15 |    5.336 |   167.91 |    5.474 |   257.22 |\r\n|   512 |    128 |    8 |   1536 |    0.114 |  4493.71 |    5.302 |   193.12 |    5.416 |   283.59 |\r\n|   512 |    128 |   16 |   2560 |    0.115 |  4435.90 |    5.558 |   368.49 |    5.673 |   451.24 |\r\n|   512 |    128 |   32 |   4608 |    0.118 |  4325.20 |    5.853 |   699.78 |    5.972 |   771.65 |\r\n|   512 |    128 |   64 |   8704 |    0.120 |  4251.54 |    7.116 |  1151.21 |    7.236 |  1202.81 |\r\n\r\n### PR\r\n\r\nmain: n_kv_max = 8704, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.109 |  4697.51 |    0.943 |   135.73 |    1.052 |   608.36 |\r\n|   512 |    128 |    2 |    768 |    0.089 |  5738.82 |    1.389 |   184.31 |    1.478 |   519.57 |\r\n|   512 |    128 |    3 |    896 |    0.099 |  5188.49 |    1.410 |   272.26 |    1.509 |   593.72 |\r\n|   512 |    128 |    4 |   1024 |    0.091 |  5633.43 |    1.438 |   355.94 |    1.529 |   669.57 |\r\n|   512 |    128 |    5 |   1152 |    0.093 |  5476.76 |    1.508 |   424.27 |    1.602 |   719.12 |\r\n|   512 |    128 |    6 |   1280 |    0.086 |  5968.90 |    1.520 |   505.42 |    1.605 |   797.35 |\r\n|   512 |    128 |    7 |   1408 |    0.092 |  5567.34 |    1.546 |   579.54 |    1.638 |   859.57 |\r\n|   512 |    128 |    8 |   1536 |    0.091 |  5653.09 |    1.574 |   650.69 |    1.664 |   922.91 |\r\n|   512 |    128 |   16 |   2560 |    0.084 |  6129.17 |    2.196 |   932.58 |    2.280 |  1123.01 |\r\n|   512 |    128 |   32 |   4608 |    0.099 |  5172.66 |    3.436 |  1192.04 |    3.535 |  1303.50 |\r\n|   512 |    128 |   64 |   8704 |    0.097 |  5279.28 |    6.336 |  1293.00 |    6.433 |  1353.10 |\r\n\r\n\r\n```bash\r\nLLAMA_CUBLAS=1 make -j batched-bench && ./batched-bench ./models/codellama-13b/ggml-model-q4_k.gguf 4096 1 99 1 512,3200 128,800 1\r\n```\r\n\r\n### master\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.226 |  2269.96 |    1.607 |    79.66 |    1.832 |   349.29 |\r\n|   512 |    800 |    1 |   1312 |    0.214 |  2387.60 |   10.557 |    75.78 |   10.771 |   121.80 |\r\n|  3200 |    128 |    1 |   3328 |    1.640 |  1950.80 |    2.294 |    55.80 |    3.934 |   845.87 |\r\n|  3200 |    800 |    1 |   4000 |    1.626 |  1968.49 |   14.876 |    53.78 |   16.501 |   242.41 |\r\n\r\n### PR\r\n\r\nmain: n_kv_max = 4096, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.152 |  3363.09 |    1.612 |    79.40 |    1.764 |   362.73 |\r\n|   512 |    800 |    1 |   1312 |    0.145 |  3522.46 |   10.201 |    78.42 |   10.347 |   126.80 |\r\n|  3200 |    128 |    1 |   3328 |    1.269 |  2521.20 |    1.986 |    64.45 |    3.255 |  1022.30 |\r\n|  3200 |    800 |    1 |   4000 |    1.268 |  2523.72 |   12.692 |    63.03 |   13.960 |   286.53 |\r\n\r\n```bash\r\nmake -j && ../scripts/run-all-perf.sh codellama-7b \"f16 q8_0 q4_0 q4_k\" \"-ngl 999 -t 1 -n 128,512 -p 512\"\r\n```\r\n\r\n### master\r\n\r\n| model                   |       size |     params | backend    | ngl |    threads | test       |              t/s |\r\n| ----------------------- | ---------: | ---------: | ---------- | --: | ---------: | ---------- | ---------------: |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     | 8972.06 \u00b1 345.72 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     62.27 \u00b1 0.19 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     61.54 \u00b1 0.04 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  5272.91 \u00b1 99.01 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    101.73 \u00b1 0.15 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     99.43 \u00b1 0.05 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     | 5017.87 \u00b1 132.87 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    158.38 \u00b1 0.13 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    152.41 \u00b1 0.85 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     | 4763.11 \u00b1 163.93 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    147.54 \u00b1 0.30 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    142.81 \u00b1 0.15 |\r\n\r\n### PR\r\n\r\n| model                   |       size |     params | backend    | ngl |    threads | test       |              t/s |\r\n| ----------------------- | ---------: | ---------: | ---------- | --: | ---------: | ---------- | ---------------: |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     | 9473.31 \u00b1 227.98 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     61.76 \u00b1 0.29 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     61.27 \u00b1 0.01 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   6473.99 \u00b1 4.55 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    100.22 \u00b1 0.14 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     98.65 \u00b1 0.07 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   6693.25 \u00b1 6.18 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    154.33 \u00b1 0.36 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    150.87 \u00b1 0.12 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   7277.83 \u00b1 4.73 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    144.40 \u00b1 0.25 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    141.19 \u00b1 0.07 |\r\n\r\n\r\n## V100\r\n\r\n```bash\r\nLLAMA_CUBLAS=1 make -j batched-bench && ./batched-bench ./models/openllama-7b-v2/ggml-model-q4_k.gguf 4096 1 99 1 512,3200 128,800 1\r\n```\r\n\r\n### master\r\n\r\nmain: n_kv_max = 4096, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.353 |  1451.95 |    1.388 |    92.23 |    1.740 |   367.73 |\r\n|   512 |    800 |    1 |   1312 |    0.351 |  1457.49 |    9.431 |    84.83 |    9.782 |   134.12 |\r\n|  3200 |    128 |    1 |   3328 |    2.648 |  1208.35 |    2.329 |    54.97 |    4.977 |   668.71 |\r\n|  3200 |    800 |    1 |   4000 |    2.653 |  1206.38 |   15.285 |    52.34 |   17.937 |   223.00 |\r\n\r\n### PR\r\n\r\nmain: n_kv_max = 4096, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.211 |  2421.42 |    1.376 |    92.99 |    1.588 |   403.05 |\r\n|   512 |    800 |    1 |   1312 |    0.212 |  2419.59 |    9.145 |    87.48 |    9.357 |   140.22 |\r\n|  3200 |    128 |    1 |   3328 |    1.767 |  1811.34 |    2.026 |    63.17 |    3.793 |   877.43 |\r\n|  3200 |    800 |    1 |   4000 |    1.765 |  1812.75 |   13.127 |    60.94 |   14.892 |   268.60 |\r\n\r\n## A100 80GB\r\n\r\n```bash\r\nLLAMA_CUBLAS=1 make -j batched batched-bench && ./batched-bench ./models/codellama-7b/ggml-model-q4_0.gguf 8704 1 99 1 512 128 1,2,3,4,5,6,7,8,16,32,64\r\n```\r\n\r\n### master\r\n\r\nmain: n_kv_max = 8704, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.234 |  2185.63 |    1.031 |   124.18 |    1.265 |   505.93 |\r\n|   512 |    128 |    2 |    768 |    0.217 |  2363.62 |   10.829 |    23.64 |   11.045 |    69.53 |\r\n|   512 |    128 |    3 |    896 |    0.217 |  2360.95 |   10.870 |    35.33 |   11.087 |    80.81 |\r\n|   512 |    128 |    4 |   1024 |    0.217 |  2361.65 |   10.904 |    46.95 |   11.121 |    92.08 |\r\n|   512 |    128 |    5 |   1152 |    0.217 |  2359.50 |   10.967 |    58.36 |   11.184 |   103.01 |\r\n|   512 |    128 |    6 |   1280 |    0.217 |  2360.19 |   10.993 |    69.86 |   11.210 |   114.19 |\r\n|   512 |    128 |    7 |   1408 |    0.217 |  2360.06 |   11.044 |    81.13 |   11.261 |   125.04 |\r\n|   512 |    128 |    8 |   1536 |    0.217 |  2360.97 |   11.081 |    92.41 |   11.297 |   135.96 |\r\n|   512 |    128 |   16 |   2560 |    0.217 |  2355.54 |   11.536 |   177.53 |   11.754 |   217.81 |\r\n|   512 |    128 |   32 |   4608 |    0.218 |  2346.86 |   11.580 |   353.72 |   11.798 |   390.58 |\r\n|   512 |    128 |   64 |   8704 |    0.220 |  2331.26 |   13.576 |   603.41 |   13.796 |   630.92 |\r\n\r\n### PR\r\n\r\nmain: n_kv_max = 8704, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.115 |  4469.78 |    1.006 |   127.18 |    1.121 |   570.93 |\r\n|   512 |    128 |    2 |    768 |    0.097 |  5258.29 |    1.853 |   138.13 |    1.951 |   393.72 |\r\n|   512 |    128 |    3 |    896 |    0.097 |  5298.23 |    1.893 |   202.84 |    1.990 |   450.31 |\r\n|   512 |    128 |    4 |   1024 |    0.098 |  5247.73 |    1.931 |   265.14 |    2.029 |   504.78 |\r\n|   512 |    128 |    5 |   1152 |    0.097 |  5277.15 |    2.147 |   298.05 |    2.244 |   513.31 |\r\n|   512 |    128 |    6 |   1280 |    0.097 |  5276.45 |    2.176 |   352.95 |    2.273 |   563.13 |\r\n|   512 |    128 |    7 |   1408 |    0.097 |  5289.42 |    2.228 |   402.20 |    2.325 |   605.71 |\r\n|   512 |    128 |    8 |   1536 |    0.097 |  5282.33 |    2.274 |   450.38 |    2.371 |   647.94 |\r\n|   512 |    128 |   16 |   2560 |    0.097 |  5259.21 |    3.386 |   604.89 |    3.483 |   734.98 |\r\n|   512 |    128 |   32 |   4608 |    0.098 |  5212.26 |    5.141 |   796.67 |    5.240 |   879.46 |\r\n|   512 |    128 |   64 |   8704 |    0.100 |  5132.47 |    8.464 |   967.89 |    8.564 |  1016.40 |\r\n\r\n```bash\r\nLLAMA_CUBLAS=1 make -j batched-bench && ./batched-bench ./models/codellama-13b/ggml-model-q4_k.gguf 4096 1 99 1 512,3200 128,800 1\r\n```\r\n\r\n### master\r\n\r\nmain: n_kv_max = 4096, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.462 |  1108.58 |    1.879 |    68.13 |    2.341 |   273.42 |\r\n|   512 |    800 |    1 |   1312 |    0.444 |  1153.87 |   12.526 |    63.87 |   12.970 |   101.16 |\r\n|  3200 |    128 |    1 |   3328 |    3.100 |  1032.42 |    2.914 |    43.92 |    6.014 |   553.38 |\r\n|  3200 |    800 |    1 |   4000 |    3.101 |  1032.07 |   19.025 |    42.05 |   22.126 |   180.78 |\r\n\r\n### PR\r\n\r\nmain: n_kv_max = 4096, is_pp_shared = 1, n_gpu_layers = 99, mmq = 1\r\n\r\n|    PP |     TG |    B |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |      T s |    S t/s |\r\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\r\n|   512 |    128 |    1 |    640 |    0.163 |  3150.34 |    1.842 |    69.50 |    2.004 |   319.31 |\r\n|   512 |    800 |    1 |   1312 |    0.145 |  3532.96 |   12.018 |    66.57 |   12.163 |   107.87 |\r\n|  3200 |    128 |    1 |   3328 |    1.239 |  2582.01 |    2.445 |    52.36 |    3.684 |   903.34 |\r\n|  3200 |    800 |    1 |   4000 |    1.241 |  2579.48 |   15.755 |    50.78 |   16.995 |   235.36 |\r\n\r\n```bash\r\nmake -j && ../scripts/run-all-perf.sh codellama-7b \"f16 q8_0 q4_0 q4_k\" \"-ngl 999 -t 1 -n 128,512 -p 512\"\r\n```\r\n\r\n| model                   |       size |     params | backend    | ngl |    threads | test       |              t/s |\r\n| ----------------------- | ---------: | ---------: | ---------- | --: | ---------: | ---------- | ---------------: |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   7883.90 \u00b1 3.58 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     77.14 \u00b1 0.05 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     74.74 \u00b1 0.03 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   2316.34 \u00b1 1.83 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     98.67 \u00b1 0.04 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     94.81 \u00b1 0.04 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   2389.03 \u00b1 1.81 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    146.72 \u00b1 0.07 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    138.33 \u00b1 0.14 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   2034.51 \u00b1 4.29 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    115.35 \u00b1 0.06 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    110.23 \u00b1 0.12 |\r\n\r\n### PR\r\n\r\n| model                   |       size |     params | backend    | ngl |    threads | test       |              t/s |\r\n| ----------------------- | ---------: | ---------: | ---------- | --: | ---------: | ---------- | ---------------: |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  7912.18 \u00b1 32.14 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     75.84 \u00b1 0.05 |\r\n| llama 7B F16            |  12.55 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     74.30 \u00b1 0.02 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  5356.38 \u00b1 15.04 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |     96.57 \u00b1 0.02 |\r\n| llama 7B Q8_0           |   6.67 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |     94.09 \u00b1 0.14 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |   5412.66 \u00b1 8.32 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    141.65 \u00b1 0.06 |\r\n| llama 7B Q4_0           |   3.56 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    136.63 \u00b1 0.62 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | pp 512     |  6044.39 \u00b1 24.43 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 128     |    112.27 \u00b1 0.14 |\r\n| llama 7B Q4_K - Medium  |   3.80 GiB |     6.74 B | CUDA       | 999 |          1 | tg 512     |    109.27 \u00b1 0.02 |\r\n\r\n\r\n\r\n# TODO\r\n\r\n- [x] Perform ppl tests to make sure I didn't break something\r\n- [x] Run tests on other cards\r\n- [x] Run tests on other models\r\n- [x] Try to add full MMQ fallback\r\n- [x] Fix `BACKEND_SPLIT` support for `src0`\r\n- [ ] Tune compile-time constants for other CUDA / AMD architectures",
                "reactions": {
                    "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3776/reactions",
                    "total_count": 3,
                    "+1": 2,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 1,
                    "eyes": 0
                },
                "timeline_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3776/timeline",
                "performed_via_github_app": null,
                "state_reason": null
            },
            "comment": {
                "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/comments/1782597350",
                "html_url": "https://github.com/ggerganov/llama.cpp/pull/3776#issuecomment-1782597350",
                "issue_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3776",
                "id": 1782597350,
                "node_id": "IC_kwDOJH_K4M5qQEbm",
                "user": {
                    "login": "Dampfinchen",
                    "id": 59751859,
                    "node_id": "MDQ6VXNlcjU5NzUxODU5",
                    "avatar_url": "https://avatars.githubusercontent.com/u/59751859?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/Dampfinchen",
                    "html_url": "https://github.com/Dampfinchen",
                    "followers_url": "https://api.github.com/users/Dampfinchen/followers",
                    "following_url": "https://api.github.com/users/Dampfinchen/following{/other_user}",
                    "gists_url": "https://api.github.com/users/Dampfinchen/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/Dampfinchen/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/Dampfinchen/subscriptions",
                    "organizations_url": "https://api.github.com/users/Dampfinchen/orgs",
                    "repos_url": "https://api.github.com/users/Dampfinchen/repos",
                    "events_url": "https://api.github.com/users/Dampfinchen/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/Dampfinchen/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "created_at": "2023-10-27T09:29:25Z",
                "updated_at": "2023-10-27T09:29:25Z",
                "author_association": "NONE",
                "body": "I was testing this PR and I didn't notice a different in terms of VRAM usage compared to MMQ in PR. While being indeed a lot faster.\r\n\r\nSo I guess this PR is safe to merge. Great job!",
                "reactions": {
                    "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/comments/1782597350/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "performed_via_github_app": null
            }
        },
        "public": true,
        "created_at": "2023-10-27T09:29:25Z"
    },
    {
        "id": "32894077710",
        "type": "WatchEvent",
        "actor": {
            "id": 3014516,
            "login": "netwood",
            "display_login": "netwood",
            "gravatar_id": "",
            "url": "https://api.github.com/users/netwood",
            "avatar_url": "https://avatars.githubusercontent.com/u/3014516?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-27T09:28:28Z"
    },
    {
        "id": "32893661711",
        "type": "WatchEvent",
        "actor": {
            "id": 4125158,
            "login": "gwolves",
            "display_login": "gwolves",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gwolves",
            "avatar_url": "https://avatars.githubusercontent.com/u/4125158?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-27T09:12:48Z"
    },
    {
        "id": "32893504670",
        "type": "IssueCommentEvent",
        "actor": {
            "id": 22598920,
            "login": "pkreissel",
            "display_login": "pkreissel",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pkreissel",
            "avatar_url": "https://avatars.githubusercontent.com/u/22598920?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "action": "created",
            "issue": {
                "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3799",
                "repository_url": "https://api.github.com/repos/ggerganov/llama.cpp",
                "labels_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3799/labels{/name}",
                "comments_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3799/comments",
                "events_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3799/events",
                "html_url": "https://github.com/ggerganov/llama.cpp/issues/3799",
                "id": 1963871231,
                "node_id": "I_kwDOJH_K4M51Dkv_",
                "number": 3799,
                "title": "Finetuning not using Metal/Apple Silicon GPU",
                "user": {
                    "login": "fakerybakery",
                    "id": 76186054,
                    "node_id": "MDQ6VXNlcjc2MTg2MDU0",
                    "avatar_url": "https://avatars.githubusercontent.com/u/76186054?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/fakerybakery",
                    "html_url": "https://github.com/fakerybakery",
                    "followers_url": "https://api.github.com/users/fakerybakery/followers",
                    "following_url": "https://api.github.com/users/fakerybakery/following{/other_user}",
                    "gists_url": "https://api.github.com/users/fakerybakery/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/fakerybakery/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/fakerybakery/subscriptions",
                    "organizations_url": "https://api.github.com/users/fakerybakery/orgs",
                    "repos_url": "https://api.github.com/users/fakerybakery/repos",
                    "events_url": "https://api.github.com/users/fakerybakery/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/fakerybakery/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "labels": [
                    {
                        "id": 5250901848,
                        "node_id": "LA_kwDOJH_K4M8AAAABOPpnWA",
                        "url": "https://api.github.com/repos/ggerganov/llama.cpp/labels/bug",
                        "name": "bug",
                        "color": "d73a4a",
                        "default": true,
                        "description": "Something isn't working"
                    }
                ],
                "state": "open",
                "locked": false,
                "assignee": null,
                "assignees": [],
                "milestone": null,
                "comments": 4,
                "created_at": "2023-10-26T15:53:19Z",
                "updated_at": "2023-10-27T09:06:55Z",
                "closed_at": null,
                "author_association": "NONE",
                "active_lock_reason": null,
                "body": "Hi,\r\nI'm running the latest version of llama.cpp (cloned yesterday from the Git repo) on macOS Sonoma 14.0 on a M1 MacBook Pro.\r\nI tried to finetune a Llama model, and the training worked, however it was extremely slow and Activity Monitor did not indicate any GPU usage for the finetuning script, although it was using most of my CPU.\r\nHere is my script:\r\n```\r\n$ ./finetune --model-base models/tinyllama/tinyllama-1.1b-intermediate-step-480k-1t.Q4_0.gguf --train-data shakespeare.txt --lora-out shakespeare.gguf --save-every 0 --threads 18 --ctx 32 --batch 32 --use-checkpointing --use-flash --sample-start \"\\n\" --escape\r\n```\r\nHere is the console output:\r\n```\r\nmain: seed: 1698285516\r\nmain: model base = 'models/tinyllama/tinyllama-1.1b-intermediate-step-480k-1t.Q4_0.gguf'\r\nllama_model_loader: loaded meta data with 19 key-value pairs and 201 tensors from models/tinyllama/tinyllama-1.1b-intermediate-step-480k-1t.Q4_0.gguf (version GGUF V2 (latest))\r\nllama_model_loader: - tensor    0:                    output.weight q6_K     [  2048, 32000,     1,     1 ]\r\nllama_model_loader: - tensor    1:                token_embd.weight q4_0     [  2048, 32000,     1,     1 ]\r\nllama_model_loader: - tensor    2:           blk.0.attn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor    7:            blk.0.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor    8:            blk.0.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor   10:            blk.0.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   11:           blk.1.attn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   16:            blk.1.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor   17:            blk.1.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor   19:            blk.1.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   20:           blk.2.attn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   25:            blk.2.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor   26:            blk.2.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor   28:            blk.2.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   29:           blk.3.attn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   34:            blk.3.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor   35:            blk.3.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor   37:            blk.3.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   38:           blk.4.attn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   43:            blk.4.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor   44:            blk.4.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor   46:            blk.4.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   47:           blk.5.attn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   52:            blk.5.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor   53:            blk.5.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor   55:            blk.5.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   56:           blk.6.attn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   61:            blk.6.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor   62:            blk.6.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor   64:            blk.6.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   65:           blk.7.attn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   70:            blk.7.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor   71:            blk.7.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor   73:            blk.7.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   74:           blk.8.attn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   79:            blk.8.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor   80:            blk.8.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor   82:            blk.8.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   83:           blk.9.attn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   88:            blk.9.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor   89:            blk.9.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor   91:            blk.9.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   92:          blk.10.attn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor   97:           blk.10.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor   98:           blk.10.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor  100:           blk.10.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  101:          blk.11.attn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  106:           blk.11.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor  107:           blk.11.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor  109:           blk.11.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  110:          blk.12.attn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  115:           blk.12.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor  116:           blk.12.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor  118:           blk.12.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  119:          blk.13.attn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  124:           blk.13.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor  125:           blk.13.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor  127:           blk.13.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  128:          blk.14.attn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  133:           blk.14.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor  134:           blk.14.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor  136:           blk.14.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  137:          blk.15.attn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  142:           blk.15.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor  143:           blk.15.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor  145:           blk.15.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  146:          blk.16.attn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  151:           blk.16.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor  152:           blk.16.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor  154:           blk.16.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  155:          blk.17.attn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  160:           blk.17.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor  161:           blk.17.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor  163:           blk.17.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  164:          blk.18.attn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  169:           blk.18.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor  170:           blk.18.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor  172:           blk.18.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  173:          blk.19.attn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  178:           blk.19.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor  179:           blk.19.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor  181:           blk.19.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  182:          blk.20.attn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  187:           blk.20.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor  188:           blk.20.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor  190:           blk.20.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  191:          blk.21.attn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  2048,   256,     1,     1 ]\r\nllama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  196:           blk.21.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - tensor  197:           blk.21.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]\r\nllama_model_loader: - tensor  199:           blk.21.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]\r\nllama_model_loader: - tensor  200:               output_norm.weight f32      [  2048,     1,     1,     1 ]\r\nllama_model_loader: - kv   0:                       general.architecture str\r\nllama_model_loader: - kv   1:                               general.name str\r\nllama_model_loader: - kv   2:                       llama.context_length u32\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32\r\nllama_model_loader: - kv   4:                          llama.block_count u32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32\r\nllama_model_loader: - kv  10:                          general.file_type u32\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str\r\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr\r\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32\r\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32\r\nllama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32\r\nllama_model_loader: - kv  18:               general.quantization_version u32\r\nllama_model_loader: - type  f32:   45 tensors\r\nllama_model_loader: - type q4_0:  155 tensors\r\nllama_model_loader: - type q6_K:    1 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V2 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 2048\r\nllm_load_print_meta: n_embd           = 2048\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 4\r\nllm_load_print_meta: n_layer          = 22\r\nllm_load_print_meta: n_rot            = 64\r\nllm_load_print_meta: n_gqa            = 8\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: n_ff             = 5632\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: model type       = ?B\r\nllm_load_print_meta: model ftype      = mostly Q4_0\r\nllm_load_print_meta: model params     = 1.10 B\r\nllm_load_print_meta: model size       = 606.53 MiB (4.63 BPW)\r\nllm_load_print_meta: general.name   = py007_tinyllama-1.1b-intermediate-step-480k-1t\r\nllm_load_print_meta: BOS token = 1 '<s>'\r\nllm_load_print_meta: EOS token = 2 '</s>'\r\nllm_load_print_meta: UNK token = 0 '<unk>'\r\nllm_load_print_meta: LF token  = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.07 MB\r\nllm_load_tensors: mem required  =  606.59 MB\r\n......................................................................................\r\nllama_new_context_with_model: n_ctx      = 512\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_new_context_with_model: kv self size  =   11.00 MB\r\nggml_metal_init: allocating\r\nggml_metal_init: found device: Apple M1 Max\r\nggml_metal_init: picking default device: Apple M1 Max\r\nggml_metal_init: default.metallib not found, loading from source\r\nggml_metal_init: loading '<REDACTED>llama.cpp/ggml-metal.metal'\r\nggml_metal_init: loaded kernel_add                            0x13fe07f20 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_add_row                        0x13fe088b0 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_mul                            0x13fe090d0 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_row                        0x13fe09980 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_scale                          0x13fe0a1c0 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_scale_4                        0x13fe0aa10 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_silu                           0x13fe0b1c0 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_relu                           0x13fe0b970 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_gelu                           0x13fe0c120 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_soft_max                       0x13fe0c650 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_soft_max_4                     0x13fe0cb80 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_diag_mask_inf                  0x13fe0d220 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_diag_mask_inf_8                0x13fe0d7b0 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_get_rows_f32                   0x13fe0dce0 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_get_rows_f16                   0x13fe0e210 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_get_rows_q4_0                  0x141106640 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_get_rows_q4_1                  0x141106b70 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_get_rows_q5_0                  0x1411070a0 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_get_rows_q5_1                  0x1411075d0 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_get_rows_q8_0                  0x141107d90 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_get_rows_q2_K                  0x1411082c0 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_get_rows_q3_K                  0x1411087f0 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_get_rows_q4_K                  0x13fe0e620 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_get_rows_q5_K                  0x13fe0ecd0 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_get_rows_q6_K                  0x13fe0f200 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_rms_norm                       0x13fe0f730 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_norm                           0x13fe0fc60 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mv_f32_f32                 0x13fe10390 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mv_f16_f32                 0x13fe108c0 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mv_f16_f32_1row            0x13fe10e50 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mv_f16_f32_l4              0x13fe115f0 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mv_q4_0_f32                0x13fe11b20 | th_max =  896 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mv_q4_1_f32                0x13fe12050 | th_max =  896 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mv_q5_0_f32                0x13fe12580 | th_max =  576 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mv_q5_1_f32                0x141204e40 | th_max =  576 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mv_q8_0_f32                0x141205850 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mv_q2_K_f32                0x141205d80 | th_max =  640 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mv_q3_K_f32                0x1412062b0 | th_max =  576 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mv_q4_K_f32                0x1412067e0 | th_max =  576 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mv_q5_K_f32                0x141206d10 | th_max =  576 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mv_q6_K_f32                0x141207240 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mm_f32_f32                 0x141207770 | th_max =  768 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x1411079e0 | th_max =  768 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x141108ea0 | th_max =  704 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x1411093d0 | th_max =  704 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mm_q5_0_f32                0x141109900 | th_max =  704 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mm_q5_1_f32                0x141109e30 | th_max =  704 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mm_q8_0_f32                0x14110a670 | th_max =  768 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x14110aba0 | th_max =  768 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x14110b0d0 | th_max =  768 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x14110b600 | th_max =  768 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x14110bb30 | th_max =  768 | th_width =   32\r\nggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x14110c060 | th_max =  768 | th_width =   32\r\nggml_metal_init: loaded kernel_rope_f32                       0x14110c590 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_rope_f16                       0x14110cac0 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_alibi_f32                      0x14110cff0 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_cpy_f32_f16                    0x14110d520 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_cpy_f32_f32                    0x14110da50 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_cpy_f16_f16                    0x14110df80 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_concat                         0x14110e4b0 | th_max = 1024 | th_width =   32\r\nggml_metal_init: loaded kernel_sqr                            0x14110ec60 | th_max = 1024 | th_width =   32\r\nggml_metal_init: GPU name:   Apple M1 Max\r\nggml_metal_init: GPU family: MTLGPUFamilyApple7 (1007)\r\nggml_metal_init: hasUnifiedMemory              = true\r\nggml_metal_init: recommendedMaxWorkingSetSize  = 49152.00 MB\r\nggml_metal_init: maxTransferRate               = built-in GPU\r\nllama_new_context_with_model: compute buffer total size = 72.63 MB\r\nllama_new_context_with_model: max tensor size =    51.27 MB\r\nggml_metal_add_buffer: allocated 'data            ' buffer, size =   607.23 MB, (  607.86 / 49152.00)\r\nggml_metal_add_buffer: allocated 'kv              ' buffer, size =    11.02 MB, (  618.88 / 49152.00)\r\nggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    66.52 MB, (  685.39 / 49152.00)\r\nmain: init model\r\nprint_params: n_vocab:   32000\r\nprint_params: n_ctx:     32\r\nprint_params: n_embd:    2048\r\nprint_params: n_ff:      5632\r\nprint_params: n_head:    32\r\nprint_params: n_head_kv: 4\r\nprint_params: n_layer:   22\r\nprint_params: norm_rms_eps          : 0.000010\r\nprint_params: rope_freq_base        : 10000.000000\r\nprint_params: rope_freq_scale       : 1.000000\r\nprint_lora_params: n_rank_attention_norm : 1\r\nprint_lora_params: n_rank_wq             : 4\r\nprint_lora_params: n_rank_wk             : 4\r\nprint_lora_params: n_rank_wv             : 4\r\nprint_lora_params: n_rank_wo             : 4\r\nprint_lora_params: n_rank_ffn_norm       : 1\r\nprint_lora_params: n_rank_w1             : 4\r\nprint_lora_params: n_rank_w2             : 4\r\nprint_lora_params: n_rank_w3             : 4\r\nprint_lora_params: n_rank_tok_embeddings : 4\r\nprint_lora_params: n_rank_norm           : 1\r\nprint_lora_params: n_rank_output         : 4\r\nmain: total train_iterations 0\r\nmain: seen train_samples     0\r\nmain: seen train_tokens      0\r\nmain: completed train_epochs 0\r\nmain: lora_size = 28433632 bytes (27.1 MB)\r\nmain: opt_size  = 42223216 bytes (40.3 MB)\r\nmain: opt iter 0\r\nmain: input_size = 131076128 bytes (125.0 MB)\r\nmain: compute_size = 4010811616 bytes (3825.0 MB)\r\nmain: evaluation order = LEFT_TO_RIGHT\r\nmain: tokenize training data\r\ntokenize_file: warning: found 2176 samples (min length 1) that are shorter than context length of 32.\r\ntokenize_file: warning: found 293 empty samples.\r\ntokenize_file: total number of samples: 2469\r\nmain: number of training tokens: 24798\r\nmain: number of unique tokens: 3131\r\nmain: train data seems to have changed. restarting shuffled epoch.\r\nmain: begin training\r\nmain: work_size = 2305192 bytes (2.2 MB)\r\ntrain_opt_callback: iter=     0 sample=1/2469 sched=0.000000 loss=0.000000 |->\r\ntrain_opt_callback: iter=     1 sample=33/2469 sched=0.010000 loss=14.714114 dt=00:01:04 eta=04:32:13 |->\r\n<...>\r\n```\r\nIs Metal supported on finetuning yet? If so, is there any configuration needed to get it to work?\r\nThanks in advance!",
                "reactions": {
                    "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3799/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "timeline_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3799/timeline",
                "performed_via_github_app": null,
                "state_reason": null
            },
            "comment": {
                "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/comments/1782565749",
                "html_url": "https://github.com/ggerganov/llama.cpp/issues/3799#issuecomment-1782565749",
                "issue_url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/3799",
                "id": 1782565749,
                "node_id": "IC_kwDOJH_K4M5qP8t1",
                "user": {
                    "login": "pkreissel",
                    "id": 22598920,
                    "node_id": "MDQ6VXNlcjIyNTk4OTIw",
                    "avatar_url": "https://avatars.githubusercontent.com/u/22598920?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/pkreissel",
                    "html_url": "https://github.com/pkreissel",
                    "followers_url": "https://api.github.com/users/pkreissel/followers",
                    "following_url": "https://api.github.com/users/pkreissel/following{/other_user}",
                    "gists_url": "https://api.github.com/users/pkreissel/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/pkreissel/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/pkreissel/subscriptions",
                    "organizations_url": "https://api.github.com/users/pkreissel/orgs",
                    "repos_url": "https://api.github.com/users/pkreissel/repos",
                    "events_url": "https://api.github.com/users/pkreissel/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/pkreissel/received_events",
                    "type": "User",
                    "site_admin": false
                },
                "created_at": "2023-10-27T09:06:55Z",
                "updated_at": "2023-10-27T09:06:55Z",
                "author_association": "NONE",
                "body": "I got this error, when trying to fine-tune on M-series Chip.\r\n```\r\n-[MTLComputePipelineDescriptorInternal setComputeFunction:withType:]:722: failed assertion `computeFunction must not be nil.'\r\nAbort trap: 6 \r\n```",
                "reactions": {
                    "url": "https://api.github.com/repos/ggerganov/llama.cpp/issues/comments/1782565749/reactions",
                    "total_count": 0,
                    "+1": 0,
                    "-1": 0,
                    "laugh": 0,
                    "hooray": 0,
                    "confused": 0,
                    "heart": 0,
                    "rocket": 0,
                    "eyes": 0
                },
                "performed_via_github_app": null
            }
        },
        "public": true,
        "created_at": "2023-10-27T09:06:55Z"
    },
    {
        "id": "32893465152",
        "type": "WatchEvent",
        "actor": {
            "id": 13796752,
            "login": "twdnhfr",
            "display_login": "twdnhfr",
            "gravatar_id": "",
            "url": "https://api.github.com/users/twdnhfr",
            "avatar_url": "https://avatars.githubusercontent.com/u/13796752?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-27T09:05:25Z"
    },
    {
        "id": "32893418038",
        "type": "WatchEvent",
        "actor": {
            "id": 3354725,
            "login": "eRaul",
            "display_login": "eRaul",
            "gravatar_id": "",
            "url": "https://api.github.com/users/eRaul",
            "avatar_url": "https://avatars.githubusercontent.com/u/3354725?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-27T09:03:38Z"
    },
    {
        "id": "32893278492",
        "type": "WatchEvent",
        "actor": {
            "id": 50049332,
            "login": "NullCarrier",
            "display_login": "NullCarrier",
            "gravatar_id": "",
            "url": "https://api.github.com/users/NullCarrier",
            "avatar_url": "https://avatars.githubusercontent.com/u/50049332?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-27T08:58:23Z"
    },
    {
        "id": "32893097680",
        "type": "WatchEvent",
        "actor": {
            "id": 14976489,
            "login": "zy445566",
            "display_login": "zy445566",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zy445566",
            "avatar_url": "https://avatars.githubusercontent.com/u/14976489?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-27T08:51:44Z"
    },
    {
        "id": "32892949241",
        "type": "WatchEvent",
        "actor": {
            "id": 124647982,
            "login": "nenryo",
            "display_login": "nenryo",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nenryo",
            "avatar_url": "https://avatars.githubusercontent.com/u/124647982?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-27T08:46:03Z"
    },
    {
        "id": "32892808194",
        "type": "WatchEvent",
        "actor": {
            "id": 87133661,
            "login": "danielsteinigen",
            "display_login": "danielsteinigen",
            "gravatar_id": "",
            "url": "https://api.github.com/users/danielsteinigen",
            "avatar_url": "https://avatars.githubusercontent.com/u/87133661?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-27T08:40:36Z"
    },
    {
        "id": "32892694755",
        "type": "WatchEvent",
        "actor": {
            "id": 147787533,
            "login": "yjf-ddr",
            "display_login": "yjf-ddr",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yjf-ddr",
            "avatar_url": "https://avatars.githubusercontent.com/u/147787533?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-27T08:36:12Z"
    },
    {
        "id": "32892000027",
        "type": "WatchEvent",
        "actor": {
            "id": 93240803,
            "login": "OsirisRaptor",
            "display_login": "OsirisRaptor",
            "gravatar_id": "",
            "url": "https://api.github.com/users/OsirisRaptor",
            "avatar_url": "https://avatars.githubusercontent.com/u/93240803?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-27T08:09:10Z"
    },
    {
        "id": "32891953572",
        "type": "WatchEvent",
        "actor": {
            "id": 75266614,
            "login": "v4lentin1879",
            "display_login": "v4lentin1879",
            "gravatar_id": "",
            "url": "https://api.github.com/users/v4lentin1879",
            "avatar_url": "https://avatars.githubusercontent.com/u/75266614?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-27T08:07:19Z"
    },
    {
        "id": "32891144312",
        "type": "WatchEvent",
        "actor": {
            "id": 16830990,
            "login": "xiaolushuo",
            "display_login": "xiaolushuo",
            "gravatar_id": "",
            "url": "https://api.github.com/users/xiaolushuo",
            "avatar_url": "https://avatars.githubusercontent.com/u/16830990?"
        },
        "repo": {
            "id": 612354784,
            "name": "ggerganov/llama.cpp",
            "url": "https://api.github.com/repos/ggerganov/llama.cpp"
        },
        "payload": {
            "action": "started"
        },
        "public": true,
        "created_at": "2023-10-27T07:33:50Z"
    }
]