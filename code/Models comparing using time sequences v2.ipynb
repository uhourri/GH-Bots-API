{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and data importing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.statespace.structural import UnobservedComponents\n",
    "from statsmodels.tsa.exponential_smoothing.ets import ETSModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, root_mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = pd.read_parquet('../data-raw/activities.parquet')\n",
    "activities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "600 events at max for each contributor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ( \n",
    "    activities\n",
    "    # keep the last 600 events for each contributor\n",
    "    .groupby('contributor')\n",
    "    .tail(600)\n",
    "    # keep the contributors who have more than 600 events\n",
    "    .groupby('contributor')\n",
    "    .filter(lambda x: len(x) == 600)\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gap_activities(train, test):\n",
    "\n",
    "    start_time = train['date'].iloc[-1] + pd.Timedelta(hours=1)\n",
    "    end_time = test['date'].iloc[0] - pd.Timedelta(hours=1)\n",
    "\n",
    "    #  check if there is a time gap between the train and test data\n",
    "    if end_time - start_time >= pd.Timedelta(hours=0):\n",
    "\n",
    "        # fill the gap with a date range and zeros for n_activities\n",
    "        gap_data = pd.DataFrame({\n",
    "            'category': train['category'].iloc[0],\n",
    "            'date': pd.date_range(start=start_time, end=end_time, freq='H'),\n",
    "            'contributor': train['contributor'].iloc[0],\n",
    "            'n_activities': 0\n",
    "        })\n",
    "\n",
    "        test = pd.concat([gap_data, test]).reset_index(drop=True)\n",
    "    \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_activities(contributor):\n",
    "\n",
    "    # spliting the data into training and testing sets for time series forecasting, using a time-based split with split size = 0.5\n",
    "    train, test = (\n",
    "        contributor\n",
    "        .apply(lambda x: x[:300]) # head(300)\n",
    "        .groupby(['category', pd.Grouper(key='date', freq='H'), 'contributor'])['activity']\n",
    "        .count()\n",
    "        .reset_index(name='n_activities'),\n",
    "\n",
    "        contributor\n",
    "        .apply(lambda x: x[300:])\n",
    "        .groupby(['category', pd.Grouper(key='date', freq='H'), 'contributor'])['activity']\n",
    "        .count()\n",
    "        .reset_index(name='n_activities')\n",
    "    )\n",
    "\n",
    "    # checking if the last timestamp of the train data is equal to the first timestamp of the second data\n",
    "    if train['date'].iloc[-1] == test['date'].iloc[0]:\n",
    "        # adding the value of the last time value (n_activities) of train data to the value of the first time (n_activities) of the test data\n",
    "        test.loc[0, 'n_activities'] += train.loc[train.index[-1], 'n_activities']\n",
    "        # removing the last time of the train data\n",
    "        train.drop(train.index[-1], inplace=True)\n",
    "\n",
    "    test = gap_activities(train, test)\n",
    "\n",
    "    # filling n_activities with zeros for the empty hours between the minimum and maximum date\n",
    "    train, test = (\n",
    "        # for train set, we take last 3 months\n",
    "        train[train['date'] >= train['date'].max() - pd.DateOffset(months=3)]\n",
    "        .set_index('date')\n",
    "        .resample('H')\n",
    "        .sum()\n",
    "        .rename_axis(None)\n",
    "        .replace({'category': 0, 'contributor': 0}, None)\n",
    "        .ffill(),\n",
    "        \n",
    "        test\n",
    "        .set_index('date')\n",
    "        .resample('H')\n",
    "        .sum()\n",
    "        .rename_axis(None)\n",
    "        .replace({'category': 0, 'contributor': 0}, None)\n",
    "        .ffill()\n",
    "    )\n",
    "\n",
    "    train.index.freq = 'H'\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New evaluation metrics PGA & CTD?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new evaluation metric that calculates the percentage of predicted values greater than or equal to the actual values. We can define this metric as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$PGA = \\frac{\\sum_{i=1}^{n} [y_i \\leq \\hat{y}_i]}{n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pga_score(y_true, y_pred):\n",
    "    return (y_pred >= y_true).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A novel evaluation metric designed to quantify the time difference between the cumulative sums of true and predicted values in reaching a specified target value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{CTD} = \\text{argmax}(C_t \\geq T) - \\text{argmax}(C_p \\geq T) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This formula represents the time difference between the cumulative sums of the true $C_t$ and predicted $C_p$ values in reaching a specified target value $T(100, 200, 300)$, where ${argmax}$ returns the time of the first occurrence where the condition is satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctd_score(y_true, y_pred, target_value):\n",
    "\n",
    "    coef = 1\n",
    "    if (sum(y_true) < target_value) | (sum(y_pred) < target_value):\n",
    "        coef = -1\n",
    "\n",
    "    true_cumsum, pred_cumsum = np.cumsum(y_true), np.cumsum(y_pred)\n",
    "    time_true, time_pred = np.argmax(true_cumsum >= target_value), np.argmax(pred_cumsum >= target_value)\n",
    "\n",
    "    display(true_cumsum.tolist(), pred_cumsum.tolist())\n",
    "\n",
    "    return coef*(time_true - time_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = [2, 11, 84, 57, 0, 38, 15, 80, 4, 30, 90, 0, 0, 0]\n",
    "pred_values = [52, 22, 95, 9, 11, 1, 73, 0, 30, 50, 100, 70, 50, 500]\n",
    "\n",
    "\n",
    "print(\"Cumulative Time Difference:\", ctd_score(true_values, pred_values, 400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Autoregressive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ar_model(contributor):\n",
    "\n",
    "    print(contributor['contributor'].iloc[0])\n",
    "\n",
    "    # Spliting the data into training and testing sets\n",
    "    train, test = split_activities(contributor)\n",
    "\n",
    "    # Fit the model\n",
    "    try: #calculate parameter based on time range\n",
    "        lags = [1, 12, 24, 168]\n",
    "        model = AutoReg(train['n_activities'], lags=lags).fit()\n",
    "        predictions = model.get_prediction(start=len(train), end=len(train)+len(test)-1).summary_frame(alpha=0.05)\n",
    "    except IndexError:\n",
    "        lags = [1, 12, 24]\n",
    "        model = AutoReg(train['n_activities'], lags=lags).fit()\n",
    "        predictions = model.get_prediction(start=len(train), end=len(train)+len(test)-1).summary_frame(alpha=0.05)\n",
    "    except: # eviter\n",
    "        lags = int(len(train)/2)-1\n",
    "        model = AutoReg(train['n_activities'], lags=lags).fit()\n",
    "        predictions = model.get_prediction(start=len(train), end=len(train)+len(test)-1).summary_frame(alpha=0.05)\n",
    "\n",
    "    # Create a series for evaluation metrics and sum of activities\n",
    "    metrics = pd.Series({\n",
    "        'contributor': contributor['contributor'].iloc[0],\n",
    "        'category': contributor['category'].iloc[0],\n",
    "        'r2': r2_score(test['n_activities'], predictions['mean']),\n",
    "        'mae': mean_absolute_error(test['n_activities'], predictions['mean']),\n",
    "        'rmse': root_mean_squared_error(test['n_activities'], predictions['mean']),\n",
    "        'pga': pga_score(test['n_activities'], predictions['mean']),\n",
    "        'ctd_100': ctd_score(test['n_activities'], predictions['mean'], 100),\n",
    "        'ctd_200': ctd_score(test['n_activities'], predictions['mean'], 200),\n",
    "        'ctd_300': ctd_score(test['n_activities'], predictions['mean'], 300),\n",
    "        'n_activities': train['n_activities'].sum(),\n",
    "        'lags': lags,\n",
    "        'true_values': test['n_activities'].values,\n",
    "        'predicted_values': predictions['mean'].values,\n",
    "    })\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each contributor\n",
    "ar_results = data.groupby(['category', 'contributor']).apply(ar_model).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_results.to_csv('../models-evaluation-v2/ar_model_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Seasonal Autoregressive integrated Moving-average model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarima_model(contributor):\n",
    "\n",
    "    print(contributor['contributor'].iloc[0])\n",
    "\n",
    "    # Spliting the data into training and testing sets\n",
    "    train, test = split_activities(contributor)\n",
    "\n",
    "    # Fit the model\n",
    "    model = SARIMAX(train['n_activities'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 24), enforce_invertibility=False, enforce_stationarity=False).fit(disp=False, method='lbfgs')\n",
    "\n",
    "    # Forecast the test set using confidence interval with 95%\n",
    "    predictions = model.get_prediction(start=len(train), end=len(train)+len(test)-1).summary_frame(alpha=0.05)\n",
    "\n",
    "    # Create a series for evaluation metrics and sum of activities\n",
    "    metrics = pd.Series({\n",
    "        'contributor': contributor['contributor'].iloc[0],\n",
    "        'category': contributor['category'].iloc[0],\n",
    "        'r2': r2_score(test['n_activities'], predictions['mean']),\n",
    "        'mae': mean_absolute_error(test['n_activities'], predictions['mean']),\n",
    "        'rmse': root_mean_squared_error(test['n_activities'], predictions['mean']),\n",
    "        'pga': pga_score(test['n_activities'], predictions['mean']),\n",
    "        'ctd_100': ctd_score(test['n_activities'], predictions['mean'], 100),\n",
    "        'ctd_200': ctd_score(test['n_activities'], predictions['mean'], 200),\n",
    "        'ctd_300': ctd_score(test['n_activities'], predictions['mean'], 300),\n",
    "        'n_activities': train['n_activities'].sum(),\n",
    "        'true_values': test['n_activities'].values,\n",
    "        'predicted_values': predictions['mean'].values,\n",
    "    })\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each contributor\n",
    "sarima_results = data.groupby(['category', 'contributor']).apply(sarima_model).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima_results['ctd_200'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima_results.to_csv('../models-evaluation-v2/sarima_model_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Unobserved components model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uc_model(contributor):\n",
    "\n",
    "    print(contributor['contributor'].iloc[0])\n",
    "\n",
    "    # Spliting the data into training and testing sets\n",
    "    train, test = split_activities(contributor)\n",
    "\n",
    "    # Fit the model\n",
    "    model = UnobservedComponents(train['n_activities'], level=True, seasonal=24).fit(disp=False, method='lbfgs')\n",
    "\n",
    "    # Forecast the test set using confidence interval with 95%\n",
    "    predictions = model.get_prediction(start=len(train), end=len(train)+len(test)-1).summary_frame(alpha=0.05)\n",
    "\n",
    "    # Create a series for evaluation metrics and sum of activities\n",
    "    metrics = pd.Series({\n",
    "        'contributor': contributor['contributor'].iloc[0],\n",
    "        'category': contributor['category'].iloc[0],\n",
    "        'r2': r2_score(test['n_activities'], predictions['mean']),\n",
    "        'mae': mean_absolute_error(test['n_activities'], predictions['mean']),\n",
    "        'rmse': root_mean_squared_error(test['n_activities'], predictions['mean']),\n",
    "        'pga': pga_score(test['n_activities'], predictions['mean']),\n",
    "        'ctd_100': ctd_score(test['n_activities'], predictions['mean'], 100),\n",
    "        'ctd_200': ctd_score(test['n_activities'], predictions['mean'], 200),\n",
    "        'ctd_300': ctd_score(test['n_activities'], predictions['mean'], 300),\n",
    "        'n_activities': train['n_activities'].sum(),\n",
    "        'true_values': test['n_activities'].values,\n",
    "        'predicted_values': predictions['mean'].values,\n",
    "    })\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each contributor\n",
    "uc_results = data.groupby(['category', 'contributor']).apply(uc_model).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uc_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uc_results[['contributor', 'category', 'r2', 'rmse', 'pga', 'ctd_100', 'ctd_200', 'ctd_300']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a scatter plot\n",
    "sns.scatterplot(data=uc_results, x='ctd_100', y='ctd_300', hue='category')\n",
    "plt.xlabel('RMSE')\n",
    "plt.ylabel('R2')\n",
    "plt.title('RMSE vs. R2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uc_results['ctd_100'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uc_results.to_csv('../models-evaluation-v2/uc_model_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Holt-Winters (triple) exponential smoothing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tes_model(contributor):\n",
    "\n",
    "    print(contributor['contributor'].iloc[0])\n",
    "\n",
    "    # Spliting the data into training and testing sets\n",
    "    train, test = split_activities(contributor)\n",
    "\n",
    "    # Fit the model\n",
    "    try:\n",
    "        model = ETSModel(train['n_activities'], error='add', trend='add', seasonal='add', seasonal_periods=24).fit(disp=False)\n",
    "    except ValueError:\n",
    "        model = ETSModel(train['n_activities'], error='add', trend='add').fit(disp=False)\n",
    "    except:\n",
    "        print(\"Something else went wrong\")\n",
    "\n",
    "    # Forecast the test set using prediction interval with 95%\n",
    "    predictions = model.get_prediction(start=len(train), end=len(train)+len(test)-1).summary_frame(alpha=0.05)\n",
    "\n",
    "    # Create a series for evaluation metrics and sum of activities\n",
    "    metrics = pd.Series({\n",
    "        'contributor': contributor['contributor'].iloc[0],\n",
    "        'category': contributor['category'].iloc[0],\n",
    "        'r2': r2_score(test['n_activities'], predictions['mean']),\n",
    "        'mae': mean_absolute_error(test['n_activities'], predictions['mean']),\n",
    "        'rmse': root_mean_squared_error(test['n_activities'], predictions['mean']),\n",
    "        'pga': pga_score(test['n_activities'], predictions['mean']),\n",
    "        'ctd_100': ctd_score(test['n_activities'], predictions['mean'], 100),\n",
    "        'ctd_200': ctd_score(test['n_activities'], predictions['mean'], 200),\n",
    "        'ctd_300': ctd_score(test['n_activities'], predictions['mean'], 300),\n",
    "        'n_activities': train['n_activities'].sum(),\n",
    "        'true_values': test['n_activities'].values,\n",
    "        'predicted_values': predictions['mean'].values,\n",
    "    })\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each contributor\n",
    "tes_results = data.groupby(['category', 'contributor']).apply(tes_model).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tes_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tes_results['ctd_100'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tes_results.to_csv('../models-evaluation-v2/tes_model_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_results = pd.read_csv('../models-evaluation-v2/ar_model_metrics.csv')\n",
    "sarima_results = pd.read_csv('../models-evaluation-v2/sarima_model_metrics.csv')\n",
    "uc_results = pd.read_csv('../models-evaluation-v2/uc_model_metrics.csv')\n",
    "tes_results = pd.read_csv('../models-evaluation-v2/tes_model_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_results['model'] = 'ar'\n",
    "sarima_results['model'] = 'sarima'\n",
    "uc_results['model'] = 'uc'\n",
    "tes_results['model'] = 'tes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_results.drop('lags', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([ar_results, sarima_results, uc_results, tes_results], ignore_index=True)\n",
    "results.drop(['n_activities', 'true_values', 'predicted_values'], axis=1, inplace=True)\n",
    "\n",
    "results['ctd_100_abs'] = results['ctd_100'].abs()\n",
    "results['ctd_200_abs'] = results['ctd_200'].abs()\n",
    "results['ctd_300_abs'] = results['ctd_300'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
    "\n",
    "sns.boxenplot(data=results, x='model', y='ctd_100', hue='category', ax=axes[0], showfliers=False)\n",
    "sns.boxenplot(data=results, x='model', y='ctd_200', hue='category', ax=axes[1], showfliers=False)\n",
    "sns.boxenplot(data=results, x='model', y='ctd_300', hue='category', ax=axes[2], showfliers=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=sarima_results.assign(nh=lambda d: d.true_values.apply(len)), x='ctd_300', y='nh', hue='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
    "\n",
    "sns.boxenplot(data=results, x='model', y='mae', hue='category', ax=axes[0], showfliers=False)\n",
    "sns.boxenplot(data=results, x='model', y='rmse', hue='category', ax=axes[1], showfliers=False)\n",
    "sns.boxenplot(data=results, x='model', y='pga', hue='category', ax=axes[2], showfliers=False)\n",
    "\n",
    "axes[0].set_ylim(0, 15)\n",
    "axes[1].set_ylim(0, 15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activitiy types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_user = data['contributor'].sample().values[0]\n",
    "\n",
    "contributor = (\n",
    "    data[data['contributor'] == random_user]\n",
    "    .reset_index(drop=True)[['activity', 'date']]\n",
    "    .groupby([pd.Grouper(key='date', freq='D'), 'activity'])['activity']\n",
    "    .count()\n",
    "    .reset_index(name='n_activities')\n",
    "    .set_index('date')\n",
    "    .rename_axis(None)\n",
    "    .pivot(columns='activity', values='n_activities')\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    ")\n",
    "contributor.insert(0, 'n_activities', contributor.sum(axis=1))\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "sns.lineplot(data=contributor, palette=\"tab10\")\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Number of Activities')\n",
    "plt.title('Activities Over Time for {}'.format(random_user))\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
